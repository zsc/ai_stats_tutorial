<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <base href="./">
    <title>第2章：统计学习理论</title>
    <link rel="stylesheet" href="assets/style.css">
    <link rel="stylesheet" href="assets/highlight.css">
    <script src="assets/script.js" defer></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$']],
                displayMath: [['$$', '$$']],
                processEscapes: false,
                packages: {'[+]': ['noerrors', 'ams']}
            },
            options: {
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            },
            loader: {
                load: ['[tex]/noerrors', '[tex]/ams']
            }
        };
    </script>
</head>
<body>
    <div class="container">
        <nav id="sidebar" class="sidebar">
            <div class="sidebar-header">
                <h3>目录</h3>
                <button id="sidebar-toggle" class="sidebar-toggle">
                    <span></span>
                    <span></span>
                    <span></span>
                </button>
            </div>
            <div class="sidebar-search">
                <input type="text" id="sidebar-search-input" placeholder="搜索..." autocomplete="off">
            </div>
            <div id="tree-container">
                <nav class="tree-nav" role="tree">
                    <div class="tree-item " >
                        <a href="index.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">现代人工智能：优化与统计视角</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter1.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第1章：优化基础与梯度下降</span>
                        </a>
                    </div>
                
                    <div class="tree-item active" >
                        <a href="chapter2.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第2章：统计学习理论</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter3.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第3章：线性模型与正则化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter4.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第4章：神经网络基础</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter5.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第5章：深度学习优化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter6.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第6章：卷积神经网络</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter7.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第7章：循环神经网络与序列建模</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter8.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第8章：Transformer与注意力机制</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter9.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第9章：大语言模型</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter10.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第10章：变分自编码器与生成建模</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter11.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第11章：扩散模型</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter12.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第12章：深度强化学习</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="CLAUDE.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Untitled</span>
                        </a>
                    </div>
                </nav>
            </div>
        </nav>
        
        <main class="content">
            <article>
                <h1 id="2">第2章：统计学习理论</h1>
<p>统计学习理论为机器学习提供了严格的数学基础，帮助我们理解为什么算法能够从有限的数据中学习，以及如何量化学习的效果。本章将探讨泛化能力的本质、学习算法的理论保证，以及如何在实践中应用这些理论见解。通过理解偏差-方差权衡、PAC学习框架和VC理论，读者将建立对机器学习本质的深刻认识。</p>
<h2 id="21-">2.1 偏差-方差权衡</h2>
<h3 id="211">2.1.1 泛化误差的分解</h3>
<p>考虑一个监督学习问题，目标是学习函数 $f: \mathcal{X} \rightarrow \mathcal{Y}$。给定训练集 $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^n$，我们的学习算法产生假设 $\hat{f}_\mathcal{D}$。</p>
<p>对于回归问题，假设真实关系为 $y = f^*(x) + \epsilon$，其中 $\epsilon$ 是均值为0、方差为 $\sigma^2$ 的噪声。在点 $x$ 处的期望预测误差可以分解为：</p>
<p>$$\mathbb{E}_{\mathcal{D}, \epsilon}[(y - \hat{f}_\mathcal{D}(x))^2] = \underbrace{\text{Bias}^2[\hat{f}(x)]}_{\text{偏差平方}} + \underbrace{\text{Var}[\hat{f}(x)]}_{\text{方差}} + \underbrace{\sigma^2}_{\text{不可约误差}}$$
其中：</p>
<ul>
<li><strong>偏差</strong> $\text{Bias}[\hat{f}(x)] = \mathbb{E}_\mathcal{D}[\hat{f}_\mathcal{D}(x)] - f^*(x)$：模型的平均预测与真实值的差异</li>
<li><strong>方差</strong> $\text{Var}[\hat{f}(x)] = \mathbb{E}_\mathcal{D}[(\hat{f}_\mathcal{D}(x) - \mathbb{E}_\mathcal{D}[\hat{f}_\mathcal{D}(x)])^2]$：不同训练集导致的预测变化</li>
</ul>
<p><strong>推导过程</strong>：
从均方误差的定义开始：
$$\begin{align}
\mathbb{E}_{\mathcal{D}, \epsilon}[(y - \hat{f}_\mathcal{D}(x))^2] &amp;= \mathbb{E}_{\mathcal{D}, \epsilon}[(f^*(x) + \epsilon - \hat{f}_\mathcal{D}(x))^2] \\
&amp;= \mathbb{E}_{\mathcal{D}, \epsilon}[(f^*(x) - \hat{f}_\mathcal{D}(x))^2] + \mathbb{E}[\epsilon^2] + 2\mathbb{E}_{\mathcal{D}, \epsilon}[(f^*(x) - \hat{f}_\mathcal{D}(x))\epsilon]
\end{align}$$
由于 $\epsilon$ 独立于 $\mathcal{D}$ 且 $\mathbb{E}[\epsilon] = 0$，第三项为0。对第一项使用偏差-方差分解的恒等式：
$$\mathbb{E}_\mathcal{D}[(f^*(x) - \hat{f}_\mathcal{D}(x))^2] = (f^*(x) - \mathbb{E}_\mathcal{D}[\hat{f}_\mathcal{D}(x)])^2 + \mathbb{E}_\mathcal{D}[(\hat{f}_\mathcal{D}(x) - \mathbb{E}_\mathcal{D}[\hat{f}_\mathcal{D}(x)])^2]$$
这就给出了偏差平方加方差的形式。</p>
<p><strong>直观理解</strong>：</p>
<ul>
<li>偏差反映模型的系统性错误，源于模型假设的限制</li>
<li>方差反映模型对训练数据扰动的敏感度</li>
<li>不可约误差是数据本身的噪声，任何模型都无法消除</li>
</ul>
<p><strong>分解的几何意义</strong>：
想象在高维空间中，每个可能的训练集 $\mathcal{D}$ 产生一个预测函数 $\hat{f}_\mathcal{D}$。所有这些函数形成一个"云"：</p>
<ul>
<li>云的中心（期望位置）与真实函数的距离是偏差</li>
<li>云的分散程度是方差</li>
<li>即使找到真实函数，噪声 $\sigma^2$ 仍然存在</li>
</ul>
<p>这种分解不仅适用于均方误差，对其他损失函数也有类似的分解，尽管形式可能更复杂。例如，0-1损失的分解涉及偏差和方差的非线性组合。</p>
<h3 id="212">2.1.2 模型复杂度的影响</h3>
<div class="codehilite"><pre><span></span><code><span class="err">泛化误差</span>
<span class="w">    </span><span class="o">^</span>
<span class="w">    </span><span class="o">|</span><span class="w">     </span><span class="err">总误差</span>
<span class="w">    </span><span class="o">|</span><span class="w">    </span><span class="o">/</span><span class="w">     </span><span class="err">\</span>
<span class="w">    </span><span class="o">|</span><span class="w">   </span><span class="o">/</span><span class="w">       </span><span class="err">\</span><span class="n">___</span>
<span class="w">    </span><span class="o">|</span><span class="w">  </span><span class="o">/</span><span class="w">            </span><span class="err">\</span><span class="n">___</span><span class="w">  </span>
<span class="w">    </span><span class="o">|</span><span class="w"> </span><span class="o">/</span><span class="w">                  </span><span class="err">\</span><span class="n">___</span>
<span class="w">    </span><span class="o">|/</span><span class="n">________________________</span><span class="err">\</span><span class="n">___</span>
<span class="w">    </span><span class="o">|</span><span class="w">         </span><span class="err">方差</span><span class="w">                  </span><span class="err">\</span>
<span class="w">    </span><span class="o">|</span><span class="w">      </span><span class="n">___</span><span class="o">/</span><span class="w">                      </span><span class="err">\</span>
<span class="w">    </span><span class="o">|</span><span class="w">   </span><span class="n">__</span><span class="o">/</span><span class="w">                           </span><span class="err">\</span>
<span class="w">    </span><span class="o">|</span><span class="n">__</span><span class="o">/</span><span class="n">______________________________</span><span class="o">|</span>
<span class="w">    </span><span class="o">|</span><span class="w">          </span><span class="err">偏差²</span>
<span class="w">    </span><span class="o">|</span><span class="n">__________________________________</span><span class="o">|</span>
<span class="w">    </span><span class="err">简单</span><span class="w"> </span><span class="o">&lt;---</span><span class="w"> </span><span class="err">模型复杂度</span><span class="w"> </span><span class="o">---&gt;</span><span class="w"> </span><span class="err">复杂</span>
</code></pre></div>

<p><strong>关键洞察</strong>：</p>
<ul>
<li>简单模型：高偏差（欠拟合）、低方差</li>
<li>复杂模型：低偏差、高方差（过拟合）</li>
<li>最优复杂度：在偏差和方差之间达到平衡</li>
</ul>
<p><strong>具体例子：多项式回归</strong>
考虑用不同阶数的多项式拟合数据：</p>
<ul>
<li>1阶（线性）：可能无法捕捉数据的非线性模式（高偏差）</li>
<li>20阶：可能完美拟合训练点但在测试点上震荡（高方差）</li>
<li>3-5阶：往往能在两者间找到平衡</li>
</ul>
<p>模型选择的核心问题是找到这个最优点。不同的模型选择方法（AIC、BIC、交叉验证）本质上都在估计这个权衡点。</p>
<p><strong>数学刻画</strong>：
设模型类 $\mathcal{F}_k$ 的复杂度为 k（例如多项式的阶数），则：</p>
<ul>
<li>偏差通常随 k 递减：$\text{Bias}(k) \sim O(k^{-\alpha})$ 对某个 $\alpha &gt; 0$</li>
<li>方差通常随 k 递增：$\text{Var}(k) \sim O(k/n)$</li>
<li>最优复杂度 $k^* \sim n^{1/(1+\alpha)}$</li>
</ul>
<p>这解释了为什么更多数据允许使用更复杂的模型：n 增大时，$k^*$ 也增大。</p>
<p><strong>模型选择准则的统计解释</strong>：</p>
<ul>
<li><strong>AIC</strong>（赤池信息准则）：$-2\ln L + 2k$，渐近等价于留一交叉验证</li>
<li><strong>BIC</strong>（贝叶斯信息准则）：$-2\ln L + k\ln n$，对复杂模型惩罚更重</li>
<li><strong>交叉验证</strong>：直接估计泛化误差，计算代价较高但最可靠</li>
</ul>
<h3 id="213">2.1.3 实践中的权衡</h3>
<p><strong>经验法则</strong> (Rule of Thumb)：</p>
<ol>
<li>
<p><strong>n/p 规则</strong>：样本数 n 应至少是参数数 p 的 5-10 倍
   - 低于这个比例，过拟合风险显著增加
   - 现代深度学习是例外（见2.6节）
   - 高维稀疏问题（如LASSO）可以容忍 p &gt;&gt; n</p>
</li>
<li>
<p><strong>验证集大小</strong>：通常使用 20-30% 的数据作为验证集
   - 数据量很大时（&gt;10万），可以用更小比例（5-10%）
   - 数据量很小时（&lt;1000），考虑留一法交叉验证
   - 时间序列：保持时间顺序，最后20-30%作为验证</p>
</li>
<li>
<p><strong>交叉验证折数</strong>：5折或10折交叉验证通常足够
   - 计算资源充足时，10折更稳定
   - 时间序列数据使用时间切分，而非随机切分
   - 分层采样确保每折中类别比例一致</p>
</li>
</ol>
<p><strong>模型选择策略</strong>：</p>
<ol>
<li>
<p><strong>由简到繁</strong>：从简单模型开始，逐步增加复杂度
   - 建立基线模型（如线性回归、逻辑回归）
   - 逐步添加非线性、交互项
   - 监控验证误差的变化</p>
</li>
<li>
<p><strong>正则化路径</strong>：固定模型复杂度，调整正则化强度
   - 从强正则化开始（大λ）
   - 逐步减小λ，观察验证误差
   - 选择验证误差最小的λ</p>
</li>
<li>
<p><strong>集成方法</strong>：组合多个模型，自动平衡偏差-方差
   - Bagging减少方差（如随机森林）
   - Boosting减少偏差（如梯度提升）
   - Stacking综合多种模型优势</p>
</li>
</ol>
<p><strong>诊断工具</strong>：
学习曲线是诊断偏差-方差问题的有力工具：</p>
<ul>
<li>高偏差：训练和验证误差都高，且接近</li>
<li>高方差：训练误差低但验证误差高，差距大</li>
<li>刚好：训练和验证误差都低，差距小</li>
</ul>
<div class="codehilite"><pre><span></span><code>误差
  ^
  |  高方差情况
  |  验证误差 --------
  |                    
  |  训练误差 ___
  |          /
  |_________|________&gt; 样本量

  |  高偏差情况
  |  验证误差 --------
  |  训练误差 --------
  |
  |_________________&gt; 样本量
</code></pre></div>

<h2 id="22-pac">2.2 PAC学习理论</h2>
<p>PAC（Probably Approximately Correct）学习理论回答了机器学习的根本问题：什么时候学习是可能的？需要多少数据？这个框架由Valiant在1984年提出，为计算学习理论奠定了基础。</p>
<h3 id="221">2.2.1 可学习性的形式化</h3>
<p><strong>定义</strong> (PAC可学习性)：假设类 $\mathcal{H}$ 是PAC可学习的，如果存在算法 $\mathcal{A}$ 和多项式函数 $m_\mathcal{H}: (0,1)^2 \rightarrow \mathbb{N}$，使得：</p>
<p>对任意 $\epsilon, \delta \in (0,1)$，任意分布 $\mathcal{D}$，任意目标概念 $c \in \mathcal{H}$，当样本数 $m \geq m_\mathcal{H}(\epsilon, \delta)$ 时：
$$\Pr_{\mathcal{S} \sim \mathcal{D}^m}[\text{err}_\mathcal{D}(h_\mathcal{S}) \leq \epsilon] \geq 1 - \delta$$
其中：</p>
<ul>
<li>$\epsilon$：精度参数（误差界）</li>
<li>$\delta$：置信参数（失败概率）</li>
<li>$h_\mathcal{S}$：算法在样本 $\mathcal{S}$ 上的输出</li>
<li>$\text{err}_\mathcal{D}(h) = \Pr_{x \sim \mathcal{D}}[h(x) \neq c(x)]$：真实误差</li>
</ul>
<p><strong>直观解释</strong>：</p>
<ul>
<li>"Probably"（概率）：至少以 $1-\delta$ 的概率</li>
<li>"Approximately"（近似）：误差不超过 $\epsilon$</li>
<li>"Correct"（正确）：学到的假设接近目标概念</li>
</ul>
<p>这个定义的强大之处在于它对任意数据分布都成立，这被称为"分布无关"（distribution-free）学习。</p>
<p><strong>PAC学习的两个关键要求</strong>：</p>
<ol>
<li><strong>样本复杂度</strong>：$m_\mathcal{H}(\epsilon, \delta)$ 必须是 $1/\epsilon$ 和 $1/\delta$ 的多项式</li>
<li><strong>计算复杂度</strong>：算法运行时间必须是多项式的</li>
</ol>
<p>第二个要求区分了信息论可学习（存在算法）和计算可学习（存在高效算法）。许多问题在信息论意义上可学习，但计算上困难（如学习一般布尔电路）。</p>
<p><strong>实例：学习轴平行矩形</strong>
在二维平面上学习轴平行矩形：</p>
<ul>
<li>输入空间：$\mathcal{X} = \mathbb{R}^2$</li>
<li>概念类：所有轴平行矩形</li>
<li>样本复杂度：$m = O(\frac{1}{\epsilon}\ln\frac{1}{\delta})$</li>
<li>算法：找到包含所有正例的最小矩形</li>
</ul>
<h3 id="222">2.2.2 有限假设空间</h3>
<p><strong>定理</strong> (有限假设空间的样本复杂度)：
对于有限假设空间 $|\mathcal{H}| &lt; \infty$，要达到 $(\epsilon, \delta)$-PAC 学习，充分的样本数为：
$$m \geq \frac{1}{\epsilon}\left(\ln|\mathcal{H}| + \ln\frac{1}{\delta}\right)$$
<strong>完整证明</strong>：
设 $h^*$ 是假设空间中与目标概念一致的假设。对于任意假设 $h \in \mathcal{H}$，定义：</p>
<ul>
<li>$h$ 是"坏"假设如果 $\text{err}_\mathcal{D}(h) &gt; \epsilon$</li>
<li>$h$ 是"误导"假设如果它是坏假设但在样本 $\mathcal{S}$ 上无错误</li>
</ul>
<p>对于一个坏假设 $h$：
$$\Pr[h \text{ 在 } \mathcal{S} \text{ 上无错误}] \leq (1-\epsilon)^m \leq e^{-\epsilon m}$$
使用联合界，存在误导假设的概率：
$$\Pr[\exists h \in \mathcal{H}_{\text{bad}}: h \text{ 在 } \mathcal{S} \text{ 上无错误}] \leq |\mathcal{H}| \cdot e^{-\epsilon m}$$
要使这个概率 $\leq \delta$：
$$|\mathcal{H}| \cdot e^{-\epsilon m} \leq \delta$$
$$m \geq \frac{1}{\epsilon}\ln\frac{|\mathcal{H}|}{\delta}$$
<strong>意义</strong>：样本复杂度与 $\ln|\mathcal{H}|$ 成正比，这是个好消息——即使假设空间很大，所需样本数增长缓慢。</p>
<h3 id="223">2.2.3 不可知学习</h3>
<p>在现实中，目标函数可能不在假设空间中。<strong>不可知PAC学习</strong>放松了这个假设：
$$\Pr[\text{err}_\mathcal{D}(h_\mathcal{S}) \leq \min_{h \in \mathcal{H}} \text{err}_\mathcal{D}(h) + \epsilon] \geq 1 - \delta$$
样本复杂度变为：$m = O\left(\frac{1}{\epsilon^2}\left(\ln|\mathcal{H}| + \ln\frac{1}{\delta}\right)\right)$</p>
<p>注意从 $1/\epsilon$ 变为 $1/\epsilon^2$，这反映了不可知设置的额外难度。</p>
<p><strong>为什么是 $1/\epsilon^2$？</strong>
在可实现情况下，一旦看到一个错误样本，就能排除一个假设。但在不可知情况下，即使最好的假设也会犯错，需要通过统计估计来区分假设的好坏，这需要更多样本。具体地，使用Hoeffding不等式：
$$\Pr[|\hat{\text{err}}_\mathcal{S}(h) - \text{err}_\mathcal{D}(h)| &gt; \epsilon] \leq 2e^{-2m\epsilon^2}$$
这个二次依赖是集中不等式的本质特征。</p>
<p><strong>不可知学习的实际意义</strong>：</p>
<ol>
<li><strong>鲁棒性</strong>：不需要假设完美模型存在</li>
<li><strong>现实性</strong>：真实问题中噪声和模型误设很常见</li>
<li><strong>算法设计</strong>：经验风险最小化（ERM）在不可知设置下仍然最优</li>
</ol>
<p><strong>样本复杂度的直观理解</strong>：</p>
<ul>
<li>可实现：$m \sim 1/\epsilon$ - 线性收敛</li>
<li>不可知：$m \sim 1/\epsilon^2$ - 二次收敛</li>
<li>含义：要将误差减半，不可知学习需要4倍的样本</li>
</ul>
<p><strong>与估计理论的联系</strong>：
不可知学习本质上是参数估计问题。$1/\epsilon^2$ 的依赖对应于中心极限定理：</p>
<ul>
<li>均值估计的标准误：$\sigma/\sqrt{n}$</li>
<li>要达到精度 $\epsilon$：$n \sim \sigma^2/\epsilon^2$</li>
</ul>
<p>这解释了为什么许多统计估计量（均值、比例、相关系数）都有 $O(1/\epsilon^2)$ 的样本复杂度。</p>
<h2 id="23-vc">2.3 VC维与泛化界</h2>
<p>有限假设空间的理论优雅但受限。VC理论将PAC框架扩展到无限假设空间，这是Vapnik和Chervonenkis的杰出贡献。</p>
<h3 id="231-vc">2.3.1 VC维的定义</h3>
<p><strong>定义</strong> (打散)：假设空间 $\mathcal{H}$ 能够打散点集 $C = \{x_1, ..., x_m\}$，如果对于 $C$ 的任意标记，都存在 $h \in \mathcal{H}$ 与之一致。</p>
<p>形式化地，$\mathcal{H}$ 打散 $C$ 当且仅当：
$$|\{(h(x_1), ..., h(x_m)) : h \in \mathcal{H}\}| = 2^m$$
<strong>定义</strong> (VC维)：$\mathcal{H}$ 的VC维是它能打散的最大点集的大小：
$$\text{VCdim}(\mathcal{H}) = \max\{m : \exists C, |C|=m, \mathcal{H} \text{ 打散 } C\}$$
<strong>直观理解</strong>：</p>
<ul>
<li>VC维衡量假设空间的"表达能力"或"复杂度"</li>
<li>如果VC维是d，意味着存在d个点的某种配置可以被任意标记</li>
<li>但任何d+1个点都无法被任意标记</li>
</ul>
<p><strong>例子：二维线性分类器</strong>
考虑平面上的直线分类器。</p>
<ul>
<li>能打散3个非共线点（$2^3=8$种标记都可实现）</li>
<li>不能打散任意4个点（考虑XOR配置）</li>
<li>因此VC维 = 3</li>
</ul>
<h3 id="232-vc">2.3.2 常见函数类的VC维</h3>
<p>| 函数类 | VC维 | 直觉解释 |</p>
<table>
<thead>
<tr>
<th>函数类</th>
<th>VC维</th>
<th>直觉解释</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>线性分类器</strong> ($\mathbb{R}^d$)</td>
<td>d + 1</td>
<td>d个权重+1个偏置</td>
</tr>
<tr>
<td><strong>多项式</strong> (度数 k, d维)</td>
<td>$\binom{d+k}{k}$</td>
<td>单项式的数量</td>
</tr>
<tr>
<td><strong>正弦函数</strong> $\sin(\omega x + \phi)$</td>
<td>∞</td>
<td>频率可任意高</td>
</tr>
<tr>
<td><strong>决策树</strong> (深度k, 二元特征)</td>
<td>$2^k$</td>
<td>最多$2^k$个叶节点</td>
</tr>
<tr>
<td><strong>k近邻</strong></td>
<td>∞</td>
<td>可以记住任意标记</td>
</tr>
<tr>
<td><strong>神经网络</strong> (W个权重)</td>
<td>$O(W^2)$</td>
<td>上界，实际可能更小</td>
</tr>
<tr>
<td><strong>RBF核SVM</strong></td>
<td>∞</td>
<td>可以实现任意决策边界</td>
</tr>
<tr>
<td><strong>有限精度线性分类器</strong></td>
<td>$O(d\log(1/\gamma))$</td>
<td>γ是精度</td>
</tr>
</tbody>
</table>
<p><strong>重要观察</strong>：</p>
<ul>
<li>参数数量不等于VC维（正弦函数只有2个参数但VC维无限）</li>
<li>VC维可以是无限的，这时需要其他复杂度度量</li>
<li>约束（如有限精度、正则化）可以有效控制VC维</li>
</ul>
<p><strong>VC维的计算技巧</strong>：</p>
<ol>
<li>
<p><strong>上界</strong>：通过参数计数
   - 如果函数类有p个实值参数，VC维 ≤ O(p²)</p>
</li>
<li>
<p><strong>下界</strong>：构造可打散的点集
   - 找到特定配置的d个点能被打散</p>
</li>
<li>
<p><strong>精确值</strong>：需要证明
   - d个点可打散（下界）
   - d+1个点不可打散（上界）</p>
</li>
</ol>
<h3 id="233-vc">2.3.3 VC维与样本复杂度</h3>
<p><strong>基本定理</strong> (VC维的泛化界)：
假设空间 $\mathcal{H}$ 的VC维为 d &lt; ∞，则：</p>
<ol>
<li>
<p><strong>上界</strong>：$\mathcal{H}$ 是PAC可学习的，样本复杂度：
$$m = O\left(\frac{d + \ln(1/\delta)}{\epsilon}\right)$$</p>
</li>
<li>
<p><strong>下界</strong>：任何学习算法都需要至少：
$$m = \Omega\left(\frac{d}{\epsilon} + \frac{\ln(1/\delta)}{\epsilon}\right)$$
<strong>证明核心思想</strong>：
关键是增长函数（growth function）：
$$\Pi_\mathcal{H}(m) = \max_{|C|=m} |\{(h(x_1),...,h(x_m)) : h \in \mathcal{H}\}|$$
Sauer引理：如果VCdim($\mathcal{H}$) = d，则：
$$\Pi_\mathcal{H}(m) \leq \sum_{i=0}^d \binom{m}{i} \leq \left(\frac{em}{d}\right)^d$$
这个多项式增长（而非指数增长）是关键，它允许我们用有限样本学习无限假设空间。</p>
</li>
</ol>
<p>不可知学习情况下：
$$m = O\left(\frac{d + \ln(1/\delta)}{\epsilon^2}\right)$$</p>
<h3 id="234-rademacher">2.3.4 Rademacher复杂度</h3>
<p>VC维是最坏情况的度量。Rademacher复杂度提供了数据依赖的复杂度度量，往往给出更紧的界。</p>
<p><strong>定义</strong>：经验Rademacher复杂度衡量假设类对随机噪声的拟合能力：
$$\hat{\mathcal{R}}_S(\mathcal{H}) = \mathbb{E}_{\sigma}\left[\sup_{h \in \mathcal{H}} \frac{1}{m}\sum_{i=1}^m \sigma_i h(x_i)\right]$$
其中 $\sigma_i$ 是独立的Rademacher随机变量（±1等概率）。</p>
<p><strong>直观理解</strong>：</p>
<ul>
<li>给数据加上随机标签</li>
<li>看假设类能多好地拟合这些随机标签</li>
<li>拟合越好，说明假设类越复杂</li>
</ul>
<p><strong>泛化界</strong>：以概率至少 $1-\delta$：
$$\text{err}_\mathcal{D}(h) \leq \text{err}_S(h) + 2\mathcal{R}_m(\mathcal{H}) + \sqrt{\frac{\ln(1/\delta)}{2m}}$$
<strong>与VC维的关系</strong>：
$$\mathcal{R}_m(\mathcal{H}) \leq O\left(\sqrt{\frac{d}{m}}\right)$$
其中d是VC维。但对特定数据集，Rademacher复杂度可能远小于这个界。</p>
<p><strong>优势</strong>：</p>
<ol>
<li>数据依赖：考虑了实际数据分布</li>
<li>可以直接计算或估计</li>
<li>对神经网络等复杂模型给出更紧的界</li>
</ol>
<h2 id="24">2.4 正则化的统计解释</h2>
<p>正则化是控制模型复杂度的核心技术。从统计学角度，它连接了频率学派的风险最小化和贝叶斯学派的先验知识。</p>
<h3 id="241">2.4.1 结构风险最小化</h3>
<p>经验风险最小化（ERM）可能导致过拟合。<strong>结构风险最小化</strong>（SRM）通过添加复杂度惩罚来平衡：
$$\hat{h} = \arg\min_{h \in \mathcal{H}} \underbrace{\frac{1}{n}\sum_{i=1}^n \ell(h(x_i), y_i)}_{\text{经验风险}} + \underbrace{\lambda \cdot \Omega(h)}_{\text{复杂度惩罚}}$$
<strong>理论基础</strong>：
根据统计学习理论，真实风险的上界为：
$$R(h) \leq R_{\text{emp}}(h) + \sqrt{\frac{\text{complexity}(h)}{n}}$$
SRM直接优化这个上界，其中：</p>
<ul>
<li>第一项鼓励拟合数据</li>
<li>第二项惩罚复杂模型</li>
<li>$\lambda$ 控制两者的权衡</li>
</ul>
<p><strong>常见的复杂度惩罚</strong>：</p>
<ol>
<li><strong>参数范数</strong>：$\Omega(h) = |\theta|_p^p$</li>
<li><strong>函数平滑度</strong>：$\Omega(h) = \int |\nabla h(x)|^2 dx$</li>
<li><strong>稀疏性</strong>：$\Omega(h) = |\theta|_0$（非凸，实践中用L1近似）</li>
</ol>
<h3 id="242">2.4.2 正则化与贝叶斯先验</h3>
<p>从贝叶斯角度，正则化对应于参数的先验分布：</p>
<p>| 正则化类型 | 惩罚项 | 对应先验 | 效果 |</p>
<table>
<thead>
<tr>
<th>正则化类型</th>
<th>惩罚项</th>
<th>对应先验</th>
<th>效果</th>
</tr>
</thead>
<tbody>
<tr>
<td>L2 (Ridge)</td>
<td>$\\</td>
<td>\theta\\</td>
<td>_2^2$</td>
</tr>
<tr>
<td>L1 (LASSO)</td>
<td>$\\</td>
<td>\theta\\</td>
<td>_1$</td>
</tr>
<tr>
<td>Elastic Net</td>
<td>$\alpha\\</td>
<td>\theta\\</td>
<td>_1 + (1-\alpha)\\</td>
</tr>
<tr>
<td>Dropout</td>
<td>随机置零</td>
<td>尖峰平板先验</td>
<td>集成效应</td>
</tr>
</tbody>
</table>
<p><strong>MAP估计与正则化的等价性</strong>：
$$\hat{\theta}_{\text{MAP}} = \arg\max_\theta \underbrace{\ln p(\mathcal{D}|\theta)}_{\text{似然}} + \underbrace{\ln p(\theta)}_{\text{先验}} = \arg\min_\theta -\ln p(\mathcal{D}|\theta) - \ln p(\theta)$$
<strong>推导示例（岭回归）</strong>：
假设先验 $p(\theta) = \mathcal{N}(0, \tau^2 I)$，似然 $p(y|X, \theta) = \mathcal{N}(X\theta, \sigma^2 I)$：
$$\begin{align}
\hat{\theta}_{\text{MAP}} &amp;= \arg\min_\theta |y - X\theta|_2^2 + \frac{\sigma^2}{\tau^2}|\theta|_2^2 \\
&amp;= \arg\min_\theta |y - X\theta|_2^2 + \lambda|\theta|_2^2
\end{align}$$
其中 $\lambda = \sigma^2/\tau^2$ 是信噪比的函数。</p>
<p><strong>为什么L1产生稀疏解？</strong></p>
<ul>
<li>L1惩罚在原点不可导，梯度有"跳跃"</li>
<li>拉普拉斯先验在0处有尖峰，偏好零值</li>
<li>几何上，L1球与等高线相切于坐标轴</li>
</ul>
<h3 id="243">2.4.3 有效自由度</h3>
<p>正则化减少模型的有效自由度，这可以精确量化。</p>
<p><strong>定义</strong>：有效自由度是预测值对观测值的敏感度：
$$\text{df} = \sum_{i=1}^n \frac{\partial \hat{y}_i}{\partial y_i}$$
<strong>岭回归的有效自由度</strong>：
$$\text{df}(\lambda) = \text{tr}(X(X^TX + \lambda I)^{-1}X^T) = \sum_{j=1}^p \frac{d_j^2}{d_j^2 + \lambda}$$
其中 $d_j$ 是 $X$ 的奇异值。</p>
<p><strong>性质</strong>：</p>
<ul>
<li>$\lambda = 0$：df = p（原始参数数）</li>
<li>$\lambda \to \infty$：df → 0（完全收缩）</li>
<li>$\lambda$ 增加，df单调递减</li>
</ul>
<p><strong>LASSO的有效自由度</strong>：
更复杂，但近似为非零系数的个数：
$$\text{df}_{\text{LASSO}} \approx |\{j : \hat{\theta}_j \neq 0\}|$$
<strong>经验法则</strong>：</p>
<ul>
<li>选择 $\lambda$ 使有效自由度约为 n/10</li>
<li>使用信息准则：AIC = $-2\ln L + 2\cdot\text{df}$</li>
<li>交叉验证通常最可靠</li>
</ul>
<h3 id="244">2.4.4 正则化路径与模型选择</h3>
<p><strong>正则化路径</strong>：参数估计 $\hat{\theta}(\lambda)$ 随 $\lambda$ 变化的轨迹。</p>
<p>对于某些问题（如LASSO），路径是分段线性的，可以高效计算整条路径（LARS算法）。</p>
<p><strong>选择最优 $\lambda$</strong>：</p>
<ol>
<li><strong>交叉验证</strong>：最常用，直接估计泛化误差</li>
<li><strong>信息准则</strong>：
   - AIC：$-2\ln L + 2\text{df}$
   - BIC：$-2\ln L + \ln(n)\text{df}$</li>
<li><strong>贝叶斯方法</strong>：将 $\lambda$ 作为超参数，用经验贝叶斯或完全贝叶斯推断</li>
</ol>
<h2 id="25">2.5 历史人物：弗拉基米尔·瓦普尼克</h2>
<p>弗拉基米尔·瓦普尼克（Vladimir Vapnik，1936-）是统计学习理论的奠基人之一。他与切尔沃年基斯（Alexey Chervonenkis）在1970年代共同提出了VC理论，为机器学习提供了坚实的理论基础。</p>
<p><strong>主要贡献</strong>：</p>
<ol>
<li><strong>VC维理论</strong> (1971)：首次给出了学习算法泛化能力的定量描述</li>
<li><strong>支持向量机</strong> (1995)：与Cortes共同提出软间隔SVM</li>
<li><strong>统计学习理论</strong> (1998)：出版经典著作《统计学习理论的本质》</li>
</ol>
<p>瓦普尼克的一个重要洞察是："当解决一个感兴趣的问题时，不要试图解决一个更一般的问题作为中间步骤。" 这个原则影响了SVM的设计——直接优化决策边界，而不是估计概率分布。</p>
<p><strong>经典语录</strong>：</p>
<blockquote>
<p>"Nothing is more practical than a good theory." （没有什么比好的理论更实用。）</p>
</blockquote>
<h2 id="26">2.6 现代连接：大模型的过参数化悖论与双下降现象</h2>
<h3 id="261">2.6.1 经典理论的挑战</h3>
<p>传统统计学习理论预测：当模型参数数量超过样本数时，会严重过拟合。然而，现代深度学习模型（特别是大语言模型）常常有数十亿参数，远超训练样本数，却展现出优异的泛化能力。</p>
<h3 id="262">2.6.2 双下降现象</h3>
<div class="codehilite"><pre><span></span><code><span class="err">测试误差</span>
<span class="w">    </span><span class="o">^</span>
<span class="w">    </span><span class="o">|</span><span class="w">     </span><span class="err">经典</span><span class="n">U型曲线</span>
<span class="w">    </span><span class="o">|</span><span class="w">    </span><span class="o">/</span><span class="w">         </span><span class="err">\</span>
<span class="w">    </span><span class="o">|</span><span class="w">   </span><span class="o">/</span><span class="w">           </span><span class="err">\</span><span class="n">___</span><span class="w">  </span><span class="err">第二次下降</span>
<span class="w">    </span><span class="o">|</span><span class="w">  </span><span class="o">/</span><span class="w">                </span><span class="err">\</span><span class="n">___</span>
<span class="w">    </span><span class="o">|</span><span class="w"> </span><span class="o">/</span><span class="w">                      </span><span class="err">\</span><span class="n">___</span>
<span class="w">    </span><span class="o">|/</span><span class="n">___________________________</span><span class="o">|</span><span class="n">___</span>
<span class="w">    </span><span class="o">|</span><span class="w">                    </span><span class="err">↑</span><span class="w">           </span><span class="err">\</span><span class="n">___</span>
<span class="w">    </span><span class="o">|</span><span class="w">              </span><span class="err">插值阈值</span><span class="w">               </span><span class="err">\</span><span class="n">___</span>
<span class="w">    </span><span class="o">|</span><span class="w">                </span><span class="p">(</span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">p</span><span class="p">)</span><span class="w">                  </span><span class="err">\</span><span class="n">___</span>
<span class="w">    </span><span class="o">|</span><span class="n">______________________________________________</span><span class="o">|</span>
<span class="w">          </span><span class="err">欠参数化</span><span class="w">    </span><span class="o">|</span><span class="w">    </span><span class="err">过参数化</span>
<span class="w">                  </span><span class="err">模型复杂度</span><span class="w"> </span><span class="o">--&gt;</span>
</code></pre></div>

<p><strong>关键观察</strong>：</p>
<ol>
<li><strong>欠参数化区域</strong> (p &lt; n)：遵循经典偏差-方差权衡</li>
<li><strong>插值阈值</strong> (p ≈ n)：测试误差达到峰值</li>
<li><strong>过参数化区域</strong> (p &gt;&gt; n)：测试误差再次下降</li>
</ol>
<h3 id="263">2.6.3 理论解释</h3>
<p><strong>隐式正则化</strong>：梯度下降在过参数化模型中倾向于找到"简单"的解：</p>
<ul>
<li>最小范数解：在所有拟合训练数据的解中，SGD倾向于找到参数范数最小的</li>
<li>平滑性偏好：神经网络倾向于学习平滑函数</li>
</ul>
<p><strong>神经切线核(NTK)视角</strong>：</p>
<ul>
<li>无限宽网络在训练过程中表现得像线性模型</li>
<li>有效模型复杂度由核函数决定，而非参数数量</li>
</ul>
<h3 id="264-llm">2.6.4 对LLM的启示</h3>
<p><strong>缩放定律</strong> (Scaling Laws)：</p>
<ul>
<li>模型性能 ∝ (计算量)^α，α ≈ 0.05-0.1</li>
<li>最优模型大小 ∝ (数据量)^β，β ≈ 0.5-0.7</li>
</ul>
<p><strong>实践建议</strong>：</p>
<ol>
<li><strong>不要过早停止增大模型</strong>：在计算资源允许的情况下，更大的模型通常更好</li>
<li><strong>数据质量 &gt; 数据数量</strong>：高质量数据对大模型尤其重要</li>
<li><strong>正则化技术仍然重要</strong>：Dropout、权重衰减在大模型中仍有效</li>
</ol>
<h2 id="_1">本章小结</h2>
<h3 id="_2">核心概念回顾</h3>
<ol>
<li>
<p><strong>偏差-方差分解</strong>：
   - 泛化误差 = 偏差² + 方差 + 噪声
   - 模型选择的核心是平衡偏差和方差</p>
</li>
<li>
<p><strong>PAC学习框架</strong>：
   - 提供了学习可行性的理论保证
   - 样本复杂度：$m = O(\frac{1}{\epsilon}(\ln|\mathcal{H}| + \ln\frac{1}{\delta}))$</p>
</li>
<li>
<p><strong>VC理论</strong>：
   - VC维刻画了假设空间的复杂度
   - 提供了与假设空间大小无关的泛化界</p>
</li>
<li>
<p><strong>正则化的统计意义</strong>：
   - 正则化 = 结构风险最小化
   - 不同正则化对应不同的贝叶斯先验</p>
</li>
<li>
<p><strong>现代深度学习的新现象</strong>：
   - 双下降曲线挑战传统理论
   - 过参数化模型的隐式正则化</p>
</li>
</ol>
<h3 id="_3">关键公式汇总</h3>
<p>| 概念 | 公式 | 说明 |</p>
<table>
<thead>
<tr>
<th>概念</th>
<th>公式</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>偏差-方差分解</td>
<td>$\mathbb{E}[(y - \hat{f}(x))^2] = \text{Bias}^2 + \text{Var} + \sigma^2$</td>
<td>泛化误差的三个来源</td>
</tr>
<tr>
<td>PAC界(有限H)</td>
<td>$m \geq \frac{1}{\epsilon}(\ln</td>
<td>\mathcal{H}</td>
</tr>
<tr>
<td>VC维泛化界</td>
<td>$\text{err}_\mathcal{D} \leq \text{err}_S + O(\sqrt{\frac{d}{m}})$</td>
<td>d是VC维</td>
</tr>
<tr>
<td>正则化目标</td>
<td>$\min_h \frac{1}{n}\sum_i \ell(h(x_i), y_i) + \lambda\Omega(h)$</td>
<td>经验风险+复杂度</td>
</tr>
</tbody>
</table>
<h3 id="_4">实用建议</h3>
<ol>
<li><strong>模型选择</strong>：使用交叉验证选择模型复杂度</li>
<li><strong>样本量估计</strong>：参数数量的5-10倍是好的起点</li>
<li><strong>正则化强度</strong>：从较大的λ开始，逐步减小</li>
<li><strong>深度模型</strong>：不要害怕过参数化，关注优化和正则化</li>
</ol>
<h2 id="_5">练习题</h2>
<h3 id="_6">基础题</h3>
<p><strong>练习2.1</strong> （偏差-方差计算）
考虑一维回归问题，真实函数 $f^*(x) = x^2$，训练数据从 $[-1, 1]$ 均匀采样。比较以下两个模型的偏差和方差：</p>
<ul>
<li>模型A：$\hat{f}_A(x) = ax + b$（线性模型）</li>
<li>模型B：$\hat{f}_B(x) = ax^4 + bx^3 + cx^2 + dx + e$（4次多项式）</li>
</ul>
<p><em>提示：线性模型无法完美拟合二次函数，会有系统性偏差。</em></p>
<details>
<summary>答案</summary>
<p>模型A（线性）：</p>
<ul>
<li>高偏差：无法表示 $x^2$ 的曲率，最佳线性近似是 $\hat{f}_A(x) = x/3$</li>
<li>低方差：只有2个参数，对训练数据的扰动不敏感</li>
<li>偏差² ≈ $\mathbb{E}_{x\sim U[-1,1]}[(x^2 - x/3)^2] = 2/15$</li>
</ul>
<p>模型B（4次多项式）：</p>
<ul>
<li>低偏差：可以完美表示 $x^2$（令 $a=b=d=e=0, c=1$）</li>
<li>高方差：5个参数，容易过拟合噪声</li>
<li>偏差² = 0（理论上），方差随样本量减少而增大</li>
</ul>
<p>结论：小样本时模型A可能更好，大样本时模型B更好。</p>
</details>
<p><strong>练习2.2</strong> （PAC样本复杂度）
假设要学习一个布尔函数，假设空间包含所有最多3个变量的合取式（如 $x_1 \land \neg x_2 \land x_3$）。输入空间有10个布尔变量。
a) 计算假设空间大小
b) 要达到误差率 ε=0.1，置信度 1-δ=0.95，需要多少样本？</p>
<p><em>提示：每个变量可以出现正面、负面或不出现。</em></p>
<details>
<summary>答案</summary>
<p>a) 假设空间大小：</p>
<ul>
<li>选择3个变量：$\binom{10}{3} = 120$ 种方式</li>
<li>每个变量3种状态（正、负、不出现）：$3^3 = 27$ 种组合</li>
<li>但"都不出现"不是有效合取式，所以 $27-1=26$ 种</li>
<li>总计：$|\mathcal{H}| = 120 \times 26 = 3120$</li>
</ul>
<p>b) 样本复杂度：
$$m \geq \frac{1}{0.1}(\ln 3120 + \ln\frac{1}{0.05}) = 10 \times (8.05 + 3.00) = 110.5$$
需要至少111个样本。</p>
</details>
<p><strong>练习2.3</strong> （VC维判断）
证明或反证：二维平面上的轴平行矩形（边平行于坐标轴）的VC维是4。</p>
<p><em>提示：考虑4个点的配置，尝试是否能实现所有16种标记。</em></p>
<details>
<summary>答案</summary>
<p><strong>证明VC维 = 4</strong>：</p>
<ol>
<li>
<p>能打散4个点：
   - 配置：将4个点放在一个矩形的4个角
   - 对任意标记，可以调整矩形大小包含正例，排除负例</p>
</li>
<li>
<p>不能打散5个点：
   - 关键观察：轴平行矩形是凸集
   - 如果5个点中有1个在其他4个的凸包内部
   - 无法标记内部点为负、外部4点为正</p>
</li>
</ol>
<p>因此，VC维 = 4。</p>
<p>这个结果可推广：d维空间中的轴平行超矩形的VC维 = 2d。</p>
</details>
<h3 id="_7">挑战题</h3>
<p><strong>练习2.4</strong> （Rademacher复杂度）
考虑线性函数类 $\mathcal{H} = \{x \mapsto w^Tx : |w|_2 \leq B\}$，其中 $x \in \mathbb{R}^d$。
证明其Rademacher复杂度满足：
$$\mathcal{R}_m(\mathcal{H}) \leq \frac{B\sqrt{d}}{m}\sqrt{\sum_{i=1}^m |x_i|_2^2}$$
<em>提示：使用Cauchy-Schwarz不等式。</em></p>
<details>
<summary>答案</summary>
<p><strong>证明</strong>：
$$\hat{\mathcal{R}}_S(\mathcal{H}) = \mathbb{E}_\sigma\left[\sup_{|w|_2 \leq B} \frac{1}{m}\sum_{i=1}^m \sigma_i w^Tx_i\right]$$</p>
<p>$$= \frac{B}{m}\mathbb{E}_\sigma\left[\sup_{|w|_2 \leq 1} w^T\sum_{i=1}^m \sigma_i x_i\right]$$</p>
<p>$$= \frac{B}{m}\mathbb{E}_\sigma\left[\left|\sum_{i=1}^m \sigma_i x_i\right|_2\right]$$
利用 $\mathbb{E}[|Z|_2] \leq \sqrt{\mathbb{E}[|Z|_2^2]}$ (Jensen不等式)：
$$\leq \frac{B}{m}\sqrt{\mathbb{E}_\sigma\left[\left|\sum_{i=1}^m \sigma_i x_i\right|_2^2\right]}$$</p>
<p>$$= \frac{B}{m}\sqrt{\sum_{i=1}^m |x_i|_2^2}$$
最后一步用了 $\mathbb{E}[\sigma_i\sigma_j] = 0$ (i≠j)。</p>
<p>对于一般的d维情况，需要更精细的分析，最终得到 $O(B\sqrt{d/m})$ 的界。</p>
</details>
<p><strong>练习2.5</strong> （双下降现象）
设计一个简单的实验设置，展示双下降现象。考虑：</p>
<ul>
<li>数据：n个点的一维多项式拟合</li>
<li>模型：不同阶数的多项式</li>
<li>描述你期望看到的现象</li>
</ul>
<p><em>提示：考虑插值阈值附近会发生什么。</em></p>
<details>
<summary>答案</summary>
<p><strong>实验设置</strong>：</p>
<ul>
<li>真实函数：$f(x) = \sin(2\pi x)$，$x \in [0, 1]$</li>
<li>训练数据：n=20个均匀采样点，加噪声 $\mathcal{N}(0, 0.1^2)$</li>
<li>模型：多项式阶数 p = 1, 2, ..., 50</li>
<li>测试：100个均匀采样点</li>
</ul>
<p><strong>预期现象</strong>：</p>
<ol>
<li>
<p>p &lt; 20：经典U型曲线
   - p很小时欠拟合（高偏差）
   - p接近20时方差增大</p>
</li>
<li>
<p>p = 20（插值阈值）：
   - 完美拟合训练数据
   - 测试误差达到峰值（高方差）</p>
</li>
<li>
<p>p &gt; 20：第二次下降
   - 多项式系数的最小范数解
   - 隐式正则化效应
   - 测试误差再次下降</p>
</li>
</ol>
<p><strong>关键洞察</strong>：</p>
<ul>
<li>在插值阈值，模型刚好能记住训练数据，但以最"激进"的方式</li>
<li>过参数化后，有无穷多解，优化算法选择了"平滑"的解</li>
</ul>
</details>
<p><strong>练习2.6</strong> （正则化路径）
对于岭回归 $\min_w |Xw - y|_2^2 + \lambda|w|_2^2$，证明：
当 $\lambda$ 从 0 增加到 ∞，解路径 $w(\lambda)$ 是连续的，且 $|w(\lambda)|_2$ 单调递减。</p>
<p><em>提示：使用解的闭式表达式。</em></p>
<details>
<summary>答案</summary>
<p><strong>证明</strong>：</p>
<p>岭回归的解：$w(\lambda) = (X^TX + \lambda I)^{-1}X^Ty$</p>
<ol>
<li>
<p><strong>连续性</strong>：
   - $(X^TX + \lambda I)^{-1}$ 关于 $\lambda$ 连续（矩阵逆的连续性）
   - 因此 $w(\lambda)$ 连续</p>
</li>
<li>
<p><strong>单调性</strong>：
   使用SVD分解 $X = U\Sigma V^T$：
$$w(\lambda) = V(\Sigma^2 + \lambda I)^{-1}\Sigma U^Ty$$</p>
</li>
</ol>
<p>$$= \sum_{i=1}^r \frac{\sigma_i}{\sigma_i^2 + \lambda} u_i^Ty \cdot v_i$$
其中 $\sigma_i$ 是奇异值。
$$|w(\lambda)|_2^2 = \sum_{i=1}^r \left(\frac{\sigma_i}{\sigma_i^2 + \lambda}\right)^2 (u_i^Ty)^2$$
对 $\lambda$ 求导：
$$\frac{d}{d\lambda}|w(\lambda)|_2^2 = -2\sum_{i=1}^r \frac{\sigma_i^2}{(\sigma_i^2 + \lambda)^3}(u_i^Ty)^2 &lt; 0$$</p>
<p>因此 $|w(\lambda)|_2$ 严格单调递减。</p>
<p><strong>几何解释</strong>：增加 $\lambda$ 相当于收缩参数向原点，是一个连续的收缩过程。</p>
</details>
<p><strong>练习2.7</strong> （开放思考题）
现代大语言模型（如GPT-4）有数千亿参数，但训练数据可能只有数TB文本。从统计学习理论角度，这种极端过参数化为什么能成功？提出至少3个可能的解释。</p>
<details>
<summary>参考思路</summary>
<p>可能的解释：</p>
<ol>
<li>
<p><strong>数据的有效样本量</strong>：
   - 文本数据高度结构化，每个token提供多个学习信号
   - 自回归训练使每个序列产生 T 个训练样本
   - 实际的"有效样本量"远大于文档数</p>
</li>
<li>
<p><strong>归纳偏置</strong>：
   - Transformer架构的归纳偏置（注意力、位置编码）
   - 预训练任务（下一个词预测）隐含了强大的先验
   - 层次化表示学习减少了有效复杂度</p>
</li>
<li>
<p><strong>隐式正则化机制</strong>：
   - Adam优化器的自适应学习率提供隐式正则化
   - Dropout、层归一化等显式正则化
   - 早停（计算预算限制）防止过度优化</p>
</li>
<li>
<p><strong>任务的内在维度</strong>：
   - 自然语言的内在维度可能远低于参数空间维度
   - 模型学习的是低维流形上的函数
   - 冗余参数提供了鲁棒性和表达灵活性</p>
</li>
<li>
<p><strong>涌现能力</strong>：
   - 过参数化允许模型同时学习多个任务
   - 不同参数子集负责不同能力
   - 规模提供了"容量"来存储世界知识</p>
</li>
</ol>
<p>这些解释并非互斥，可能共同作用使大模型成功。</p>
</details>
<h2 id="_8">常见陷阱与错误</h2>
<h3 id="1">1. 概念混淆</h3>
<p>❌ <strong>错误</strong>：认为复杂模型总是过拟合
✅ <strong>正确</strong>：在大数据、适当正则化下，复杂模型可以很好地泛化</p>
<p>❌ <strong>错误</strong>：将训练误差当作泛化误差
✅ <strong>正确</strong>：必须在独立的测试集上评估泛化性能</p>
<h3 id="2_1">2. 实践误区</h3>
<p>❌ <strong>错误</strong>：只看验证集性能选择模型
✅ <strong>正确</strong>：考虑模型复杂度、训练时间、可解释性等多个因素</p>
<p>❌ <strong>错误</strong>：正则化参数越大越好
✅ <strong>正确</strong>：过度正则化会导致欠拟合，需要平衡</p>
<h3 id="3">3. 理论应用</h3>
<p>❌ <strong>错误</strong>：直接使用VC维界选择模型
✅ <strong>正确</strong>：VC界通常过于保守，实践中用交叉验证</p>
<p>❌ <strong>错误</strong>：认为PAC界给出了精确的样本需求
✅ <strong>正确</strong>：PAC界是最坏情况界，实际可能需要更少样本</p>
<h3 id="4">4. 现代深度学习</h3>
<p>❌ <strong>错误</strong>：避免过参数化模型
✅ <strong>正确</strong>：适当的过参数化+正则化often比小模型更好</p>
<p>❌ <strong>错误</strong>：期望理论完全解释实践
✅ <strong>正确</strong>：深度学习的成功仍有许多开放理论问题</p>
<h3 id="_9">调试技巧</h3>
<ol>
<li>
<p><strong>过拟合诊断</strong>：
   - 绘制学习曲线（训练/验证误差 vs 训练轮数）
   - 如果训练误差低但验证误差高 → 过拟合</p>
</li>
<li>
<p><strong>欠拟合诊断</strong>：
   - 训练误差和验证误差都高
   - 增加模型容量或减少正则化</p>
</li>
<li>
<p><strong>正则化调试</strong>：
   - 使用网格搜索或贝叶斯优化选择超参数
   - 监控有效自由度的变化</p>
</li>
<li>
<p><strong>样本量估计</strong>：
   - 绘制学习曲线（性能 vs 样本量）
   - 如果曲线仍在上升，需要更多数据</p>
</li>
</ol>
            </article>
            
            <nav class="page-nav"><a href="chapter1.html" class="nav-link prev">← 第1章：优化基础与梯度下降</a><a href="chapter3.html" class="nav-link next">第3章：线性模型与正则化 →</a></nav>
        </main>
    </div>
</body>
</html>