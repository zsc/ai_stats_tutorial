<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <base href="./">
    <title>现代人工智能：优化与统计视角</title>
    <link rel="stylesheet" href="assets/style.css">
    <link rel="stylesheet" href="assets/highlight.css">
    <script src="assets/script.js" defer></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$']],
                displayMath: [['$$', '$$']],
                processEscapes: false,
                packages: {'[+]': ['noerrors', 'ams']}
            },
            options: {
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            },
            loader: {
                load: ['[tex]/noerrors', '[tex]/ams']
            }
        };
    </script>
</head>
<body>
    <div class="container">
        <nav id="sidebar" class="sidebar">
            <div class="sidebar-header">
                <h3>目录</h3>
                <button id="sidebar-toggle" class="sidebar-toggle">
                    <span></span>
                    <span></span>
                    <span></span>
                </button>
            </div>
            <div class="sidebar-search">
                <input type="text" id="sidebar-search-input" placeholder="搜索..." autocomplete="off">
            </div>
            <div id="tree-container">
                <nav class="tree-nav" role="tree">
                    <div class="tree-item active" >
                        <a href="index.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">现代人工智能：优化与统计视角</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter1.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第1章：优化基础与梯度下降</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter2.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第2章：统计学习理论</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter3.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第3章：线性模型与正则化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter4.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第4章：神经网络基础</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter5.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第5章：深度学习优化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter6.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第6章：卷积神经网络</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter7.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第7章：循环神经网络与序列建模</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter8.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第8章：Transformer与注意力机制</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter9.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第9章：大语言模型</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter10.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第10章：变分自编码器与生成建模</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter11.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第11章：扩散模型</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter12.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第12章：深度强化学习</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="CLAUDE.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Untitled</span>
                        </a>
                    </div>
                </nav>
            </div>
        </nav>
        
        <main class="content">
            <article>
                <h1 id="_1">现代人工智能：优化与统计视角</h1>
<h2 id="_2">前言</h2>
<p>本书从优化理论和统计学的视角系统介绍现代人工智能的核心概念与方法。我们将理论见解与实用技巧相结合，帮助读者建立对AI技术的深刻理解。</p>
<p>每个章节都包含大量练习题，旨在巩固概念理解并培养实践直觉。本书适合AI初学者以及希望刷新知识体系的从业者。</p>
<h2 id="_3">目录</h2>
<h3 id="_4">第一部分：基础理论</h3>
<p><strong><a href="chapter1.html">第1章：优化基础与梯度下降</a></strong></p>
<ul>
<li>凸优化与非凸优化</li>
<li>梯度下降及其变体</li>
<li>收敛性分析</li>
<li>学习率选择的经验法则</li>
<li><strong>历史人物</strong>：柯西(Cauchy)与最速下降法的诞生</li>
<li><strong>现代连接</strong>：LLM训练中的梯度累积与混合精度优化</li>
</ul>
<p><strong><a href="chapter2.html">第2章：统计学习理论</a></strong></p>
<ul>
<li>偏差-方差权衡</li>
<li>PAC学习理论</li>
<li>VC维与泛化界</li>
<li>正则化的统计解释</li>
<li><strong>历史人物</strong>：瓦普尼克(Vapnik)与统计学习理论的建立</li>
<li><strong>现代连接</strong>：大模型的过参数化悖论与双下降现象</li>
</ul>
<p><strong><a href="chapter3.html">第3章：线性模型与正则化</a></strong></p>
<ul>
<li>最小二乘法与岭回归</li>
<li>LASSO与稀疏性</li>
<li>弹性网络</li>
<li>贝叶斯线性回归</li>
<li><strong>历史人物</strong>：高斯(Gauss)与最小二乘法的天文学应用</li>
<li><strong>现代连接</strong>：LoRA低秩适应在LLM微调中的应用</li>
</ul>
<h3 id="_5">第二部分：深度学习</h3>
<p><strong><a href="chapter4.html">第4章：神经网络基础</a></strong></p>
<ul>
<li>反向传播算法</li>
<li>激活函数的选择</li>
<li>初始化策略</li>
<li>梯度消失与爆炸问题</li>
<li><strong>历史人物</strong>：鲁梅尔哈特(Rumelhart)与反向传播的复兴</li>
<li><strong>现代连接</strong>：Transformer中的残差连接与层归一化设计</li>
</ul>
<p><strong><a href="chapter5.html">第5章：深度学习优化</a></strong></p>
<ul>
<li>SGD及其动量变体</li>
<li>Adam与自适应学习率</li>
<li>批归一化与层归一化</li>
<li>二阶优化方法</li>
<li><strong>历史人物</strong>：金马(Kingma)与Adam优化器的革命</li>
<li><strong>现代连接</strong>：LLM训练中的梯度检查点与ZeRO优化</li>
</ul>
<p><strong><a href="chapter6.html">第6章：卷积神经网络</a></strong></p>
<ul>
<li>卷积操作的统计解释</li>
<li>感受野与特征层次</li>
<li>残差连接与深度</li>
<li>数据增强策略</li>
<li><strong>历史人物</strong>：杨立昆(LeCun)与LeNet的手写识别突破</li>
<li><strong>现代连接</strong>：Vision Transformer与多模态LLM的视觉编码器</li>
</ul>
<h3 id="_6">第三部分：序列模型与注意力机制</h3>
<p><strong><a href="chapter7.html">第7章：循环神经网络与序列建模</a></strong></p>
<ul>
<li>BPTT与梯度问题</li>
<li>LSTM与GRU</li>
<li>序列到序列模型</li>
<li>束搜索与解码策略</li>
<li><strong>历史人物</strong>：霍克赖特(Hochreiter)与长短期记忆的发明</li>
<li><strong>现代连接</strong>：RNN在扩散模型的时间条件编码中的应用</li>
</ul>
<p><strong><a href="chapter8.html">第8章：Transformer与注意力机制</a></strong></p>
<ul>
<li>自注意力的数学原理</li>
<li>位置编码设计</li>
<li>多头注意力</li>
<li>计算复杂度分析</li>
<li><strong>历史人物</strong>：瓦斯瓦尼(Vaswani)与"Attention is All You Need"</li>
<li><strong>现代连接</strong>：Flash Attention与长上下文LLM的效率优化</li>
</ul>
<p><strong><a href="chapter9.html">第9章：大语言模型</a></strong></p>
<ul>
<li>GPT架构演进</li>
<li>缩放定律</li>
<li>上下文学习与提示工程</li>
<li>RLHF与对齐技术</li>
<li><strong>历史人物</strong>：拉德福德(Radford)与GPT的开创性工作</li>
<li><strong>现代连接</strong>：思维链推理与多模态理解的统一</li>
</ul>
<h3 id="_7">第四部分：生成模型</h3>
<p><strong><a href="chapter10.html">第10章：变分自编码器与生成建模</a></strong></p>
<ul>
<li>ELBO与变分推断</li>
<li>重参数化技巧</li>
<li>β-VAE与解耦表示</li>
<li>后验崩塌问题</li>
<li><strong>历史人物</strong>：金马(Kingma)与VAE的变分推断框架</li>
<li><strong>现代连接</strong>：VAE在LLM嵌入空间压缩中的应用</li>
</ul>
<p><strong><a href="chapter11.html">第11章：扩散模型</a></strong></p>
<ul>
<li>前向与反向扩散过程</li>
<li>去噪分数匹配</li>
<li>DDPM与DDIM</li>
<li>条件生成与引导</li>
<li><strong>历史人物</strong>：宋飏(Song)与分数生成模型的统一理论</li>
<li><strong>现代连接</strong>：文本到图像生成与CLIP引导</li>
</ul>
<h3 id="_8">第五部分：强化学习</h3>
<p><strong><a href="chapter12.html">第12章：深度强化学习</a></strong></p>
<ul>
<li>价值函数近似</li>
<li>策略梯度方法</li>
<li>Actor-Critic架构</li>
<li>AlphaGo与自我对弈</li>
<li>OpenAI Five与多智能体学习</li>
<li><strong>历史人物</strong>：萨顿(Sutton)与强化学习的现代框架</li>
<li><strong>现代连接</strong>：RLHF在LLM对齐与Constitutional AI中的应用</li>
</ul>
<h2 id="_9">数学符号约定</h2>
<ul>
<li>$\mathbf{x}$：向量</li>
<li>$\mathbf{X}$：矩阵</li>
<li>$\mathcal{D}$：数据集</li>
<li>$\mathbb{E}$：期望</li>
<li>$\mathcal{L}$：损失函数</li>
<li>$\nabla$：梯度</li>
<li>$\theta$：模型参数</li>
<li>$\eta$：学习率</li>
<li>$\lambda$：正则化系数</li>
</ul>
<h2 id="_10">如何使用本书</h2>
<ol>
<li><strong>循序渐进</strong>：章节按难度递增编排，建议按顺序学习</li>
<li><strong>动手练习</strong>：每章练习题分为基础题和挑战题，答案可展开查看</li>
<li><strong>关注直觉</strong>：重点理解概念背后的直觉，而非死记公式</li>
<li><strong>实用为主</strong>：书中提供大量经验法则(rules of thumb)，可直接应用于实践</li>
</ol>
<h2 id="_11">致谢</h2>
<p>感谢所有为现代AI发展做出贡献的研究者和工程师。本书试图将这些智慧结晶以易懂的方式呈现给中文读者。</p>
<hr />
<p>开始学习：<a href="chapter1.html">第1章：优化基础与梯度下降</a> →</p>
            </article>
            
            <nav class="page-nav"><a href="chapter1.html" class="nav-link next">第1章：优化基础与梯度下降 →</a></nav>
        </main>
    </div>
</body>
</html>