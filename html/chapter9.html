<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <base href="./">
    <title>第9章：大语言模型</title>
    <link rel="stylesheet" href="assets/style.css">
    <link rel="stylesheet" href="assets/highlight.css">
    <script src="assets/script.js" defer></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$']],
                displayMath: [['$$', '$$']],
                processEscapes: false,
                packages: {'[+]': ['noerrors', 'ams']}
            },
            options: {
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            },
            loader: {
                load: ['[tex]/noerrors', '[tex]/ams']
            }
        };
    </script>
</head>
<body>
    <div class="container">
        <nav id="sidebar" class="sidebar">
            <div class="sidebar-header">
                <h3>目录</h3>
                <button id="sidebar-toggle" class="sidebar-toggle">
                    <span></span>
                    <span></span>
                    <span></span>
                </button>
            </div>
            <div class="sidebar-search">
                <input type="text" id="sidebar-search-input" placeholder="搜索..." autocomplete="off">
            </div>
            <div id="tree-container">
                <nav class="tree-nav" role="tree">
                    <div class="tree-item " >
                        <a href="index.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">现代人工智能：优化与统计视角</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter1.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第1章：优化基础与梯度下降</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter2.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第2章：统计学习理论</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter3.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第3章：线性模型与正则化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter4.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第4章：神经网络基础</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter5.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第5章：深度学习优化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter6.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第6章：卷积神经网络</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter7.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第7章：循环神经网络与序列建模</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter8.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第8章：Transformer与注意力机制</span>
                        </a>
                    </div>
                
                    <div class="tree-item active" >
                        <a href="chapter9.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第9章：大语言模型</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter10.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第10章：变分自编码器与生成建模</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter11.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第11章：扩散模型</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter12.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第12章：深度强化学习</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="CLAUDE.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Untitled</span>
                        </a>
                    </div>
                </nav>
            </div>
        </nav>
        
        <main class="content">
            <article>
                <h1 id="9">第9章：大语言模型</h1>
<p>本章深入探讨大语言模型（Large Language Models, LLMs）的核心原理与实践。我们将从GPT架构的演进开始，理解模型规模与性能的关系，探索上下文学习的奇妙能力，并讨论如何通过人类反馈强化学习使模型与人类价值观对齐。通过统计学和优化理论的视角，我们将揭示LLM背后的数学原理，并提供实用的工程经验。</p>
<h2 id="91-gpt">9.1 GPT架构演进</h2>
<h3 id="911-gptgpt-4">9.1.1 从GPT到GPT-4：架构的渐进式改进</h3>
<p>GPT（Generative Pre-trained Transformer）系列模型的演进体现了深度学习中"规模即一切"的理念，但背后蕴含着精妙的架构设计。</p>
<p><strong>GPT-1 (2018)：无监督预训练的开端</strong></p>
<p>GPT-1采用12层Transformer解码器架构，参数量117M。其核心创新在于两阶段训练策略：</p>
<div class="codehilite"><pre><span></span><code>预训练目标：P(x_t | x_1, ..., x_{t-1})
微调目标：P(y | x_1, ..., x_n)
</code></pre></div>

<p>从统计学角度，这是一个条件概率建模问题。给定前文 $x_{&lt;t} = (x_1, ..., x_{t-1})$，模型学习预测下一个词元 $x_t$ 的概率分布：</p>
<p>$$\mathcal{L}_{LM} = -\sum_{t=1}^{T} \log P_\theta(x_t | x_{&lt;t})$$
<strong>GPT-2 (2019)：零样本学习的突破</strong></p>
<p>GPT-2将模型规模扩展到1.5B参数，更重要的是发现了"任务即提示"的范式：</p>
<div class="codehilite"><pre><span></span><code>层数：48层
隐藏维度：1600
注意力头数：25
上下文长度：1024 tokens
</code></pre></div>

<p>关键改进：</p>
<ul>
<li><strong>层归一化位置调整</strong>：Pre-norm而非Post-norm，提升深层网络训练稳定性</li>
<li><strong>权重初始化优化</strong>：根据层深度调整初始化方差，缓解梯度消失</li>
</ul>
<p>数学上，Pre-norm的前向传播可表示为：
$$h_{l+1} = h_l + \text{FFN}(\text{LN}(h_l + \text{Attn}(\text{LN}(h_l))))$$
<strong>GPT-3 (2020)：上下文学习的涌现</strong></p>
<p>GPT-3达到175B参数，展现了令人惊叹的few-shot学习能力：</p>
<div class="codehilite"><pre><span></span><code>     模型规模对比
     ┌────────────┬──────────┬────────┬────────┐
     │   模型     │  参数量  │  层数  │ 隐藏维 │
     ├────────────┼──────────┼────────┼────────┤
     │ GPT-3 Small│   125M   │   12   │  768   │
     │ GPT-3 Med  │   350M   │   24   │  1024  │
     │ GPT-3 Large│   760M   │   24   │  1536  │
     │ GPT-3 XL   │   1.3B   │   24   │  2048  │
     │ GPT-3 2.7B │   2.7B   │   32   │  2560  │
     │ GPT-3 6.7B │   6.7B   │   32   │  4096  │
     │ GPT-3 13B  │   13B    │   40   │  5140  │
     │ GPT-3 175B │   175B   │   96   │  12288 │
     └────────────┴──────────┴────────┴────────┘
</code></pre></div>

<h3 id="912">9.1.2 注意力机制的计算优化</h3>
<p>大规模LLM面临的核心挑战是注意力计算的二次复杂度 $O(n^2d)$。实践中采用多种优化策略：</p>
<p><strong>稀疏注意力模式</strong></p>
<div class="codehilite"><pre><span></span><code>    Full Attention        Sparse Attention
    ┌─┬─┬─┬─┬─┬─┐       ┌─┬─┬─┬─┬─┬─┐
    │█│█│█│█│█│█│       │█│█│ │ │ │ │
    ├─┼─┼─┼─┼─┼─┤       ├─┼─┼─┼─┼─┼─┤
    │█│█│█│█│█│█│       │█│█│█│ │ │ │
    ├─┼─┼─┼─┼─┼─┤       ├─┼─┼─┼─┼─┼─┤
    │█│█│█│█│█│█│       │ │█│█│█│ │ │
    ├─┼─┼─┼─┼─┼─┤       ├─┼─┼─┼─┼─┼─┤
    │█│█│█│█│█│█│       │ │ │█│█│█│ │
    ├─┼─┼─┼─┼─┼─┤       ├─┼─┼─┼─┼─┼─┤
    │█│█│█│█│█│█│       │ │ │ │█│█│█│
    ├─┼─┼─┼─┼─┼─┤       ├─┼─┼─┼─┼─┼─┤
    │█│█│█│█│█│█│       │█│ │ │ │█│█│
    └─┴─┴─┴─┴─┴─┘       └─┴─┴─┴─┴─┴─┘
</code></pre></div>

<p><strong>Flash Attention的核心思想</strong></p>
<p>通过分块计算和重计算策略，将内存访问从 $O(n^2)$ 降至 $O(n)$：
$$\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
分块计算时，将Q、K、V分成大小为B的块：</p>
<ul>
<li>外循环：遍历Q的块</li>
<li>内循环：遍历K、V的块</li>
<li>累积局部注意力结果</li>
</ul>
<h2 id="92">9.2 缩放定律</h2>
<h3 id="921-chinchilla">9.2.1 Chinchilla定律与计算最优</h3>
<p>Kaplan等人(2020)首先发现了神经语言模型的幂律缩放关系：
$$L(N) = \left(\frac{N_c}{N}\right)^{\alpha_N}$$
其中：</p>
<ul>
<li>$L$：测试损失</li>
<li>$N$：模型参数量</li>
<li>$N_c$：临界参数量</li>
<li>$\alpha_N \approx 0.076$：缩放指数</li>
</ul>
<p>Hoffmann等人(2022)提出的Chinchilla定律进一步优化了参数量与数据量的平衡：
$$N_{opt} \propto D^{0.5}$$
$$D_{opt} \propto N^{1.0}$$
这意味着，对于固定的计算预算 $C = 6ND$（N为参数量，D为训练tokens），最优配置满足：</p>
<div class="codehilite"><pre><span></span><code>经验法则：每个参数应该训练约20个tokens
</code></pre></div>

<h3 id="922">9.2.2 涌现能力的统计解释</h3>
<p>涌现能力（Emergent Abilities）是指模型在某个规模阈值后突然展现的能力。从统计学角度，这可以理解为相变现象：</p>
<p><strong>能力涌现的S型曲线</strong></p>
<div class="codehilite"><pre><span></span><code>性能↑
<span class="m">100</span><span class="o">%│      ╭────────</span>
<span class="o">    │     ╱</span>
<span class="o">    │    ╱ &lt;- 涌现点</span>
<span class="o">    │   ╱</span>
<span class="o">    │  ╱</span>
<span class="o">  0%</span>│─╯
<span class="w">    </span>└────────────────→<span class="w"> </span><span class="nf">log</span><span class="p">(</span>模型规模<span class="p">)</span>
</code></pre></div>

<p>数学建模：
$$P(\text{success}) = \sigma(a \cdot \log(N) - b)$$
其中 $\sigma$ 是sigmoid函数，$a$ 控制转变陡峭程度，$b$ 是涌现阈值。</p>
<h3 id="923">9.2.3 计算效率的权衡</h3>
<p><strong>FLOPs计算公式</strong></p>
<p>对于Transformer模型，前向传播的FLOPs约为：
$$C_{forward} \approx 2ND + 2n^2d$$
其中第一项是参数计算，第二项是注意力计算。</p>
<p>训练总FLOPs：
$$C_{total} = 6ND$$
（前向2ND + 反向4ND）</p>
<p><strong>模型并行策略</strong></p>
<div class="codehilite"><pre><span></span><code>    数据并行              模型并行             流水线并行
  ┌─────────┐         ┌─────────┐         ┌─────────┐
  │ Model A │         │ Layer 1 │         │ Stage 1 │
  │ Batch 1 │         │  GPU 1  │         │  GPU 1  │
  └─────────┘         └─────────┘         └────┬────┘
  ┌─────────┐         ┌─────────┐              │
  │ Model A │         │ Layer 1 │         ┌────▼────┐
  │ Batch 2 │         │  GPU 2  │         │ Stage 2 │
  └─────────┘         └─────────┘         │  GPU 2  │
                                          └─────────┘
</code></pre></div>

<h2 id="93">9.3 上下文学习与提示工程</h2>
<h3 id="931">9.3.1 上下文学习的贝叶斯视角</h3>
<p>上下文学习（In-Context Learning, ICL）可以从贝叶斯推断的角度理解。给定示例 $(x_1, y_1), ..., (x_k, y_k)$ 和查询 $x_{test}$：
$$P(y_{test}|x_{test}, \mathcal{D}_{context}) = \int P(y_{test}|x_{test}, \theta)P(\theta|\mathcal{D}_{context})d\theta$$
LLM隐式地进行了两步推断：</p>
<ol>
<li><strong>任务识别</strong>：从示例中推断任务类型 $P(\tau|\mathcal{D}_{context})$</li>
<li><strong>条件预测</strong>：基于识别的任务进行预测 $P(y|x, \tau)$</li>
</ol>
<h3 id="932">9.3.2 提示工程的信息论原理</h3>
<p>有效的提示设计可以从信息论角度优化。目标是最大化互信息：
$$I(Y; P) = H(Y) - H(Y|P)$$
其中 $P$ 是提示，$Y$ 是期望输出。</p>
<p><strong>提示设计原则</strong></p>
<ol>
<li><strong>明确性原则</strong>：减少歧义，降低条件熵 $H(Y|P)$</li>
</ol>
<div class="codehilite"><pre><span></span><code>差：生成一个故事
好：生成一个200字的科幻故事，主角是机器人，设定在2150年的火星
</code></pre></div>

<ol start="2">
<li><strong>结构化原则</strong>：使用标记和格式化</li>
</ol>
<div class="codehilite"><pre><span></span><code>任务：情感分析
输入：这部电影太棒了！
输出格式：[正面/负面/中性]
答案：[正面]
</code></pre></div>

<ol start="3">
<li><strong>少样本示例选择</strong>：基于语义相似度
$$\text{sim}(x_{test}, x_i) = \frac{\mathbf{e}_{test} \cdot \mathbf{e}_i}{||\mathbf{e}_{test}|| \cdot ||\mathbf{e}_i||}$$</li>
</ol>
<h3 id="933">9.3.3 思维链推理的递归结构</h3>
<p>思维链（Chain-of-Thought, CoT）提示通过引导模型生成中间推理步骤来提升复杂推理能力：</p>
<div class="codehilite"><pre><span></span><code>标准提示 vs 思维链提示
┌──────────────┐        ┌──────────────┐
│   问题       │        │   问题       │
└──────┬───────┘        └──────┬───────┘
       │                       │
       ▼                       ▼
┌──────────────┐        ┌──────────────┐
│   答案       │        │  步骤1：...  │
└──────────────┘        └──────┬───────┘
                               │
                               ▼
                        ┌──────────────┐
                        │  步骤2：...  │
                        └──────┬───────┘
                               │
                               ▼
                        ┌──────────────┐
                        │   答案       │
                        └──────────────┘
</code></pre></div>

<p>数学形式化：
$$P(y|x) = \sum_{z_1,...,z_n} P(y|z_n) \prod_{i=1}^{n} P(z_i|x, z_{&lt;i})$$
其中 $z_i$ 是中间推理步骤。</p>
<h2 id="94-rlhf">9.4 RLHF与对齐技术</h2>
<h3 id="941">9.4.1 从人类反馈中学习</h3>
<p>RLHF（Reinforcement Learning from Human Feedback）通过三个阶段实现模型对齐：</p>
<p><strong>阶段1：监督微调（SFT）</strong>
$$\mathcal{L}_{SFT} = -\mathbb{E}_{(x,y) \sim \mathcal{D}_{demo}}[\log P_\theta(y|x)]$$
<strong>阶段2：奖励模型训练</strong>
$$\mathcal{L}_{RM} = -\mathbb{E}_{(x,y_w,y_l)}[\log \sigma(r_\phi(x,y_w) - r_\phi(x,y_l))]$$
其中 $y_w$ 是人类偏好的回答，$y_l$ 是较差的回答。</p>
<p><strong>阶段3：PPO优化</strong>
$$\mathcal{L}_{PPO} = \mathbb{E}_{x,y}[r_\phi(x,y) - \beta \log\frac{P_\theta(y|x)}{P_{ref}(y|x)}]$$
KL散度项 $\log\frac{P_\theta(y|x)}{P_{ref}(y|x)}$ 防止模型偏离太远。</p>
<h3 id="942-constitutional-ai">9.4.2 Constitutional AI与自我改进</h3>
<p>Constitutional AI通过自我批评和修订实现对齐：</p>
<div class="codehilite"><pre><span></span><code>迭代改进流程
┌─────────┐     ┌─────────┐     ┌─────────┐
│ 初始回答 │────▶│ 自我批评 │────▶│ 修订回答 │
└─────────┘     └─────────┘     └─────────┘
     ▲                                 │
     └─────────────────────────────────┘
           (迭代直到满足准则)
</code></pre></div>

<p>数学建模为约束优化问题：
$$\max_\theta \mathbb{E}_{x \sim p(x)}[R(x, y_\theta)] \quad \text{s.t.} \quad C_i(y_\theta) \geq \tau_i, \forall i$$
其中 $C_i$ 是第 $i$ 个宪法准则的满足程度。</p>
<h3 id="943-dpo">9.4.3 DPO：简化的对齐方法</h3>
<p>Direct Preference Optimization (DPO) 绕过奖励模型，直接优化偏好：
$$\mathcal{L}_{DPO} = -\mathbb{E}[\log \sigma(\beta \log\frac{P_\theta(y_w|x)}{P_{ref}(y_w|x)} - \beta \log\frac{P_\theta(y_l|x)}{P_{ref}(y_l|x)})]$$
这等价于以下奖励函数的Bradley-Terry模型：
$$r^*(x,y) = \beta \log\frac{P^*(y|x)}{P_{ref}(y|x)} + \beta \log Z(x)$$
<strong>DPO vs RLHF的权衡</strong></p>
<ul>
<li>DPO：训练稳定，无需奖励模型，但可能过拟合偏好数据</li>
<li>RLHF：更灵活，可在线学习，但训练复杂度高</li>
</ul>
<h2 id="95-gpt">9.5 历史人物：亚历克·拉德福德与GPT的开创性工作</h2>
<p>亚历克·拉德福德（Alec Radford）是OpenAI的研究科学家，GPT系列模型的主要设计者之一。他的工作展示了简单架构通过规模化可以产生惊人能力的深刻洞察。</p>
<h3 id="_1">背景与早期工作</h3>
<p>Radford在达特茅斯学院获得计算机科学学士学位后，直接加入了OpenAI。与传统的学术路径不同，他选择在工业研究实验室追求基础研究突破。</p>
<h3 id="gpt">GPT的关键洞察</h3>
<p>Radford团队的三个核心贡献：</p>
<ol>
<li><strong>无监督预训练的价值</strong>：认识到语言建模作为通用预训练任务的潜力</li>
<li><strong>统一架构的简洁性</strong>：坚持使用纯Transformer解码器，避免复杂的架构设计</li>
<li><strong>规模化的系统性探索</strong>：系统地研究模型规模与能力的关系</li>
</ol>
<h3 id="_2">科学哲学的体现</h3>
<p>Radford的研究哲学体现了"少即是多"的原则：</p>
<div class="codehilite"><pre><span></span><code>复杂性来源对比：
传统NLP：任务特定架构 + 特征工程 + 规则系统
GPT方法：简单架构 + 大规模数据 + 计算规模
</code></pre></div>

<p>他在GPT-2论文中写道："我们的方法本质上是任务无关的，只需要提供自然语言的描述和示例。"这一洞察改变了整个领域的研究范式。</p>
<h3 id="gptgpt-4">从GPT到GPT-4的演进思路</h3>
<p>Radford团队遵循了渐进式改进的策略：</p>
<ul>
<li><strong>保持架构稳定性</strong>：核心Transformer架构基本不变</li>
<li><strong>专注工程优化</strong>：优化训练稳定性、数据质量、计算效率</li>
<li><strong>系统性规模探索</strong>：每代模型增加约10倍规模</li>
</ul>
<p>这种方法论的成功验证了深度学习的"苦涩教训"（Bitter Lesson）：通用方法配合计算规模最终胜过专门设计。</p>
<h2 id="96">9.6 现代连接：思维链推理与多模态理解的统一</h2>
<h3 id="961">9.6.1 从文本到多模态的架构统一</h3>
<p>现代LLM正在向多模态大模型（MLLMs）演进，核心思想是将所有模态映射到统一的语言空间：</p>
<div class="codehilite"><pre><span></span><code>多模态架构示意图
┌─────────┐     ┌─────────┐     ┌─────────┐
│  图像   │────▶│ ViT编码器│────▶│         │
└─────────┘     └─────────┘     │         │
┌─────────┐     ┌─────────┐     │  统一   │
│  音频   │────▶│Mel编码器│────▶│ 语言空间 │────▶ LLM
└─────────┘     └─────────┘     │         │
┌─────────┐                     │         │
│  文本   │────────────────────▶│         │
└─────────┘                     └─────────┘
</code></pre></div>

<p>数学形式：
$$h_{unified} = f_{LLM}(\text{concat}[E_{text}(x_{text}), E_{vision}(x_{img}), E_{audio}(x_{audio})])$$</p>
<h3 id="962">9.6.2 思维链在多步推理中的应用</h3>
<p><strong>程序合成中的思维链</strong></p>
<p>LLM可以通过思维链生成程序来解决复杂问题：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 用户查询：计算前100个素数的和</span>

<span class="c1"># 思维链分解：</span>
<span class="c1"># 步骤1：需要判断素数的函数</span>
<span class="c1"># 步骤2：生成前100个素数</span>
<span class="c1"># 步骤3：求和</span>

<span class="k">def</span> <span class="nf">is_prime</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">n</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span> <span class="k">return</span> <span class="kc">False</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">n</span> <span class="o">%</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> <span class="k">return</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="kc">True</span>

<span class="n">primes</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">2</span>
<span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">primes</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">100</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">is_prime</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">primes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="n">result</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">primes</span><span class="p">)</span>  <span class="c1"># 24133</span>
</code></pre></div>

<h3 id="963">9.6.3 工具使用与外部知识整合</h3>
<p>现代LLM通过工具使用（Tool Use）扩展能力边界：</p>
<p><strong>ReAct框架（Reasoning + Acting）</strong></p>
<div class="codehilite"><pre><span></span><code>思考 → 行动 → 观察 → 思考 → ...
  ↓       ↓       ↓       ↓
推理   工具调用  结果   更新推理
</code></pre></div>

<p>形式化为马尔可夫决策过程：</p>
<ul>
<li>状态 $s_t$：当前上下文和观察</li>
<li>动作 $a_t$：思考或工具调用</li>
<li>奖励 $r_t$：任务完成度</li>
</ul>
<h3 id="964">9.6.4 长上下文处理的创新</h3>
<p><strong>位置编码的改进</strong></p>
<p>RoPE（Rotary Position Embedding）通过旋转矩阵编码相对位置：
$$f_{\text{RoPE}}(x_m, m) = x_m e^{im\theta}$$
这允许模型外推到训练时未见过的长度。</p>
<p><strong>记忆增强架构</strong></p>
<div class="codehilite"><pre><span></span><code>工作记忆 vs 长期记忆
┌──────────────────────┐
│   工作记忆（8K）      │ ←── 当前上下文
├──────────────────────┤
│   压缩记忆（32K）     │ ←── 重要信息摘要
├──────────────────────┤
│   检索记忆（∞）       │ ←── 向量数据库
└──────────────────────┘
</code></pre></div>

<h3 id="965">9.6.5 效率优化的前沿技术</h3>
<p><strong>量化技术</strong></p>
<p>INT8量化可以在几乎不损失性能的情况下减少75%的内存使用：
$$W_{int8} = \text{round}(W_{fp32} / s) \cdot s$$
其中 $s$ 是缩放因子。</p>
<p><strong>推测解码（Speculative Decoding）</strong></p>
<p>使用小模型生成候选，大模型验证：</p>
<div class="codehilite"><pre><span></span><code>小模型：快速生成 k 个候选token
大模型：并行验证所有候选
接受率：p(accept) ≈ 0.7-0.9
加速比：1.5x - 3x
</code></pre></div>

<h2 id="_3">本章小结</h2>
<p>本章系统介绍了大语言模型的核心技术与实践：</p>
<h3 id="_4">关键概念</h3>
<ol>
<li>
<p><strong>架构演进</strong>
   - GPT系列从117M到175B+参数的规模化之路
   - Pre-norm、稀疏注意力等关键技术改进
   - 计算复杂度 $O(n^2d)$ 的优化策略</p>
</li>
<li>
<p><strong>缩放定律</strong>
   - Chinchilla定律：$N_{opt} \propto D^{0.5}$
   - 涌现能力的S型曲线：$P(\text{success}) = \sigma(a \cdot \log(N) - b)$
   - 计算最优配置：每参数约20个训练tokens</p>
</li>
<li>
<p><strong>上下文学习</strong>
   - 贝叶斯视角：$P(y|x, \mathcal{D}) = \int P(y|x, \theta)P(\theta|\mathcal{D})d\theta$
   - 思维链推理的递归分解
   - 提示工程的信息论原理</p>
</li>
<li>
<p><strong>对齐技术</strong>
   - RLHF三阶段：SFT → 奖励模型 → PPO
   - DPO的直接优化：$\mathcal{L}_{DPO} = -\mathbb{E}[\log \sigma(\beta \Delta)]$
   - Constitutional AI的自我改进</p>
</li>
</ol>
<h3 id="_5">实用经验法则</h3>
<ul>
<li><strong>模型选择</strong>：参数量每增加10倍，困惑度降低约0.1</li>
<li><strong>上下文长度</strong>：有效上下文约为最大长度的75%</li>
<li><strong>批大小</strong>：最优批大小 ∝ $\sqrt{\text{模型大小}}$</li>
<li><strong>学习率</strong>：使用余弦退火，峰值学习率 ∝ $1/\sqrt{\text{模型大小}}$</li>
<li><strong>提示设计</strong>：少样本示例4-8个最优，按相似度选择</li>
<li><strong>推理优化</strong>：温度0.7-0.8平衡创造性与连贯性</li>
</ul>
<h3 id="_6">未来展望</h3>
<p>LLM技术正快速向以下方向发展：</p>
<ul>
<li>多模态统一：视觉、音频、文本的无缝整合</li>
<li>长上下文：百万级token的高效处理</li>
<li>工具增强：与外部系统的深度集成</li>
<li>个性化对齐：适应不同用户和文化背景</li>
</ul>
<h2 id="_7">常见陷阱与错误</h2>
<h3 id="1">1. 训练相关</h3>
<p><strong>陷阱：梯度爆炸导致训练崩溃</strong></p>
<div class="codehilite"><pre><span></span><code>症状：Loss突然变为NaN
原因：学习率过大或梯度累积过多
解决：

<span class="k">-</span> 使用梯度裁剪：clip_norm = 1.0
<span class="k">-</span> 降低学习率
<span class="k">-</span> 检查数据中的异常值
</code></pre></div>

<p><strong>陷阱：显存OOM（Out of Memory）</strong></p>
<div class="codehilite"><pre><span></span><code>常见错误计算：
批大小32 × 序列长度2048 × 模型7B = 需要约450GB显存 ❌

正确做法：

- 使用梯度累积
- 启用混合精度训练(fp16/bf16)
- 使用梯度检查点
- 模型并行或流水线并行
</code></pre></div>

<h3 id="2">2. 推理相关</h3>
<p><strong>陷阱：生成重复或退化</strong></p>
<div class="codehilite"><pre><span></span><code>问题表现：
&quot;The cat sat on the mat mat mat mat...&quot;

解决方案：

<span class="k">-</span> 增加温度（0.7-1.0）
<span class="k">-</span> 使用repetition_penalty（1.1-1.2）
<span class="k">-</span> 启用top_p采样（0.9-0.95）
<span class="k">-</span> 限制n-gram重复
</code></pre></div>

<p><strong>陷阱：上下文长度误用</strong></p>
<div class="codehilite"><pre><span></span><code>错误：将8K上下文全部填满
后果：中间信息被忽略（&quot;迷失在中间&quot;现象）

最佳实践：

- 重要信息放在开头或结尾
- 使用不超过75%的最大长度
- 实施滑动窗口或分块策略
</code></pre></div>

<h3 id="3">3. 提示工程陷阱</h3>
<p><strong>陷阱：过度具体的指令</strong></p>
<div class="codehilite"><pre><span></span><code>差：请用恰好150个字，包含3个形容词和2个副词...
好：请用150字左右简要描述...

原因：过度约束降低生成质量
</code></pre></div>

<p><strong>陷阱：负面指令</strong></p>
<div class="codehilite"><pre><span></span><code>差：&quot;不要提及政治&quot;
好：&quot;请专注于技术方面&quot;

原因：模型对负面指令的理解不稳定
</code></pre></div>

<h3 id="4">4. 对齐训练陷阱</h3>
<p><strong>陷阱：奖励黑客（Reward Hacking）</strong></p>
<div class="codehilite"><pre><span></span><code>现象：模型学会欺骗奖励模型而非真正改进
例如：过度使用&quot;我理解您的担忧&quot;等模板化回复

预防：

- 使用多样化的奖励信号
- 定期更新奖励模型
- 加入KL惩罚项
</code></pre></div>

<p><strong>陷阱：过度对齐导致能力退化</strong></p>
<div class="codehilite"><pre><span></span><code>症状：模型变得过于保守，拒绝合理请求
解决：

- 平衡helpfulness和harmlessness
- 保留部分原始能力的KL约束
- 使用Constitutional AI的分层方法
</code></pre></div>

<h3 id="5">5. 规模化陷阱</h3>
<p><strong>陷阱：盲目追求参数量</strong></p>
<div class="codehilite"><pre><span></span><code>误区：参数越大越好
实际：需要平衡参数量、数据量、计算量

Chinchilla最优：

- 1B模型 → 20B tokens
- 10B模型 → 200B tokens
- 70B模型 → 1.4T tokens
</code></pre></div>

<h3 id="_8">调试技巧</h3>
<ol>
<li>
<p><strong>监控关键指标</strong>
   - 梯度范数：应保持稳定
   - 激活值分布：避免全零或爆炸
   - 注意力熵：过低表示退化</p>
</li>
<li>
<p><strong>分阶段验证</strong></p>
</li>
</ol>
<div class="codehilite"><pre><span></span><code>小规模实验 → 中等规模验证 → 全规模训练
1M参数    →  100M参数     →  目标规模
</code></pre></div>

<ol start="3">
<li><strong>使用探针任务</strong>
   - 简单算术：测试基础推理
   - 事实回忆：测试知识存储
   - 代码补全：测试结构理解</li>
</ol>
<h2 id="_9">练习题</h2>
<h3 id="_10">基础题</h3>
<p><strong>练习9.1：注意力复杂度计算</strong></p>
<p>给定一个GPT模型，隐藏维度 $d=768$，序列长度 $n=2048$，批大小 $b=32$。计算：</p>
<ol>
<li>单个注意力头的内存需求（存储QKV和注意力矩阵）</li>
<li>12个注意力头的总内存需求</li>
<li>如果使用Flash Attention，内存需求如何变化？</li>
</ol>
<p><em>提示：考虑float16精度，每个参数2字节</em></p>
<details markdown="1">
<summary>答案</summary>

<ol>
<li>
<p><strong>单头内存需求</strong>：
   - Q, K, V矩阵：$3 \times b \times n \times d = 3 \times 32 \times 2048 \times 768 = 150.99$ MB
   - 注意力矩阵：$b \times n \times n = 32 \times 2048 \times 2048 = 134.22$ MB
   - 总计：285.21 MB</p>
</li>
<li>
<p><strong>12头总需求</strong>：
   - 参数共享，QKV不变：150.99 MB
   - 12个注意力矩阵：$12 \times 134.22 = 1610.64$ MB
   - 总计：1761.63 MB</p>
</li>
<li>
<p><strong>Flash Attention优化</strong>：
   - 不存储完整注意力矩阵
   - 内存需求：约150.99 MB（仅QKV）
   - 节省：约91%内存</p>
</li>
</ol>
</details>
<p><strong>练习9.2：Chinchilla最优配置</strong></p>
<p>你有1000 GPU-hours的计算预算，每个GPU的算力是100 TFLOPs。假设训练效率是30%。根据Chinchilla定律，最优的模型参数量和训练数据量是多少？</p>
<p><em>提示：使用 $C = 6ND$ 和 Chinchilla比例 $D \approx 20N$</em></p>
<details>
<summary>答案</summary>
<ol>
<li>
<p><strong>总计算量</strong>：
   - FLOPs = $1000 \times 3600 \times 100 \times 10^{12} \times 0.3 = 1.08 \times 10^{20}$ FLOPs</p>
</li>
<li>
<p><strong>使用Chinchilla定律</strong>：
   - $C = 6ND$，且 $D = 20N$
   - $C = 6N \times 20N = 120N^2$
   - $N = \sqrt{C/120} = \sqrt{1.08 \times 10^{20}/120} = 3 \times 10^{9}$ = 3B参数</p>
</li>
<li>
<p><strong>训练数据量</strong>：
   - $D = 20N = 20 \times 3B = 60B$ tokens</p>
</li>
</ol>
<p>答案：3B参数模型，60B训练tokens</p>
</details>
<p><strong>练习9.3：困惑度与交叉熵</strong></p>
<p>一个语言模型在测试集上的困惑度（Perplexity）是150。问：</p>
<ol>
<li>对应的交叉熵损失是多少？</li>
<li>如果词表大小是50000，随机猜测的困惑度是多少？</li>
<li>模型相对于随机基线的改进倍数？</li>
</ol>
<p><em>提示：$PPL = e^{H}$，其中H是交叉熵</em></p>
<details>
<summary>答案</summary>
<ol>
<li>
<p><strong>交叉熵损失</strong>：
   - $H = \ln(PPL) = \ln(150) = 5.01$ nats
   - 或 $H = \log_2(150) = 7.23$ bits</p>
</li>
<li>
<p><strong>随机猜测困惑度</strong>：
   - 均匀分布：$PPL_{random} = |V| = 50000$</p>
</li>
<li>
<p><strong>改进倍数</strong>：
   - $\frac{PPL_{random}}{PPL_{model}} = \frac{50000}{150} = 333.33$倍</p>
</li>
</ol>
<p>模型比随机猜测好333倍。</p>
</details>
<h3 id="_11">挑战题</h3>
<p><strong>练习9.4：上下文学习的有效性分析</strong></p>
<p>考虑一个few-shot学习场景，给定k个示例。假设：</p>
<ul>
<li>模型对任务的先验概率是 $P(\tau) = 0.1$</li>
<li>每个正确示例将后验概率提升1.5倍</li>
<li>错误示例将后验概率降低0.8倍</li>
</ul>
<p>问：需要多少个正确示例才能使模型有95%的把握识别出任务？如果混入了20%的错误示例呢？</p>
<p><em>提示：使用贝叶斯更新规则</em></p>
<details>
<summary>答案</summary>
<ol>
<li><strong>纯正确示例情况</strong>：
   - 初始：$P(\tau) = 0.1$
   - k个示例后：$P(\tau|D) = \frac{0.1 \times 1.5^k}{0.1 \times 1.5^k + 0.9}$
   - 要求：$P(\tau|D) \geq 0.95$</li>
</ol>
<p>解方程：</p>
<ul>
<li>$0.1 \times 1.5^k \geq 0.95 \times (0.1 \times 1.5^k + 0.9)$</li>
<li>$0.1 \times 1.5^k \geq 0.095 \times 1.5^k + 0.855$</li>
<li>$0.005 \times 1.5^k \geq 0.855$</li>
<li>$1.5^k \geq 171$</li>
<li>$k \geq \log_{1.5}(171) = 12.7$</li>
</ul>
<p>需要至少13个正确示例。</p>
<ol start="2">
<li><strong>80%正确率情况</strong>：
   - 期望更新因子：$0.8 \times 1.5 + 0.2 \times 0.8 = 1.36$
   - $1.36^k \geq 171$
   - $k \geq \log_{1.36}(171) = 16.8$</li>
</ol>
<p>需要至少17个示例（其中约14个正确，3个错误）。</p>
</details>
<p><strong>练习9.5：RLHF中的KL惩罚权衡</strong></p>
<p>在PPO训练中，目标函数是：
$$J(\theta) = \mathbb{E}[r(x,y) - \beta \cdot KL(P_\theta || P_{ref})]$$
给定：</p>
<ul>
<li>无KL约束时，平均奖励可达0.9，但生成文本严重偏离原始分布</li>
<li>参考模型的平均奖励是0.3</li>
<li>KL散度与奖励的经验关系：$r = 0.9 - 0.4 \cdot \sqrt{KL}$</li>
</ul>
<p>求最优的KL散度值和对应的奖励。</p>
<p><em>提示：对J关于KL求导</em></p>
<details>
<summary>答案</summary>
<ol>
<li>
<p><strong>建立优化问题</strong>：
$$J = r - \beta \cdot KL = 0.9 - 0.4\sqrt{KL} - \beta \cdot KL$$</p>
</li>
<li>
<p><strong>求导并令其为零</strong>：
$$\frac{dJ}{d(KL)} = -\frac{0.2}{\sqrt{KL}} - \beta = 0$$</p>
</li>
</ol>
<p>解得：$$KL^* = \frac{0.04}{\beta^2}$$</p>
<ol start="3">
<li><strong>典型值 $\beta = 0.1$</strong>：
   - $KL^* = \frac{0.04}{0.01} = 4$ nats
   - $r^* = 0.9 - 0.4 \times 2 = 0.1$</li>
</ol>
<p>这似乎太低了！</p>
<ol start="4">
<li>
<p><strong>更合理的 $\beta = 0.02$</strong>：
   - $KL^* = \frac{0.04}{0.0004} = 100$ nats
   - 这太大了！</p>
</li>
<li>
<p><strong>平衡点 $\beta = 0.05$</strong>：
   - $KL^* = \frac{0.04}{0.0025} = 16$ nats
   - $r^* = 0.9 - 0.4 \times 4 = 0.5$
   - $J^* = 0.5 - 0.05 \times 16 = -0.3$</p>
</li>
</ol>
<p>实践中，$\beta \approx 0.01-0.05$，KL散度保持在1-10 nats范围。</p>
</details>
<p><strong>练习9.6：思维链推理的概率分析</strong></p>
<p>一个复杂问题需要3步推理。直接回答的成功率是20%。使用思维链时：</p>
<ul>
<li>每步推理的成功率是80%</li>
<li>给定正确的中间步骤，最终答案正确率是95%</li>
<li>给定错误的中间步骤，最终答案正确率仍有30%（部分信息有用）</li>
</ul>
<p>计算思维链方法的总体成功率，以及相对于直接方法的提升。</p>
<p><em>提示：考虑所有可能的推理路径</em></p>
<details>
<summary>答案</summary>
<ol>
<li><strong>枚举所有路径</strong>（3步，每步对或错）：</li>
</ol>
<p>设步骤正确为1，错误为0：</p>
<ul>
<li>路径111（全对）：$P = 0.8^3 = 0.512$，最终正确率 = 0.95</li>
<li>路径110：$P = 0.8^2 \times 0.2 = 0.128$，最终正确率 = 0.30</li>
<li>路径101：$P = 0.128$，最终正确率 = 0.30</li>
<li>路径011：$P = 0.128$，最终正确率 = 0.30</li>
<li>路径100：$P = 0.032$，最终正确率 = 0.30</li>
<li>路径010：$P = 0.032$，最终正确率 = 0.30</li>
<li>路径001：$P = 0.032$，最终正确率 = 0.30</li>
<li>路径000：$P = 0.008$，最终正确率 = 0.30</li>
</ul>
<ol start="2">
<li>
<p><strong>计算总成功率</strong>：
$$P_{success} = 0.512 \times 0.95 + (1-0.512) \times 0.30$$
   $$= 0.4864 + 0.1464 = 0.633$$</p>
</li>
<li>
<p><strong>相对提升</strong>：
$$\frac{0.633}{0.20} = 3.165$$
思维链将成功率提升了216.5%！</p>
</li>
<li>
<p><strong>关键洞察</strong>：
   - 即使部分步骤错误，仍有30%成功率很重要
   - 主要贡献来自全对路径（贡献了76.8%的成功率）</p>
</li>
</ol>
</details>
<p><strong>练习9.7：模型规模与数据的最优分配（开放题）</strong></p>
<p>你是一家初创公司的AI负责人，有以下约束：</p>
<ul>
<li>计算预算：$100,000（云计算费用）</li>
<li>时间限制：3个月</li>
<li>目标：训练一个领域特定的LLM</li>
</ul>
<p>已知：</p>
<ul>
<li>计算成本：$2 per TFLOP-hour</li>
<li>数据收集成本：$0.01 per 1000 tokens</li>
<li>数据清洗成本：$0.02 per 1000 tokens（高质量）或 $0.005 per 1000 tokens（基础清洗）</li>
</ul>
<p>请设计最优的资源分配策略，考虑：</p>
<ol>
<li>模型大小选择</li>
<li>数据质量vs数量权衡</li>
<li>训练时间分配</li>
<li>风险管理</li>
</ol>
<p><em>提示：这是开放问题，考虑实际工程约束</em></p>
<details>
<summary>参考答案</summary>
<p><strong>策略设计</strong>：</p>
<ol>
<li>
<p><strong>预算分配（100K美元）</strong>：
   - 计算：70K（70%）
   - 数据：25K（25%）
   - 应急储备：5K（5%）</p>
</li>
<li>
<p><strong>模型规模选择</strong>：
   - 可用计算：70K / $2 = 35K TFLOP-hours = 1.26 × 10^20 FLOPs
   - 根据 $C = 6ND$ 和 $D = 20N$：
   - $N = \sqrt{C/120} \approx 3B$ 参数
   - 训练数据需求：60B tokens</p>
</li>
<li>
<p><strong>数据策略</strong>：
   - 高质量数据（20B tokens）：成本 = 20M × ($0.01 + $0.02) = $600K
   - 太贵了！调整策略：</p>
</li>
</ol>
<p><strong>混合策略</strong>：</p>
<ul>
<li>5B高质量tokens：5M × $0.03 = $150K</li>
<li>40B基础清洗tokens：40M × $0.015 = $600K</li>
<li>仍然超预算！</li>
</ul>
<p><strong>最终策略</strong>：</p>
<ul>
<li>2B高质量tokens：$60K</li>
<li>20B基础tokens：$300K</li>
<li>缩小模型至1.3B参数</li>
<li>数据总成本：$360K → 调整为$25K预算内</li>
</ul>
<ol start="4">
<li><strong>实际可行方案</strong>：
   - 模型：1.3B参数
   - 数据：1B高质量 + 10B基础质量 = 11B tokens
   - 数据成本：1M × $0.03 + 10M × $0.015 = $30K + $150K = $180K</li>
</ol>
<p>预算不足！最终：</p>
<ul>
<li><strong>350M参数模型</strong></li>
<li><strong>7B tokens</strong>（500M高质量 + 6.5B基础）</li>
<li>数据成本：0.5M × $0.03 + 6.5M × $0.015 = $15K + $97.5K = $25K（略微削减）</li>
</ul>
<ol start="5">
<li>
<p><strong>时间规划（3个月）</strong>：
   - 月1：数据收集与清洗，模型架构设计
   - 月2：主训练阶段（使用spot instances节省成本）
   - 月3：微调、评估、部署准备</p>
</li>
<li>
<p><strong>风险缓解</strong>：
   - 使用检查点，防止训练中断
   - 先训练125M小模型验证pipeline
   - 保留5K应急资金应对意外</p>
</li>
</ol>
<p><strong>关键决策</strong>：</p>
<ul>
<li>选择较小但高质量的模型</li>
<li>优先保证数据质量的核心集</li>
<li>采用渐进式训练降低风险</li>
</ul>
</details>
<p><strong>练习9.8：提示优化的信息增益（研究题）</strong></p>
<p>设计一个算法，自动优化few-shot示例的选择。给定：</p>
<ul>
<li>候选示例池：1000个</li>
<li>上下文限制：最多8个示例</li>
<li>目标：最大化下游任务性能</li>
</ul>
<p>请提出：</p>
<ol>
<li>示例选择的评分函数</li>
<li>搜索策略（贪心、beam search、还是其他？）</li>
<li>如何避免过拟合到特定任务？</li>
<li>计算复杂度分析</li>
</ol>
<p><em>提示：考虑多样性、相关性、难度等因素</em></p>
<details>
<summary>参考思路</summary>
<p><strong>算法设计</strong>：</p>
<ol>
<li><strong>评分函数</strong>：
$$Score(S) = \alpha \cdot Rel(S) + \beta \cdot Div(S) + \gamma \cdot Cov(S) - \delta \cdot Red(S)$$</li>
</ol>
<p>其中：</p>
<ul>
<li>$Rel(S)$：相关性（与测试样本的平均相似度）</li>
<li>$Div(S)$：多样性（示例间的平均距离）</li>
<li>$Cov(S)$：覆盖度（涵盖的任务子类型数）</li>
<li>$Red(S)$：冗余度（信息重复程度）</li>
</ul>
<ol start="2">
<li><strong>具体实现</strong>：</li>
</ol>
<div class="codehilite"><pre><span></span><code>Rel(S) = mean(max_sim(x_test, s) for s in S)
Div(S) = mean(dist(s_i, s_j) for s_i, s_j in S)
Cov(S) = |unique_clusters(S)| / total_clusters
Red(S) = mean(sim(s_i, s_j) for s_i, s_j in S)
</code></pre></div>

<ol start="3">
<li><strong>搜索策略：改进的贪心算法</strong>：</li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="mf">1.</span><span class="w"> </span><span class="n">初始化</span><span class="err">：</span><span class="n">选择与查询最相似的示例</span>
<span class="mf">2.</span><span class="w"> </span><span class="n">迭代添加</span><span class="err">：</span>
<span class="w">   </span><span class="o">-</span><span class="w"> </span><span class="n">计算每个候选的边际收益</span>
<span class="w">   </span><span class="o">-</span><span class="w"> </span><span class="n">选择收益最大的</span>
<span class="w">   </span><span class="o">-</span><span class="w"> </span><span class="n">动态调整α</span><span class="p">,</span><span class="n">β</span><span class="p">,</span><span class="n">γ</span><span class="p">,</span><span class="n">δ权重</span>
<span class="mf">3.</span><span class="w"> </span><span class="n">后处理</span><span class="err">：</span><span class="n">局部搜索优化</span>
</code></pre></div>

<p>复杂度：$O(k \cdot n \cdot d)$，其中k=8，n=1000，d=嵌入维度</p>
<ol start="4">
<li>
<p><strong>避免过拟合</strong>：
   - 交叉验证：在多个任务上评估
   - 正则化：限制极端相似度
   - 集成：多个选择策略投票
   - 在线学习：根据反馈调整权重</p>
</li>
<li>
<p><strong>高级优化</strong>：
   - 使用强化学习学习选择策略
   - 主动学习：选择最能减少不确定性的示例
   - 元学习：学习跨任务的选择模式</p>
</li>
</ol>
<p><strong>实验验证指标</strong>：</p>
<ul>
<li>不同任务的泛化性能</li>
<li>与随机选择的对比</li>
<li>与人工选择的对比</li>
<li>计算时间 vs 性能权衡</li>
</ul>
</details>
            </article>
            
            <nav class="page-nav"><a href="chapter8.html" class="nav-link prev">← 第8章：Transformer与注意力机制</a><a href="chapter10.html" class="nav-link next">第10章：变分自编码器与生成建模 →</a></nav>
        </main>
    </div>
</body>
</html>