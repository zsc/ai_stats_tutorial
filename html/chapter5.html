<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <base href="./">
    <title>第5章：深度学习优化</title>
    <link rel="stylesheet" href="assets/style.css">
    <link rel="stylesheet" href="assets/highlight.css">
    <script src="assets/script.js" defer></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$']],
                displayMath: [['$$', '$$']],
                processEscapes: false,
                packages: {'[+]': ['noerrors', 'ams']}
            },
            options: {
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            },
            loader: {
                load: ['[tex]/noerrors', '[tex]/ams']
            }
        };
    </script>
</head>
<body>
    <div class="container">
        <nav id="sidebar" class="sidebar">
            <div class="sidebar-header">
                <h3>目录</h3>
                <button id="sidebar-toggle" class="sidebar-toggle">
                    <span></span>
                    <span></span>
                    <span></span>
                </button>
            </div>
            <div class="sidebar-search">
                <input type="text" id="sidebar-search-input" placeholder="搜索..." autocomplete="off">
            </div>
            <div id="tree-container">
                <nav class="tree-nav" role="tree">
                    <div class="tree-item " >
                        <a href="index.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">现代人工智能：优化与统计视角</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter1.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第1章：优化基础与梯度下降</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter2.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第2章：统计学习理论</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter3.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第3章：线性模型与正则化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter4.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第4章：神经网络基础</span>
                        </a>
                    </div>
                
                    <div class="tree-item active" >
                        <a href="chapter5.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第5章：深度学习优化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter6.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第6章：卷积神经网络</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter7.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第7章：循环神经网络与序列建模</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter8.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第8章：Transformer与注意力机制</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter9.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第9章：大语言模型</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter10.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第10章：变分自编码器与生成建模</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter11.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第11章：扩散模型</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter12.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第12章：深度强化学习</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="CLAUDE.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Untitled</span>
                        </a>
                    </div>
                </nav>
            </div>
        </nav>
        
        <main class="content">
            <article>
                <h1 id="5">第5章：深度学习优化</h1>
<p>深度学习的成功很大程度上依赖于优化算法的进步。本章将从优化理论的角度深入探讨深度神经网络的训练方法，包括随机梯度下降的各种变体、自适应学习率算法、归一化技术以及二阶优化方法。我们将特别关注这些方法背后的统计学原理，以及它们在现代大规模模型训练中的实际应用。</p>
<h2 id="51-sgd">5.1 随机梯度下降（SGD）及其动量变体</h2>
<h3 id="511">5.1.1 从批量梯度到随机梯度</h3>
<p>在深度学习中，我们通常面对的优化问题是：</p>
<p>$$\min_{\theta} \mathcal{L}(\theta) = \frac{1}{N} \sum_{i=1}^{N} \ell(f_\theta(\mathbf{x}_i), y_i)$$
其中 $N$ 是训练样本数，$\ell$ 是损失函数，$f_\theta$ 是参数为 $\theta$ 的神经网络。</p>
<p><strong>批量梯度下降（BGD）</strong> 使用全部数据计算梯度：
$$\theta_{t+1} = \theta_t - \eta \nabla \mathcal{L}(\theta_t)$$
但当 $N$ 很大时，计算成本过高。现代数据集动辄百万甚至十亿样本，单次梯度计算可能需要数小时。</p>
<p><strong>随机梯度下降（SGD）</strong> 每次只使用一个样本：
$$\theta_{t+1} = \theta_t - \eta \nabla \ell(f_{\theta_t}(\mathbf{x}_i), y_i)$$
这带来了一个关键洞察：随机梯度虽然有噪声，但其期望值等于真实梯度。这种噪声反而有助于逃离局部最小值和鞍点。</p>
<p><strong>小批量SGD（Mini-batch SGD）</strong> 是实践中的标准做法：
$$\theta_{t+1} = \theta_t - \eta \frac{1}{B} \sum_{i \in \mathcal{B}_t} \nabla \ell(f_{\theta_t}(\mathbf{x}_i), y_i)$$
其中 $\mathcal{B}_t$ 是大小为 $B$ 的小批量。</p>
<p><strong>批量大小的权衡</strong>：</p>
<ul>
<li><strong>计算效率</strong>：现代GPU针对矩阵运算优化，批量计算比单样本快得多</li>
<li><strong>内存限制</strong>：批量越大，需要的GPU内存越多</li>
<li><strong>泛化性能</strong>：适度的批量大小（32-512）通常泛化最好</li>
<li><strong>并行性</strong>：批量内样本可以完全并行计算</li>
</ul>
<p>从优化轨迹看三种方法的区别：</p>
<div class="codehilite"><pre><span></span><code><span class="n">BGD</span><span class="o">:</span><span class="w">  </span><span class="err">平滑但缓慢</span><span class="w">    </span><span class="err">═══════════→</span><span class="w"> </span><span class="o">(</span><span class="err">确定性路径</span><span class="o">)</span>
<span class="n">SGD</span><span class="o">:</span><span class="w">  </span><span class="err">噪声但快速</span><span class="w">    </span><span class="err">～～～～～→</span><span class="w"> </span><span class="o">(</span><span class="err">高方差路径</span><span class="o">)</span><span class="w">  </span>
<span class="n">Mini</span><span class="o">:</span><span class="w"> </span><span class="err">平衡选择</span><span class="w">      </span><span class="err">≈≈≈≈≈≈≈→</span><span class="w"> </span><span class="o">(</span><span class="err">适度噪声路径</span><span class="o">)</span>
</code></pre></div>

<h3 id="512-sgd">5.1.2 SGD的统计性质</h3>
<p>从统计角度看，SGD的梯度估计是无偏的：
$$\mathbb{E}[\nabla \ell(f_{\theta}(\mathbf{x}_i), y_i)] = \nabla \mathcal{L}(\theta)$$
但存在方差：
$$\text{Var}[\nabla \ell(f_{\theta}(\mathbf{x}_i), y_i)] = \sigma^2$$
小批量可以降低方差：
$$\text{Var}\left[\frac{1}{B} \sum_{i \in \mathcal{B}} \nabla \ell_i\right] = \frac{\sigma^2}{B}$$
这个方差-批量关系揭示了一个深刻的权衡：</p>
<p><strong>信噪比（SNR）分析</strong>：
$$\text{SNR} = \frac{|\mathbb{E}[\nabla]|^2}{\text{Var}[\nabla]} = \frac{|\nabla \mathcal{L}|^2}{\sigma^2/B} \propto B$$
信噪比随批量大小线性增长，但这并不意味着越大越好。实际上，适度的噪声（小SNR）有助于：</p>
<ul>
<li><strong>正则化效果</strong>：梯度噪声类似于在损失函数中添加噪声，防止过拟合</li>
<li><strong>探索能力</strong>：帮助逃离平坦区域和鞍点</li>
<li><strong>隐式退火</strong>：训练后期自然降低学习率时，噪声也相应减小</li>
</ul>
<p><strong>有效批量大小的概念</strong>：
当批量大小超过某个临界值 $B_{critical}$ 后，进一步增大批量的收益递减。这个临界值与数据集的内在噪声和模型容量有关：
$$B_{critical} \approx \frac{\text{数据集大小}}{\text{有效样本多样性}}$$
<strong>经验法则</strong>：</p>
<ul>
<li>批量大小通常选择2的幂次（32, 64, 128, 256）以利用硬件加速</li>
<li>增大批量大小时，学习率可以线性增加（线性缩放规则）</li>
<li>批量大小存在临界值，超过后收敛性能不再改善</li>
<li>视觉任务：批量256-1024效果好</li>
<li>NLP任务：可以使用更大批量（2K-32K）</li>
<li>强化学习：通常需要大批量（&gt;1K）以降低策略梯度方差</li>
</ul>
<h3 id="513">5.1.3 动量方法</h3>
<p>动量方法的核心思想来自物理学：将优化过程类比为粒子在势能面上的运动。梯度提供加速度，而动量维持速度。</p>
<p><strong>经典动量（Momentum）</strong>：
$$\begin{aligned}
\mathbf{v}_{t+1} &amp;= \beta \mathbf{v}_t - \eta \nabla \mathcal{L}(\theta_t) \\
\theta_{t+1} &amp;= \theta_t + \mathbf{v}_{t+1}
\end{aligned}$$
其中 $\beta \in [0,1)$ 是动量系数，典型值为0.9。</p>
<p>动量方法可以看作是指数移动平均：
$$\mathbf{v}_t = -\eta \sum_{i=0}^{t} \beta^{t-i} \nabla \mathcal{L}(\theta_i)$$
<strong>物理直觉</strong>：</p>
<ul>
<li>$\beta = 0$：无摩擦，粒子不断加速</li>
<li>$\beta = 0.9$：有摩擦，最终达到终端速度</li>
<li>$\beta = 0.99$：高摩擦，缓慢加速但稳定</li>
</ul>
<p><strong>动量的三大优势</strong>：</p>
<ol>
<li><strong>加速收敛</strong>：在一致的梯度方向上累积速度</li>
<li><strong>减少振荡</strong>：在振荡方向上动量相互抵消</li>
<li><strong>逃离局部最小值</strong>：积累的动量可能越过小的局部障碍</li>
</ol>
<div class="codehilite"><pre><span></span><code>病态条件下的优化轨迹：
无动量：<span class="w">  </span>↗↘↗↘<span class="w">  </span><span class="p">(</span>沿陡峭方向振荡<span class="p">)</span>
有动量：<span class="w">  </span>→→→→<span class="w">  </span><span class="p">(</span>沿平缓方向加速<span class="p">)</span>

在峡谷地形中：
<span class="w">     </span>╱╲╱╲╱╲<span class="w">    </span><span class="o">&lt;-</span><span class="w"> </span>无动量：之字形下降
<span class="w">    </span>╱<span class="w">       </span>╲<span class="w">   </span>
<span class="w">   </span>╱<span class="w">  </span>────→<span class="w">  </span>╲<span class="w">  </span><span class="o">&lt;-</span><span class="w"> </span>有动量：平滑快速下降
</code></pre></div>

<p><strong>Nesterov加速梯度（NAG）</strong>：
$$\begin{aligned}
\mathbf{v}_{t+1} &amp;= \beta \mathbf{v}_t - \eta \nabla \mathcal{L}(\theta_t + \beta \mathbf{v}_t) \\
\theta_{t+1} &amp;= \theta_t + \mathbf{v}_{t+1}
\end{aligned}$$
NAG在计算梯度前先"向前看"，具有更好的理论收敛速度。</p>
<p><strong>NAG vs 标准动量</strong>：</p>
<ul>
<li>标准动量：先加速，后修正</li>
<li>NAG：预测未来位置，在那里计算梯度</li>
<li>收敛速度：NAG达到 $O(1/t^2)$，标准动量为 $O(1/t)$</li>
</ul>
<p><strong>实际使用中的技巧</strong>：</p>
<ul>
<li>初始阶段使用较小动量（0.5），后期增加到0.9或0.99</li>
<li>学习率衰减时，可以同时增加动量系数</li>
<li>对于RNN，动量可能导致梯度爆炸，需谨慎使用</li>
</ul>
<h3 id="514">5.1.4 学习率调度</h3>
<p>学习率调度对深度学习至关重要：</p>
<p><strong>阶梯衰减（Step Decay）</strong>：
$$\eta_t = \eta_0 \cdot \gamma^{\lfloor t/s \rfloor}$$
其中 $\gamma &lt; 1$ 是衰减因子，$s$ 是步长。</p>
<p><strong>余弦退火（Cosine Annealing）</strong>：
$$\eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})\left(1 + \cos\left(\frac{t}{T}\pi\right)\right)$$
<strong>线性预热（Linear Warmup）</strong>：
$$\eta_t = \begin{cases}
\frac{t}{T_{warmup}} \eta_{target} &amp; t &lt; T_{warmup} \\
\eta_{target} &amp; t \geq T_{warmup}
\end{cases}$$
<strong>经验法则</strong>：</p>
<ul>
<li>训练初期使用预热，避免梯度爆炸</li>
<li>大批量训练需要更长的预热期</li>
<li>余弦退火通常优于阶梯衰减</li>
</ul>
<h2 id="52-adam">5.2 Adam与自适应学习率方法</h2>
<h3 id="521-adagrad">5.2.1 AdaGrad：自适应梯度算法</h3>
<p>AdaGrad的核心洞察是：不同参数应该有不同的学习率。频繁更新的参数应该用较小学习率，稀疏更新的参数应该用较大学习率。</p>
<p>AdaGrad为每个参数维持独立的学习率：
$$\begin{aligned}
\mathbf{g}_t &amp;= \nabla \mathcal{L}(\theta_t) \\
\mathbf{G}_t &amp;= \mathbf{G}_{t-1} + \mathbf{g}_t^2 \\
\theta_{t+1} &amp;= \theta_t - \frac{\eta}{\sqrt{\mathbf{G}_t + \epsilon}} \odot \mathbf{g}_t
\end{aligned}$$
其中 $\odot$ 表示逐元素乘法，$\epsilon$ 是小常数（如 $10^{-8}$）防止除零。</p>
<p><strong>自适应机制的解释</strong>：</p>
<ul>
<li>$\mathbf{G}_t$ 累积历史梯度的平方，反映参数的更新频率</li>
<li>对于梯度一直很大的参数，$\mathbf{G}_t$ 快速增长，学习率快速下降</li>
<li>对于梯度稀疏的参数，$\mathbf{G}_t$ 缓慢增长，保持较大学习率</li>
</ul>
<p><strong>为什么对稀疏数据有效</strong>：
在NLP任务中，词嵌入的更新非常稀疏（每个批次只更新出现的词）。AdaGrad能够：</p>
<ul>
<li>为罕见词维持较大学习率，快速学习</li>
<li>为常见词降低学习率，精细调整</li>
<li>自动实现了类似词频加权的效果</li>
</ul>
<p><strong>问题与局限</strong>：</p>
<ul>
<li><strong>学习率单调递减</strong>：$\mathbf{G}_t$ 只增不减，最终 $\frac{\eta}{\sqrt{\mathbf{G}_t}} \to 0$</li>
<li><strong>对初始学习率敏感</strong>：需要仔细调整 $\eta$</li>
<li><strong>内存需求</strong>：需要存储 $\mathbf{G}_t$，与参数数量相同</li>
</ul>
<p>理论保证：对于凸函数，AdaGrad的regret bound为 $O(\sqrt{T})$</p>
<h3 id="522-rmsprop">5.2.2 RMSprop：指数移动平均</h3>
<p>RMSprop解决了AdaGrad学习率递减问题：
$$\begin{aligned}
\mathbf{g}_t &amp;= \nabla \mathcal{L}(\theta_t) \\
\mathbf{v}_t &amp;= \beta_2 \mathbf{v}_{t-1} + (1-\beta_2) \mathbf{g}_t^2 \\
\theta_{t+1} &amp;= \theta_t - \frac{\eta}{\sqrt{\mathbf{v}_t + \epsilon}} \odot \mathbf{g}_t
\end{aligned}$$
典型的 $\beta_2 = 0.999$。</p>
<h3 id="523-adam">5.2.3 Adam：自适应矩估计</h3>
<p>Adam结合了动量和RMSprop的优点，是深度学习中最流行的优化器：
$$\begin{aligned}
\mathbf{g}_t &amp;= \nabla \mathcal{L}(\theta_t) \\
\mathbf{m}_t &amp;= \beta_1 \mathbf{m}_{t-1} + (1-\beta_1) \mathbf{g}_t &amp; \text{(一阶矩估计)} \\
\mathbf{v}_t &amp;= \beta_2 \mathbf{v}_{t-1} + (1-\beta_2) \mathbf{g}_t^2 &amp; \text{(二阶矩估计)} \\
\hat{\mathbf{m}}_t &amp;= \frac{\mathbf{m}_t}{1-\beta_1^t} &amp; \text{(偏差修正)} \\
\hat{\mathbf{v}}_t &amp;= \frac{\mathbf{v}_t}{1-\beta_2^t} &amp; \text{(偏差修正)} \\
\theta_{t+1} &amp;= \theta_t - \eta \frac{\hat{\mathbf{m}}_t}{\sqrt{\hat{\mathbf{v}}_t} + \epsilon}
\end{aligned}$$
<strong>Adam的四个关键创新</strong>：</p>
<ol>
<li><strong>动量与自适应结合</strong>：$\mathbf{m}_t$ 提供方向，$\mathbf{v}_t$ 调整步长</li>
<li><strong>偏差修正</strong>：解决初始化时的偏差问题</li>
<li><strong>二阶矩估计</strong>：比简单的梯度平方更稳定</li>
<li><strong>逐参数自适应</strong>：每个参数有独立的"学习率"</li>
</ol>
<p><strong>偏差修正的必要性</strong>：
初始时 $\mathbf{m}_0 = \mathbf{v}_0 = 0$，导致：
$$\mathbb{E}[\mathbf{m}_t] = (1-\beta_1^t)\mathbb{E}[\mathbf{g}] \neq \mathbb{E}[\mathbf{g}]$$
偏差修正确保：
$$\mathbb{E}[\hat{\mathbf{m}}_t] = \mathbb{E}[\mathbf{g}]$$
<strong>有效学习率的理解</strong>：
Adam的每个参数的有效学习率为：
$$\eta_{effective} = \eta \cdot \frac{1}{\sqrt{\hat{\mathbf{v}}_t} + \epsilon}$$
这意味着：</p>
<ul>
<li>梯度持续大的参数：学习率自动降低</li>
<li>梯度稀疏的参数：保持较大学习率</li>
<li>梯度方向变化频繁的参数：学习率降低（$\mathbf{v}_t$ 增大）</li>
</ul>
<p><strong>默认超参数</strong>：</p>
<ul>
<li>$\beta_1 = 0.9$（动量系数）</li>
<li>$\beta_2 = 0.999$（二阶矩衰减率）</li>
<li>$\eta = 0.001$（学习率）</li>
<li>$\epsilon = 10^{-8}$</li>
</ul>
<p><strong>超参数敏感性分析</strong>：</p>
<ul>
<li>$\beta_1$：控制动量，0.9适用于大多数任务，0.8用于噪声数据</li>
<li>$\beta_2$：控制自适应性，0.999标准，0.99用于更激进的适应</li>
<li>$\eta$：比SGD的学习率小10倍左右</li>
<li>$\epsilon$：通常不需要调整，但在梯度非常小时可增大到 $10^{-4}$</li>
</ul>
<h3 id="524-adam">5.2.4 Adam的变体</h3>
<p><strong>AdamW（权重衰减解耦）</strong>：
$$\theta_{t+1} = \theta_t - \eta \left(\frac{\hat{\mathbf{m}}_t}{\sqrt{\hat{\mathbf{v}}_t} + \epsilon} + \lambda \theta_t\right)$$
将L2正则化从梯度计算中分离，改善泛化性能。</p>
<p><strong>RAdam（矫正Adam）</strong>：
自动调整自适应学习率的方差，解决训练初期的不稳定问题。</p>
<p><strong>经验法则</strong>：</p>
<ul>
<li>Adam通常是深度学习的默认选择</li>
<li>对于视觉任务，SGD+动量可能获得更好的泛化</li>
<li>对于NLP和Transformer，Adam/AdamW效果更好</li>
<li>学习率通常需要根据模型大小调整</li>
</ul>
<h2 id="53">5.3 批归一化与层归一化</h2>
<h3 id="531">5.3.1 内部协变量偏移问题</h3>
<p>深度网络训练中，每层输入的分布随着前层参数更新而改变，这种现象称为内部协变量偏移（Internal Covariate Shift）。</p>
<p><strong>问题的本质</strong>：
考虑一个深度网络的第 $l$ 层，其输入 $\mathbf{x}^{(l)}$ 依赖于所有前层参数：
$$\mathbf{x}^{(l)} = f_{l-1}(f_{l-2}(...f_1(\mathbf{x}^{(0)})))$$
当前层参数更新时，后层看到的输入分布持续变化：</p>
<ul>
<li>每层需要不断适应新的输入分布</li>
<li>降低了学习效率</li>
<li>限制了可用的学习率（避免激活值爆炸）</li>
</ul>
<p><strong>数学表述</strong>：
设第 $l$ 层在时刻 $t$ 的输入分布为 $p_t(\mathbf{x}^{(l)})$，则：
$$D_{KL}(p_t(\mathbf{x}^{(l)}) | p_{t+1}(\mathbf{x}^{(l)})) &gt; 0$$
这种分布偏移导致：</p>
<ol>
<li><strong>梯度消失/爆炸</strong>：激活值进入饱和区</li>
<li><strong>学习率限制</strong>：需要小心控制更新幅度</li>
<li><strong>收敛缓慢</strong>：每层都在"追赶"变化的目标</li>
</ol>
<p><strong>传统解决方案的局限</strong>：</p>
<ul>
<li><strong>careful initialization</strong>：只在初始时有效</li>
<li><strong>小学习率</strong>：训练缓慢</li>
<li><strong>特殊激活函数</strong>：如ReLU，但不完全解决问题</li>
</ul>
<h3 id="532-batch-normalization">5.3.2 批归一化（Batch Normalization）</h3>
<p>批归一化通过标准化激活值来稳定训练，是深度学习的重大突破之一：</p>
<p><strong>训练时</strong>：
$$\begin{aligned}
\mu_B &amp;= \frac{1}{B} \sum_{i=1}^{B} \mathbf{x}_i \\
\sigma_B^2 &amp;= \frac{1}{B} \sum_{i=1}^{B} (\mathbf{x}_i - \mu_B)^2 \\
\hat{\mathbf{x}}_i &amp;= \frac{\mathbf{x}_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}} \\
\mathbf{y}_i &amp;= \gamma \hat{\mathbf{x}}_i + \beta
\end{aligned}$$
其中 $\gamma$ 和 $\beta$ 是可学习的缩放和偏移参数。</p>
<p><strong>为什么需要 $\gamma$ 和 $\beta$</strong>：</p>
<ul>
<li>标准化可能限制网络的表达能力</li>
<li>$\gamma$ 和 $\beta$ 恢复表达能力</li>
<li>如果最优解需要恒等变换：$\gamma = \sqrt{\sigma_B^2}$, $\beta = \mu_B$</li>
</ul>
<p><strong>推理时</strong>：
使用训练时的移动平均统计量：
$$\hat{\mathbf{x}} = \frac{\mathbf{x} - \mu_{running}}{\sqrt{\sigma_{running}^2 + \epsilon}}$$
移动平均更新：
$$\begin{aligned}
\mu_{running} &amp;= (1-\alpha) \mu_{running} + \alpha \mu_B \\
\sigma_{running}^2 &amp;= (1-\alpha) \sigma_{running}^2 + \alpha \sigma_B^2
\end{aligned}$$
其中 $\alpha$ 通常为0.1。</p>
<p><strong>批归一化的多重效应</strong>：</p>
<ol>
<li>
<p><strong>平滑损失曲面</strong>：使优化更容易
   - 减少了参数间的相互依赖
   - 使Hessian矩阵条件数改善</p>
</li>
<li>
<p><strong>正则化效果</strong>：由于批统计的随机性
   - 每个样本看到的是批内其他样本的"噪声"统计
   - 类似于dropout的随机性</p>
</li>
<li>
<p><strong>允许更大学习率</strong>：
   - 防止激活值爆炸
   - 梯度更稳定</p>
</li>
<li>
<p><strong>减少对初始化的敏感性</strong>：
   - 即使初始化不当，也能快速调整到合理范围</p>
</li>
</ol>
<p><strong>放置位置的最佳实践</strong>：</p>
<div class="codehilite"><pre><span></span><code>方案1（原始）: Conv → BN → ReLU
方案2（推荐）: Conv → ReLU → BN
方案3（ResNet）: Conv → BN → ReLU → Conv → BN → Add
</code></pre></div>

<p><strong>批归一化的计算图影响</strong>：
BN使得每个样本的梯度依赖于整个批次，这带来了独特的动力学特性。</p>
<h3 id="533-layer-normalization">5.3.3 层归一化（Layer Normalization）</h3>
<p>层归一化在特征维度上进行标准化，不依赖批量大小：
$$\begin{aligned}
\mu_l &amp;= \frac{1}{H} \sum_{i=1}^{H} x_i \\
\sigma_l^2 &amp;= \frac{1}{H} \sum_{i=1}^{H} (x_i - \mu_l)^2 \\
\hat{x}_i &amp;= \frac{x_i - \mu_l}{\sqrt{\sigma_l^2 + \epsilon}}
\end{aligned}$$
其中 $H$ 是隐藏层维度。</p>
<p><strong>批归一化 vs 层归一化</strong>：</p>
<div class="codehilite"><pre><span></span><code>批归一化：跨批量样本归一化
  样本1: [x11, x12, x13]  ↓
  样本2: [x21, x22, x23]  ↓ 
  样本3: [x31, x32, x33]  ↓
         归一化每一列

层归一化：跨特征维度归一化
  样本1: [x11, x12, x13] → 归一化
  样本2: [x21, x22, x23] → 归一化
  样本3: [x31, x32, x33] → 归一化
</code></pre></div>

<h3 id="534">5.3.4 其他归一化技术</h3>
<p><strong>组归一化（Group Normalization）</strong>：
将通道分组，在每组内进行归一化，适用于小批量场景。</p>
<p><strong>实例归一化（Instance Normalization）</strong>：
每个样本独立归一化，常用于风格迁移。</p>
<p><strong>RMSNorm</strong>：
简化的层归一化，只使用二阶矩：
$$\text{RMSNorm}(\mathbf{x}) = \frac{\mathbf{x}}{\text{RMS}(\mathbf{x})} \cdot \gamma$$
其中 $\text{RMS}(\mathbf{x}) = \sqrt{\frac{1}{H}\sum_{i=1}^{H} x_i^2}$</p>
<p><strong>经验法则</strong>：</p>
<ul>
<li>CNN通常使用批归一化</li>
<li>RNN和Transformer使用层归一化</li>
<li>批量大小小于32时考虑组归一化</li>
<li>大模型训练倾向于使用RMSNorm（计算效率更高）</li>
</ul>
<h2 id="54">5.4 二阶优化方法</h2>
<h3 id="541">5.4.1 牛顿法与拟牛顿法</h3>
<p>二阶优化方法利用曲率信息加速收敛。虽然在大规模深度学习中计算成本高昂，但其思想影响了许多实用算法。</p>
<p><strong>牛顿法</strong>使用Hessian矩阵（二阶导数）：
$$\theta_{t+1} = \theta_t - \eta \mathbf{H}^{-1} \nabla \mathcal{L}(\theta_t)$$
其中 $\mathbf{H} = \nabla^2 \mathcal{L}(\theta)$ 是Hessian矩阵。</p>
<p><strong>几何解释</strong>：</p>
<ul>
<li>一阶方法（SGD）：假设局部是线性的</li>
<li>二阶方法（牛顿）：假设局部是二次的</li>
</ul>
<p>牛顿法的更新可以理解为在二次近似下的最优步：
$$\mathcal{L}(\theta + \Delta) \approx \mathcal{L}(\theta) + \nabla \mathcal{L}^T \Delta + \frac{1}{2} \Delta^T \mathbf{H} \Delta$$
最小化右侧得到：$\Delta^* = -\mathbf{H}^{-1} \nabla \mathcal{L}$</p>
<p><strong>牛顿法的优势</strong>：</p>
<ul>
<li><strong>尺度不变性</strong>：对参数重新缩放不敏感</li>
<li><strong>二次收敛</strong>：在最优点附近收敛极快</li>
<li><strong>自适应步长</strong>：自动考虑不同方向的曲率</li>
</ul>
<p><strong>实际问题</strong>：</p>
<ul>
<li><strong>计算复杂度</strong>：计算Hessian需要 $O(n^2)$ 存储和 $O(n^3)$ 求逆</li>
<li><strong>Hessian不正定</strong>：非凸区域可能导致上升方向</li>
<li><strong>噪声敏感</strong>：随机梯度的Hessian估计方差极大</li>
<li><strong>鞍点问题</strong>：在鞍点处Hessian奇异</li>
</ul>
<p><strong>拟牛顿法的思路</strong>：
不直接计算Hessian，而是通过历史梯度信息逐步构建近似：</p>
<p><strong>BFGS更新公式</strong>：
$$\mathbf{B}_{k+1} = \mathbf{B}_k + \frac{\mathbf{y}_k \mathbf{y}_k^T}{\mathbf{y}_k^T \mathbf{s}_k} - \frac{\mathbf{B}_k \mathbf{s}_k \mathbf{s}_k^T \mathbf{B}_k}{\mathbf{s}_k^T \mathbf{B}_k \mathbf{s}_k}$$
其中：</p>
<ul>
<li>$\mathbf{s}_k = \theta_{k+1} - \theta_k$（参数变化）</li>
<li>$\mathbf{y}_k = \nabla \mathcal{L}_{k+1} - \nabla \mathcal{L}_k$（梯度变化）</li>
<li>$\mathbf{B}_k$ 是Hessian的近似</li>
</ul>
<h3 id="542-l-bfgs">5.4.2 L-BFGS</h3>
<p>L-BFGS（Limited-memory BFGS）通过低秩近似避免存储完整Hessian：</p>
<p>使用最近 $m$ 步的梯度信息构造Hessian逆的近似：
$$\mathbf{s}_k = \theta_{k+1} - \theta_k, \quad \mathbf{y}_k = \nabla \mathcal{L}_{k+1} - \nabla \mathcal{L}_k$$
通过递归计算得到搜索方向，内存需求为 $O(mn)$。</p>
<h3 id="543">5.4.3 自然梯度下降</h3>
<p>自然梯度考虑参数空间的几何结构：
$$\theta_{t+1} = \theta_t - \eta \mathbf{F}^{-1} \nabla \mathcal{L}(\theta_t)$$
其中 $\mathbf{F}$ 是Fisher信息矩阵：
$$\mathbf{F} = \mathbb{E}_{p(x|\theta)}\left[\nabla \log p(x|\theta) \nabla \log p(x|\theta)^T\right]$$</p>
<h3 id="544-k-fackronecker">5.4.4 K-FAC（Kronecker分解近似）</h3>
<p>K-FAC通过Kronecker积近似Fisher矩阵：
$$\mathbf{F} \approx \mathbf{A} \otimes \mathbf{G}$$
其中 $\mathbf{A}$ 是输入协方差，$\mathbf{G}$ 是梯度协方差。</p>
<p>优点：</p>
<ul>
<li>计算效率高于完整Fisher矩阵</li>
<li>可以并行计算</li>
<li>在某些任务上收敛更快</li>
</ul>
<h3 id="545-shampoo">5.4.5 Shampoo优化器</h3>
<p>Shampoo是一种实用的二阶优化器，通过预条件矩阵加速收敛：
$$\theta_{t+1} = \theta_t - \eta \mathbf{H}_t^{-1/4} \nabla \mathcal{L}(\theta_t) \mathbf{G}_t^{-1/4}$$
其中 $\mathbf{H}_t$ 和 $\mathbf{G}_t$ 分别是左右预条件矩阵。</p>
<p><strong>经验法则</strong>：</p>
<ul>
<li>二阶方法在小到中等规模问题上效果好</li>
<li>大规模深度学习通常使用一阶方法（计算效率）</li>
<li>L-BFGS适合批量训练，不适合随机优化</li>
<li>K-FAC和Shampoo在某些场景可以加速训练2-3倍</li>
</ul>
<h2 id="55-diederik-p-kingmaadam">5.5 历史人物：Diederik P. Kingma与Adam优化器的革命</h2>
<p>Diederik P. Kingma是现代深度学习优化算法的关键贡献者。2014年，他与Jimmy Ba共同提出了Adam优化器，这篇论文成为机器学习领域引用最多的论文之一，彻底改变了深度学习的训练方式。</p>
<h3 id="_1">学术轨迹</h3>
<p>Kingma在阿姆斯特丹大学获得博士学位，师从Max Welling教授。他的研究兴趣横跨变分推断、生成模型和优化理论。除了Adam，他还是变分自编码器（VAE）的共同发明者，这两项工作都对深度学习产生了深远影响。</p>
<h3 id="adam">Adam的诞生背景</h3>
<p>2014年前，深度学习社区主要使用SGD及其动量变体。虽然AdaGrad和RMSprop等自适应方法已经出现，但都存在各自的问题：</p>
<ul>
<li>AdaGrad学习率单调递减</li>
<li>RMSprop缺乏理论保证</li>
<li>不同任务需要不同的优化器</li>
</ul>
<p>Kingma意识到需要一个通用、鲁棒的优化器，能够自适应地调整学习率，同时保持动量的优势。</p>
<h3 id="_2">核心创新</h3>
<p>Adam的关键创新在于：</p>
<ol>
<li><strong>结合一阶和二阶矩估计</strong>：同时追踪梯度的均值和方差</li>
<li><strong>偏差修正</strong>：解决初始化偏差问题</li>
<li><strong>逐参数自适应</strong>：为每个参数维持独立的学习率</li>
<li><strong>理论保证</strong>：提供了收敛性证明</li>
</ol>
<h3 id="_3">影响与争议</h3>
<p>Adam迅速成为深度学习的默认优化器，特别是在：</p>
<ul>
<li>自然语言处理</li>
<li>生成对抗网络</li>
<li>变分自编码器</li>
<li>Transformer模型</li>
</ul>
<p>但也存在争议：</p>
<ul>
<li>Wilson等人(2017)指出Adam可能导致泛化性能下降</li>
<li>导致了AdamW等改进版本的出现</li>
</ul>
<h3 id="_4">后续贡献</h3>
<p>Kingma继续在优化和生成模型领域做出贡献：</p>
<ul>
<li>Glow：基于流的生成模型</li>
<li>改进的变分推断方法</li>
<li>扩散模型的理论基础</li>
</ul>
<p>他的工作体现了理论与实践的完美结合，不仅提出了实用的算法，还提供了严格的理论分析。Adam优化器的成功证明了在深度学习中，好的优化算法可以极大地推动整个领域的进步。</p>
<h2 id="56-llmzero">5.6 现代连接：LLM训练中的梯度检查点与ZeRO优化</h2>
<h3 id="561-gradient-checkpointing">5.6.1 梯度检查点（Gradient Checkpointing）</h3>
<p>训练大模型的主要挑战是内存限制。标准反向传播需要存储所有中间激活值：</p>
<p><strong>内存需求</strong>：</p>
<ul>
<li>前向传播：$O(n \cdot l)$（$n$是批量大小，$l$是层数）</li>
<li>反向传播：需要所有中间激活值</li>
</ul>
<p><strong>梯度检查点策略</strong>：</p>
<ol>
<li>前向传播时只保存部分检查点</li>
<li>反向传播时重新计算未保存的激活值</li>
<li>时间换空间：增加33%计算，减少$O(\sqrt{l})$内存</li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="n">标准BP</span><span class="err">：</span><span class="w">  </span><span class="o">[</span><span class="n">A1</span><span class="o">][</span><span class="n">A2</span><span class="o">][</span><span class="n">A3</span><span class="o">][</span><span class="n">A4</span><span class="o">][</span><span class="n">A5</span><span class="o">][</span><span class="n">A6</span><span class="o">][</span><span class="n">A7</span><span class="o">][</span><span class="n">A8</span><span class="o">]</span><span class="w"> </span><span class="p">(</span><span class="n">保存所有</span><span class="p">)</span>
<span class="n">检查点</span><span class="err">：</span><span class="w">  </span><span class="o">[</span><span class="n">A1</span><span class="o">]</span><span class="w">    </span><span class="o">[</span><span class="n">A3</span><span class="o">]</span><span class="w">    </span><span class="o">[</span><span class="n">A5</span><span class="o">]</span><span class="w">    </span><span class="o">[</span><span class="n">A7</span><span class="o">]</span><span class="w">     </span><span class="p">(</span><span class="n">保存部分</span><span class="p">)</span>
<span class="w">         </span><span class="n">重算A2</span><span class="w">  </span><span class="n">重算A4</span><span class="w">  </span><span class="n">重算A6</span><span class="w">  </span><span class="n">重算A8</span>
</code></pre></div>

<h3 id="562-zerozero-redundancy-optimizer">5.6.2 ZeRO优化器（Zero Redundancy Optimizer）</h3>
<p>ZeRO通过分片技术减少内存冗余：</p>
<p><strong>ZeRO-1：优化器状态分片</strong></p>
<ul>
<li>Adam需要存储：参数、梯度、动量、方差</li>
<li>内存需求：$4 \times$ 模型大小</li>
<li>ZeRO-1：将优化器状态分布到多个GPU</li>
</ul>
<p><strong>ZeRO-2：梯度分片</strong></p>
<ul>
<li>每个GPU只存储部分梯度</li>
<li>通过all-reduce聚合梯度</li>
</ul>
<p><strong>ZeRO-3：参数分片</strong></p>
<ul>
<li>模型参数也分布存储</li>
<li>需要时通过通信获取</li>
</ul>
<p><strong>内存节省</strong>（以GPT-3 175B为例）：</p>
<div class="codehilite"><pre><span></span><code>标准数据并行：每GPU需要 1.4TB 内存
ZeRO-1：      每GPU需要 350GB
ZeRO-2：      每GPU需要 175GB  
ZeRO-3：      每GPU需要 20GB
</code></pre></div>

<h3 id="563">5.6.3 混合精度训练</h3>
<p>使用FP16/BF16代替FP32：</p>
<p><strong>核心技术</strong>：</p>
<ol>
<li><strong>主权重副本</strong>：FP32主权重，FP16计算</li>
<li><strong>损失缩放</strong>：防止梯度下溢</li>
<li><strong>动态损失缩放</strong>：自适应调整缩放因子</li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="c1"># 伪代码</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">compute_loss</span><span class="p">(</span><span class="n">model_fp16</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
<span class="n">scaled_loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">*</span> <span class="n">scale_factor</span>
<span class="n">scaled_gradients</span> <span class="o">=</span> <span class="n">backward</span><span class="p">(</span><span class="n">scaled_loss</span><span class="p">)</span>
<span class="n">gradients</span> <span class="o">=</span> <span class="n">scaled_gradients</span> <span class="o">/</span> <span class="n">scale_factor</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">master_weights_fp32</span><span class="p">)</span>
</code></pre></div>

<h3 id="564">5.6.4 大模型训练的优化策略组合</h3>
<p><strong>典型配置</strong>（训练100B+参数模型）：</p>
<ol>
<li><strong>优化器</strong>：AdamW + 梯度裁剪</li>
<li><strong>学习率</strong>：余弦衰减 + 线性预热</li>
<li><strong>内存优化</strong>：ZeRO-3 + 梯度检查点</li>
<li><strong>精度</strong>：BF16混合精度</li>
<li><strong>并行策略</strong>：
   - 数据并行（DP）
   - 张量并行（TP）
   - 流水线并行（PP）
   - 序列并行（SP）</li>
</ol>
<p><strong>3D并行示例</strong>：</p>
<div class="codehilite"><pre><span></span><code>模型分片：
  GPU集群 (8×8×8 = 512 GPUs)
  ├── PP=8 (流水线8段)
  ├── TP=8 (张量并行8路)
  └── DP=8 (数据并行8份)
</code></pre></div>

<h3 id="565">5.6.5 通信优化</h3>
<p><strong>Ring-AllReduce</strong>：
环形拓扑减少通信开销，时间复杂度 $O(n)$ → $O(\log n)$</p>
<p><strong>梯度累积</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="k">for</span> <span class="n">micro_batch</span> <span class="ow">in</span> <span class="n">mini_batch</span><span class="p">:</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">micro_batch</span><span class="p">)</span> <span class="o">/</span> <span class="n">accumulation_steps</span>
    <span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
<span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="n">accumulation_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</code></pre></div>

<p><strong>经验法则</strong>：</p>
<ul>
<li>梯度检查点在层数&gt;50时效果明显</li>
<li>ZeRO-2通常是最佳平衡点</li>
<li>BF16比FP16更稳定（动态范围更大）</li>
<li>通信成本随GPU数量增加，需要权衡并行度</li>
</ul>
<h2 id="_5">本章小结</h2>
<p>本章系统介绍了深度学习优化的核心方法和实践技巧：</p>
<h3 id="_6">关键概念</h3>
<ol>
<li>
<p><strong>随机梯度下降及其变体</strong>
   - SGD通过小批量降低计算成本
   - 动量方法平滑优化轨迹：$\mathbf{v}_{t+1} = \beta \mathbf{v}_t - \eta \nabla \mathcal{L}$
   - 学习率调度对收敛至关重要</p>
</li>
<li>
<p><strong>自适应学习率方法</strong>
   - Adam结合动量和自适应学习率：$\theta_{t+1} = \theta_t - \eta \frac{\hat{\mathbf{m}}_t}{\sqrt{\hat{\mathbf{v}}_t} + \epsilon}$
   - AdamW将权重衰减从梯度中解耦
   - 默认超参数：$\beta_1=0.9$, $\beta_2=0.999$, $\eta=0.001$</p>
</li>
<li>
<p><strong>归一化技术</strong>
   - 批归一化：跨批量样本归一化，适用于CNN
   - 层归一化：跨特征维度归一化，适用于Transformer
   - RMSNorm：简化版层归一化，计算效率更高</p>
</li>
<li>
<p><strong>二阶优化方法</strong>
   - 利用Hessian信息加速收敛
   - L-BFGS、K-FAC等方法在特定场景有效
   - 计算成本限制了在大规模深度学习中的应用</p>
</li>
<li>
<p><strong>大规模训练技术</strong>
   - 梯度检查点：时间换空间
   - ZeRO优化：分片减少内存冗余
   - 混合精度：FP16/BF16加速计算
   - 3D并行：数据、张量、流水线并行结合</p>
</li>
</ol>
<h3 id="_7">实用建议</h3>
<p>| 场景 | 推荐配置 |</p>
<table>
<thead>
<tr>
<th>场景</th>
<th>推荐配置</th>
</tr>
</thead>
<tbody>
<tr>
<td>小规模实验</td>
<td>Adam + 固定学习率</td>
</tr>
<tr>
<td>CNN训练</td>
<td>SGD+动量 + 批归一化 + 阶梯衰减</td>
</tr>
<tr>
<td>Transformer</td>
<td>AdamW + 层归一化 + 余弦退火 + 预热</td>
</tr>
<tr>
<td>大模型训练</td>
<td>AdamW + RMSNorm + ZeRO-2 + BF16</td>
</tr>
<tr>
<td>微调</td>
<td>较小学习率 + AdamW + 梯度裁剪</td>
</tr>
</tbody>
</table>
<h3 id="_8">核心公式汇总</h3>
<ul>
<li><strong>SGD with Momentum</strong>: $\mathbf{v}_{t+1} = \beta \mathbf{v}_t - \eta \nabla \mathcal{L}$</li>
<li><strong>Adam更新规则</strong>: </li>
<li>$\mathbf{m}_t = \beta_1 \mathbf{m}_{t-1} + (1-\beta_1) \mathbf{g}_t$</li>
<li>$\mathbf{v}_t = \beta_2 \mathbf{v}_{t-1} + (1-\beta_2) \mathbf{g}_t^2$</li>
<li>$\theta_{t+1} = \theta_t - \eta \frac{\mathbf{m}_t/(1-\beta_1^t)}{\sqrt{\mathbf{v}_t/(1-\beta_2^t)} + \epsilon}$</li>
<li><strong>批归一化</strong>: $\hat{\mathbf{x}} = \frac{\mathbf{x} - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}$</li>
<li><strong>层归一化</strong>: $\hat{x}_i = \frac{x_i - \mu_l}{\sqrt{\sigma_l^2 + \epsilon}}$</li>
</ul>
<p>优化是深度学习成功的关键。选择合适的优化器和超参数，结合现代加速技术，可以显著提升模型训练效率和最终性能。</p>
<h2 id="_9">练习题</h2>
<h3 id="_10">基础题</h3>
<p><strong>练习5.1</strong> 考虑一个二次函数 $f(x) = \frac{1}{2}x^TAx - b^Tx$，其中 $A$ 是正定矩阵，特征值为 $0 &lt; \lambda_{min} \leq ... \leq \lambda_{max}$。</p>
<ul>
<li>(a) 推导梯度下降的更新公式和收敛条件</li>
<li>(b) 证明当学习率 $\eta &lt; \frac{2}{\lambda_{max}(A)}$ 时，梯度下降收敛</li>
<li>(c) 分析动量如何改善条件数 $\kappa = \frac{\lambda_{max}}{\lambda_{min}}$ 的影响</li>
<li>(d) 计算达到 $\epsilon$-精度需要的迭代次数</li>
</ul>
<p><em>提示：考虑特征值分解 $A = Q\Lambda Q^T$，在特征空间分析收敛</em></p>
<details>
<summary>答案</summary>
<p>(a) 梯度为 $\nabla f(x) = Ax - b$，最优解 $x^* = A^{-1}b$
   更新公式：$x_{t+1} = x_t - \eta(Ax_t - b) = (I - \eta A)x_t + \eta b$</p>
<p>(b) 令 $e_t = x_t - x^*$ 为误差向量，则：
$$e_{t+1} = (I - \eta A)e_t$$
在特征空间 $\tilde{e}_t = Q^T e_t$：
$$\tilde{e}_{t+1} = (I - \eta \Lambda)\tilde{e}_t$$
收敛条件：$\rho(I - \eta A) &lt; 1$
   即 $|1 - \eta \lambda_i| &lt; 1$ 对所有特征值 $\lambda_i$
   这要求：$-1 &lt; 1 - \eta \lambda_i &lt; 1$
   因此：$0 &lt; \eta &lt; \frac{2}{\lambda_{max}}$</p>
<p>(c) 动量方法的特征多项式：
$$\rho^2 - (1 + \beta - \eta \lambda)\rho + \beta = 0$$</p>
<p>最优动量 $\beta_{opt} = \left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^2$
   收敛率从 $\frac{\kappa-1}{\kappa+1}$ 改善到 $\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}$</p>
<p>例如：$\kappa = 100$ 时，收敛率从0.98改善到0.82</p>
<p>(d) 达到 $|e_t| \leq \epsilon |e_0|$ 需要：</p>
<ul>
<li>无动量：$t \geq \frac{\log \epsilon}{\log(\frac{\kappa-1}{\kappa+1})} \approx \kappa \log(1/\epsilon)$</li>
<li>有动量：$t \geq \frac{\log \epsilon}{\log(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1})} \approx \sqrt{\kappa} \log(1/\epsilon)$</li>
</ul>
</details>
<p><strong>练习5.2</strong> 比较SGD和Adam在以下场景的表现：</p>
<ul>
<li>(a) 稀疏梯度（如词嵌入训练）</li>
<li>(b) 非平稳目标（如GAN训练）</li>
<li>(c) 需要精确收敛到最优解</li>
</ul>
<p><em>提示：考虑自适应学习率的影响</em></p>
<details>
<summary>答案</summary>
<p>(a) 稀疏梯度：Adam更好，因为它为每个参数维持独立的学习率，稀疏更新的参数能保持较大学习率</p>
<p>(b) 非平稳目标：Adam更好，自适应学习率能够快速适应目标函数的变化</p>
<p>(c) 精确收敛：SGD更好，Adam的自适应学习率可能导致在最优点附近振荡，而SGD配合学习率衰减能够精确收敛</p>
</details>
<p><strong>练习5.3</strong> 实现批归一化的前向和反向传播。给定输入 $\mathbf{x} \in \mathbb{R}^{B \times D}$（批量大小$B$，特征维度$D$），计算：</p>
<ul>
<li>(a) 前向传播输出</li>
<li>(b) 对输入的梯度 $\frac{\partial \mathcal{L}}{\partial \mathbf{x}}$</li>
<li>(c) 对参数 $\gamma, \beta$ 的梯度</li>
</ul>
<p><em>提示：使用链式法则，注意 $\mu$ 和 $\sigma^2$ 也依赖于 $\mathbf{x}$</em></p>
<details>
<summary>答案</summary>
<p>(a) 前向传播：</p>
<ul>
<li>$\mu = \frac{1}{B}\sum_i x_i$</li>
<li>$\sigma^2 = \frac{1}{B}\sum_i (x_i - \mu)^2$</li>
<li>$\hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}}$</li>
<li>$y_i = \gamma \hat{x}_i + \beta$</li>
</ul>
<p>(b) 反向传播（给定 $\frac{\partial \mathcal{L}}{\partial y}$）：</p>
<ul>
<li>$\frac{\partial \mathcal{L}}{\partial \hat{x}_i} = \frac{\partial \mathcal{L}}{\partial y_i} \cdot \gamma$</li>
<li>$\frac{\partial \mathcal{L}}{\partial \sigma^2} = \sum_i \frac{\partial \mathcal{L}}{\partial \hat{x}_i} \cdot (x_i - \mu) \cdot (-\frac{1}{2})(\sigma^2 + \epsilon)^{-3/2}$</li>
<li>$\frac{\partial \mathcal{L}}{\partial \mu} = \sum_i \frac{\partial \mathcal{L}}{\partial \hat{x}_i} \cdot (-\frac{1}{\sqrt{\sigma^2 + \epsilon}})$</li>
<li>$\frac{\partial \mathcal{L}}{\partial x_i} = \frac{\partial \mathcal{L}}{\partial \hat{x}_i} \cdot \frac{1}{\sqrt{\sigma^2 + \epsilon}} + \frac{\partial \mathcal{L}}{\partial \sigma^2} \cdot \frac{2(x_i - \mu)}{B} + \frac{\partial \mathcal{L}}{\partial \mu} \cdot \frac{1}{B}$</li>
</ul>
<p>(c) 参数梯度：</p>
<ul>
<li>$\frac{\partial \mathcal{L}}{\partial \gamma} = \sum_i \frac{\partial \mathcal{L}}{\partial y_i} \cdot \hat{x}_i$</li>
<li>$\frac{\partial \mathcal{L}}{\partial \beta} = \sum_i \frac{\partial \mathcal{L}}{\partial y_i}$</li>
</ul>
</details>
<p><strong>练习5.4</strong> 分析不同批量大小对优化的影响：</p>
<ul>
<li>(a) 批量大小如何影响梯度估计的方差？</li>
<li>(b) 为什么大批量训练可能导致泛化性能下降？</li>
<li>(c) 线性缩放规则的理论依据是什么？</li>
</ul>
<p><em>提示：考虑梯度噪声的作用</em></p>
<details>
<summary>答案</summary>
<p>(a) 梯度方差与批量大小成反比：$\text{Var}[\nabla_B] = \frac{\sigma^2}{B}$，其中 $B$ 是批量大小</p>
<p>(b) 大批量训练的问题：</p>
<ul>
<li>梯度噪声减少，可能陷入尖锐最小值（泛化性差）</li>
<li>探索能力下降，难以跳出局部最优</li>
<li>每个epoch的参数更新次数减少</li>
</ul>
<p>(c) 线性缩放规则：批量大小增加 $k$ 倍时，学习率也增加 $k$ 倍
   理论依据：保持每次更新的期望变化量不变
   $k$ 个小批量的梯度和 ≈ $k$ 倍的大批量梯度</p>
</details>
<h3 id="_11">挑战题</h3>
<p><strong>练习5.5</strong> 推导Adam优化器的收敛界。假设：</p>
<ul>
<li>梯度有界：$|\nabla f(x)| \leq G$</li>
<li>目标函数 $L$-smooth：$|\nabla f(x) - \nabla f(y)| \leq L|x - y|$</li>
</ul>
<p>证明Adam的收敛率为 $O(1/\sqrt{T})$。</p>
<p><em>提示：使用 regret bound 分析</em></p>
<details>
<summary>答案</summary>
<p>定义regret：$R_T = \sum_{t=1}^T f(x_t) - f(x^*)$</p>
<p>关键步骤：</p>
<ol>
<li>
<p>建立递归关系：利用L-smooth性质
   $f(x_{t+1}) \leq f(x_t) + \langle \nabla f(x_t), x_{t+1} - x_t \rangle + \frac{L}{2}|x_{t+1} - x_t|^2$</p>
</li>
<li>
<p>代入Adam更新规则，使用偏差修正后的估计</p>
</li>
<li>
<p>利用自适应学习率的性质：
   $\sum_{t=1}^T \frac{|g_t|^2}{\sqrt{v_t}} \leq 2G\sqrt{T\sum_{t=1}^T |g_t|^2}$</p>
</li>
<li>
<p>最终得到：$R_T \leq O(\sqrt{T})$，即平均收敛率 $O(1/\sqrt{T})$</p>
</li>
</ol>
</details>
<p><strong>练习5.6</strong> 设计一个自适应的学习率调度策略，要求：</p>
<ul>
<li>自动检测损失平台期</li>
<li>在平台期降低学习率</li>
<li>避免过早降低学习率</li>
</ul>
<p>描述你的算法并分析其优缺点。</p>
<p><em>提示：可以使用移动平均检测平台</em></p>
<details>
<summary>答案</summary>
<p><strong>自适应学习率调度算法</strong>：</p>
<div class="codehilite"><pre><span></span><code>初始化：patience = 10, factor = 0.5, threshold = 0.01
best_loss = inf, wait = 0

for epoch in training:
    current_loss = evaluate()
    relative_improvement = (best_loss - current_loss) / best_loss

    if relative_improvement &lt; threshold:
        wait += 1
        if wait &gt;= patience:
            lr = lr * factor
            wait = 0
    else:
        best_loss = current_loss
        wait = 0
</code></pre></div>

<p>优点：</p>
<ul>
<li>自动适应不同任务</li>
<li>避免手动调整</li>
<li>对噪声鲁棒（通过patience）</li>
</ul>
<p>缺点：</p>
<ul>
<li>需要调整超参数（patience, threshold）</li>
<li>可能错过最佳降低时机</li>
<li>对初始学习率敏感</li>
</ul>
</details>
<p><strong>练习5.7</strong> 分析ZeRO优化器的通信成本。给定：</p>
<ul>
<li>模型参数量 $P$</li>
<li>GPU数量 $N$</li>
<li>带宽 $B$</li>
</ul>
<p>计算ZeRO-1、ZeRO-2、ZeRO-3的通信量和内存节省。</p>
<p><em>提示：考虑前向、反向、参数更新的通信</em></p>
<details>
<summary>答案</summary>
<p><strong>内存占用</strong>（每个GPU）：</p>
<ul>
<li>标准数据并行：$16P$ 字节（FP16参数2P + FP32参数4P + 动量4P + 方差4P + 梯度2P）</li>
<li>ZeRO-1：$4P + 12P/N$ 字节（优化器状态分片）</li>
<li>ZeRO-2：$2P + 14P/N$ 字节（+梯度分片）</li>
<li>ZeRO-3：$16P/N$ 字节（全部分片）</li>
</ul>
<p><strong>通信量</strong>（每步）：</p>
<ul>
<li>ZeRO-1：$2P$ （all-reduce梯度）</li>
<li>ZeRO-2：$2P$ （scatter-reduce + all-gather）</li>
<li>ZeRO-3：$3P$ （额外的参数all-gather）</li>
</ul>
<p><strong>通信时间</strong>：</p>
<ul>
<li>ZeRO-1, ZeRO-2：$\frac{2P}{B \cdot N}$</li>
<li>ZeRO-3：$\frac{3P}{B \cdot N}$</li>
</ul>
<p>权衡：ZeRO-3内存最省但通信最多，适合带宽充足的场景</p>
</details>
<p><strong>练习5.8</strong> 设计一个结合一阶和二阶信息的混合优化器。要求：</p>
<ul>
<li>在优化初期使用一阶方法（快速）</li>
<li>接近收敛时切换到二阶方法（精确）</li>
<li>自动判断切换时机</li>
</ul>
<p><em>提示：可以监控梯度范数或损失变化率</em></p>
<details>
<summary>答案</summary>
<p><strong>混合优化器设计</strong>：</p>
<ol>
<li>
<p><strong>切换条件</strong>：
   - 梯度范数：$|\nabla f| &lt; \epsilon_g$
   - 损失变化率：$|f_t - f_{t-1}|/|f_{t-1}| &lt; \epsilon_f$
   - 迭代次数：$t &gt; t_{min}$</p>
</li>
<li>
<p><strong>算法</strong>：</p>
</li>
</ol>
<div class="codehilite"><pre><span></span><code>if t &lt; t_min or ||∇f|| &gt; ε_g:
    # 使用Adam（一阶）
    θ = Adam_update(θ, ∇f)
else:
    # 使用L-BFGS（二阶）
    if mod(t, update_freq) == 0:
        H_inv = approximate_hessian_inverse()
    θ = θ - η * H_inv @ ∇f
</code></pre></div>

<ol start="3">
<li><strong>实现细节</strong>：
   - 维护历史梯度用于L-BFGS
   - 平滑切换：逐渐增加二阶信息权重
   - 内存限制：只保存最近m个梯度对</li>
</ol>
<p>优点：结合快速探索和精确收敛
缺点：切换时机难以确定，需要额外内存</p>
</details>
<h2 id="_12">常见陷阱与调试技巧</h2>
<h3 id="1">1. 学习率选择错误</h3>
<p><strong>问题</strong>：学习率过大导致发散，过小导致收敛缓慢</p>
<p><strong>症状识别</strong>：</p>
<ul>
<li>过大：损失NaN/Inf，训练曲线剧烈振荡，梯度爆炸</li>
<li>过小：损失下降极慢，梯度范数很小，长时间无进展</li>
</ul>
<p><strong>调试技巧</strong>：</p>
<ul>
<li>
<p><strong>学习率范围测试（LR Range Test）</strong>：
  1. 从很小的学习率开始（如1e-7）
  2. 每个批次指数增加学习率
  3. 绘制损失vs学习率曲线
  4. 选择损失下降最快的区间</p>
</li>
<li>
<p><strong>1-cycle策略</strong>：
  1. 前半程：学习率从小增大到峰值
  2. 后半程：学习率从峰值降到很小
  3. 同时反向调整动量</p>
</li>
<li>
<p><strong>经验起点</strong>：</p>
</li>
<li>SGD: 0.1（CNN）, 0.01（其他）</li>
<li>Adam: 1e-3（标准）, 3e-4（Transformer）</li>
<li>大模型：按参数量缩放，如 $\eta \propto 1/\sqrt{N_{params}}$</li>
</ul>
<h3 id="2">2. 梯度爆炸/消失</h3>
<p><strong>问题</strong>：深层网络中梯度指数级增长或衰减</p>
<p><strong>解决方法</strong>：</p>
<ul>
<li>梯度裁剪：<code>clip_grad_norm_(parameters, max_norm=1.0)</code></li>
<li>合适的初始化（Xavier/He初始化）</li>
<li>使用归一化技术（BN/LN）</li>
<li>残差连接</li>
</ul>
<h3 id="3-adam">3. Adam的泛化问题</h3>
<p><strong>问题</strong>：Adam训练损失低但测试性能差</p>
<p><strong>解决方法</strong>：</p>
<ul>
<li>使用AdamW（权重衰减解耦）</li>
<li>最后阶段切换到SGD</li>
<li>调整 $\beta_2$（如0.98而非0.999）</li>
<li>使用更强的正则化</li>
</ul>
<h3 id="4">4. 批归一化的陷阱</h3>
<p><strong>问题</strong>：训练和测试性能差异大</p>
<p><strong>常见原因</strong>：</p>
<ul>
<li>批量大小太小（&lt;16）</li>
<li>忘记设置model.eval()模式</li>
<li>moving average统计量更新不当</li>
<li>数据分布shift</li>
</ul>
<p><strong>解决方法</strong>：</p>
<ul>
<li>使用组归一化或层归一化</li>
<li>确保足够的批量大小</li>
<li>调整momentum参数（默认0.1）</li>
</ul>
<h3 id="5_1">5. 大批量训练的困难</h3>
<p><strong>问题</strong>：增大批量后性能下降</p>
<p><strong>解决方法</strong>：</p>
<ul>
<li>线性缩放学习率</li>
<li>更长的预热期（warmup）</li>
<li>使用LARS/LAMB优化器</li>
<li>调整正则化强度</li>
</ul>
<h3 id="6">6. 混合精度训练不稳定</h3>
<p><strong>问题</strong>：FP16训练出现NaN或Inf</p>
<p><strong>解决方法</strong>：</p>
<ul>
<li>使用动态损失缩放</li>
<li>切换到BF16（如果硬件支持）</li>
<li>检查模型中的数值不稳定操作</li>
<li>在关键层保持FP32（如层归一化）</li>
</ul>
<h3 id="_13">调试工具推荐</h3>
<ol>
<li><strong>TensorBoard</strong>：可视化损失、梯度、权重分布</li>
<li><strong>梯度检查</strong>：数值梯度vs解析梯度</li>
<li><strong>学习率记录</strong>：确保调度器正常工作</li>
<li><strong>梯度直方图</strong>：检测梯度消失/爆炸</li>
<li><strong>激活值统计</strong>：监控各层输出范围</li>
</ol>
<h3 id="_14">经验法则总结</h3>
<ul>
<li>先用小数据集快速迭代</li>
<li>从简单模型开始，逐步增加复杂度</li>
<li>保持可重复性（固定随机种子）</li>
<li>记录所有超参数配置</li>
<li>定期保存检查点</li>
<li>监控多个指标，不只是损失</li>
</ul>
<p>记住：深度学习优化是科学也是艺术，需要系统的方法和经验积累。</p>
            </article>
            
            <nav class="page-nav"><a href="chapter4.html" class="nav-link prev">← 第4章：神经网络基础</a><a href="chapter6.html" class="nav-link next">第6章：卷积神经网络 →</a></nav>
        </main>
    </div>
</body>
</html>