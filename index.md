# 现代人工智能：优化与统计视角

## 前言

本书从优化理论和统计学的视角系统介绍现代人工智能的核心概念与方法。我们将理论见解与实用技巧相结合，帮助读者建立对AI技术的深刻理解。

每个章节都包含大量练习题，旨在巩固概念理解并培养实践直觉。本书适合AI初学者以及希望刷新知识体系的从业者。

## 目录

### 第一部分：基础理论

**[第1章：优化基础与梯度下降](chapter1.md)**
- 凸优化与非凸优化
- 梯度下降及其变体
- 收敛性分析
- 学习率选择的经验法则

**[第2章：统计学习理论](chapter2.md)**
- 偏差-方差权衡
- PAC学习理论
- VC维与泛化界
- 正则化的统计解释

**[第3章：线性模型与正则化](chapter3.md)**
- 最小二乘法与岭回归
- LASSO与稀疏性
- 弹性网络
- 贝叶斯线性回归

### 第二部分：深度学习

**[第4章：神经网络基础](chapter4.md)**
- 反向传播算法
- 激活函数的选择
- 初始化策略
- 梯度消失与爆炸问题

**[第5章：深度学习优化](chapter5.md)**
- SGD及其动量变体
- Adam与自适应学习率
- 批归一化与层归一化
- 二阶优化方法

**[第6章：卷积神经网络](chapter6.md)**
- 卷积操作的统计解释
- 感受野与特征层次
- 残差连接与深度
- 数据增强策略

### 第三部分：序列模型与注意力机制

**[第7章：循环神经网络与序列建模](chapter7.md)**
- BPTT与梯度问题
- LSTM与GRU
- 序列到序列模型
- 束搜索与解码策略

**[第8章：Transformer与注意力机制](chapter8.md)**
- 自注意力的数学原理
- 位置编码设计
- 多头注意力
- 计算复杂度分析

**[第9章：大语言模型](chapter9.md)**
- GPT架构演进
- 缩放定律
- 上下文学习与提示工程
- RLHF与对齐技术

### 第四部分：生成模型

**[第10章：变分自编码器与生成建模](chapter10.md)**
- ELBO与变分推断
- 重参数化技巧
- β-VAE与解耦表示
- 后验崩塌问题

**[第11章：扩散模型](chapter11.md)**
- 前向与反向扩散过程
- 去噪分数匹配
- DDPM与DDIM
- 条件生成与引导

### 第五部分：强化学习

**[第12章：深度强化学习](chapter12.md)**
- 价值函数近似
- 策略梯度方法
- Actor-Critic架构
- AlphaGo与自我对弈
- OpenAI Five与多智能体学习

## 数学符号约定

- $\mathbf{x}$：向量
- $\mathbf{X}$：矩阵
- $\mathcal{D}$：数据集
- $\mathbb{E}$：期望
- $\mathcal{L}$：损失函数
- $\nabla$：梯度
- $\theta$：模型参数
- $\eta$：学习率
- $\lambda$：正则化系数

## 如何使用本书

1. **循序渐进**：章节按难度递增编排，建议按顺序学习
2. **动手练习**：每章练习题分为基础题和挑战题，答案可展开查看
3. **关注直觉**：重点理解概念背后的直觉，而非死记公式
4. **实用为主**：书中提供大量经验法则(rules of thumb)，可直接应用于实践

## 致谢

感谢所有为现代AI发展做出贡献的研究者和工程师。本书试图将这些智慧结晶以易懂的方式呈现给中文读者。

---

开始学习：[第1章：优化基础与梯度下降](chapter1.md) →