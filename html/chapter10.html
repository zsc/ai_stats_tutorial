<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <base href="./">
    <title>第10章：变分自编码器与生成建模</title>
    <link rel="stylesheet" href="assets/style.css">
    <link rel="stylesheet" href="assets/highlight.css">
    <script src="assets/script.js" defer></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$']],
                displayMath: [['$$', '$$']],
                processEscapes: false,
                packages: {'[+]': ['noerrors', 'ams']}
            },
            options: {
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            },
            loader: {
                load: ['[tex]/noerrors', '[tex]/ams']
            }
        };
    </script>
</head>
<body>
    <div class="container">
        <nav id="sidebar" class="sidebar">
            <div class="sidebar-header">
                <h3>目录</h3>
                <button id="sidebar-toggle" class="sidebar-toggle">
                    <span></span>
                    <span></span>
                    <span></span>
                </button>
            </div>
            <div class="sidebar-search">
                <input type="text" id="sidebar-search-input" placeholder="搜索..." autocomplete="off">
            </div>
            <div id="tree-container">
                <nav class="tree-nav" role="tree">
                    <div class="tree-item " >
                        <a href="index.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">现代人工智能：优化与统计视角</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter1.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第1章：优化基础与梯度下降</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter2.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第2章：统计学习理论</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter3.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第3章：线性模型与正则化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter4.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第4章：神经网络基础</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter5.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第5章：深度学习优化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter6.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第6章：卷积神经网络</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter7.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第7章：循环神经网络与序列建模</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter8.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第8章：Transformer与注意力机制</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter9.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第9章：大语言模型</span>
                        </a>
                    </div>
                
                    <div class="tree-item active" >
                        <a href="chapter10.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第10章：变分自编码器与生成建模</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter11.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第11章：扩散模型</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter12.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第12章：深度强化学习</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="CLAUDE.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Untitled</span>
                        </a>
                    </div>
                </nav>
            </div>
        </nav>
        
        <main class="content">
            <article>
                <h1 id="10">第10章：变分自编码器与生成建模</h1>
<p>变分自编码器（VAE）优雅地结合了贝叶斯推断与深度学习，为生成模型开辟了新的方向。本章将从统计推断的视角深入理解VAE的原理，掌握其训练技巧，并探讨在现代AI系统中的应用。我们将看到，VAE不仅是一个生成模型，更是理解表示学习的重要框架。</p>
<h2 id="101">10.1 从自编码器到变分自编码器</h2>
<h3 id="1011">10.1.1 传统自编码器的局限</h3>
<p>传统自编码器是深度学习早期的重要架构，通过瓶颈结构强制网络学习数据的压缩表示。其基本架构遵循编码-解码范式：</p>
<div class="codehilite"><pre><span></span><code>输入 x → 编码器 f(x) → 隐变量 z → 解码器 g(z) → 重构 x̂
         (降维压缩)      (瓶颈层)      (升维重构)
</code></pre></div>

<p>经典的自编码器通过最小化重构误差来训练：
$$\mathcal{L}_{AE} = |x - \hat{x}|^2 = |x - g(f(x))|^2$$
这种方法在降维和特征学习方面取得了成功，但存在根本性的限制。</p>
<p><strong>主要问题</strong>：</p>
<ol>
<li>
<p><strong>隐空间的不规则性</strong>：传统自编码器没有对隐空间施加任何约束，导致编码器可能将相似的输入映射到隐空间中相距很远的点。例如，两张相似的人脸图像可能被编码到完全不同的隐向量，使得隐空间缺乏语义连续性。</p>
</li>
<li>
<p><strong>生成能力的缺失</strong>：由于隐空间的不规则性，我们无法从隐空间中随机采样来生成有意义的新样本。隐空间中的大部分区域可能对应着无意义的重构结果——这些区域在训练时从未被访问过。</p>
</li>
<li>
<p><strong>缺乏不确定性建模</strong>：传统自编码器输出确定性的隐表示，无法捕捉数据的内在随机性或模型的认知不确定性。对于同一输入，模型总是产生相同的隐编码，这在许多实际应用中是不合理的。</p>
</li>
<li>
<p><strong>过拟合风险</strong>：没有正则化的自编码器容易记忆训练数据，特别是当隐维度较高时。模型可能学会为每个训练样本分配一个独特的隐编码，失去泛化能力。</p>
</li>
</ol>
<h3 id="1012">10.1.2 生成模型的概率视角</h3>
<p>变分自编码器的革命性在于将自编码器重新定义为概率生成模型。这种范式转变不仅解决了传统自编码器的问题，还开辟了贝叶斯深度学习的新方向。</p>
<p><strong>概率生成模型的基本假设</strong>：</p>
<p>VAE假设观察数据$x$是由一个简单的隐变量$z$通过复杂的非线性变换生成的。这个假设反映了一个深刻的信念：高维复杂数据（如图像）实际上由低维的生成因子（如物体类型、姿态、光照等）决定。</p>
<ol>
<li><strong>生成过程的层次结构</strong>：</li>
</ol>
<p>VAE定义了一个两阶段的生成过程：</p>
<ul>
<li>
<p><strong>第一阶段</strong>：从简单的先验分布采样隐变量：$z \sim p(z) = \mathcal{N}(0, I)$</p>
<p>选择标准高斯分布作为先验有多重考虑：数学上易处理、具有最大熵（信息论意义上最"无偏"）、便于采样和计算KL散度。</p>
</li>
<li>
<p><strong>第二阶段</strong>：通过参数化的条件分布生成数据：$x \sim p_\theta(x|z)$</p>
<p>这里$\theta$表示解码器网络的参数，解码器定义了从隐空间到数据空间的概率映射。</p>
</li>
</ul>
<ol start="2">
<li><strong>推断问题的本质</strong>：</li>
</ol>
<p>生成模型定义了从$z$到$x$的前向过程，但在实际应用中，我们通常需要解决逆问题：给定观察数据$x$，推断其最可能的隐变量$z$。这就是贝叶斯推断的核心任务。</p>
<p>根据贝叶斯定理，后验分布为：
$$p(z|x) = \frac{p(x|z)p(z)}{p(x)} = \frac{p(x|z)p(z)}{\int p(x|z)p(z)dz}$$
这个公式看似简单，但分母的积分$p(x) = \int p(x|z)p(z)dz$在高维空间中几乎不可能解析计算。</p>
<ol start="3">
<li><strong>难解性的根源</strong>：</li>
</ol>
<p>边际似然$p(x)$的计算需要对所有可能的隐变量$z$进行积分，这涉及：</p>
<ul>
<li><strong>高维积分</strong>：隐空间通常是几十到几百维</li>
<li><strong>复杂的被积函数</strong>：$p(x|z)$由深度神经网络参数化，高度非线性</li>
<li><strong>没有共轭性</strong>：深度网络破坏了分布族的共轭结构</li>
</ul>
<p>传统的数值积分方法（如蒙特卡洛）在高维空间中收敛极慢，实际不可行。</p>
<p><strong>从确定性到概率性的思维转变</strong>：</p>
<p>VAE的创新不仅在于技术细节，更在于思维方式的转变：</p>
<ul>
<li><strong>从点估计到分布估计</strong>：不再寻找单一的最优隐表示，而是学习隐表示的分布</li>
<li><strong>从优化到推断</strong>：将学习问题转化为贝叶斯推断问题</li>
<li><strong>从判别到生成</strong>：不仅能编码数据，还能生成新数据</li>
</ul>
<p>这种概率视角为后续的变分推断方法奠定了理论基础。</p>
<h2 id="102-elbo">10.2 变分推断与ELBO</h2>
<h3 id="1021">10.2.1 变分推断的核心思想</h3>
<p>变分推断是贝叶斯统计中的一个强大工具，它将难解的推断问题转化为优化问题。这个思想可以追溯到物理学中的变分原理——通过优化来寻找系统的平衡态。</p>
<p><strong>变分推断的基本策略</strong>：</p>
<p>面对难以计算的真实后验分布$p(z|x)$，变分推断采用"以简代繁"的策略：我们选择一个参数化的分布族$\mathcal{Q} = \{q_\phi(z|x): \phi \in \Phi\}$，然后在这个族中寻找最接近真实后验的分布。
$$q_\phi^*(z|x) = \arg\min_{q_\phi \in \mathcal{Q}} \text{KL}[q_\phi(z|x) | p(z|x)]$$
这里$\phi$是变分参数，通常对应编码器神经网络的权重。选择$q_\phi(z|x)$为高斯分布族是常见做法：
$$q_\phi(z|x) = \mathcal{N}(z; \mu_\phi(x), \sigma_\phi^2(x)I)$$
其中$\mu_\phi(x)$和$\sigma_\phi^2(x)$由编码器网络输出。</p>
<p><strong>KL散度作为距离度量</strong>：</p>
<p>KL散度（Kullback-Leibler散度）衡量两个概率分布的"距离"：
$$\text{KL}[q_\phi(z|x) | p(z|x)] = \mathbb{E}_{q_\phi(z|x)}\left[\log\frac{q_\phi(z|x)}{p(z|x)}\right]$$
虽然KL散度不是对称的（$\text{KL}[q|p] \neq \text{KL}[p|q]$），但在变分推断中，我们特意选择$\text{KL}[q|p]$而非$\text{KL}[p|q]$，原因包括：</p>
<ul>
<li>计算只需要从$q$采样，而$q$是我们可以控制的</li>
<li>这种方向的KL散度倾向于让$q$的支撑集小于$p$的支撑集（模式寻找行为）</li>
<li>数学上可以导出ELBO，便于优化</li>
</ul>
<p><strong>变分推断的哲学意义</strong>：</p>
<p>变分推断体现了一个深刻的思想：当精确解不可得时，我们可以在一个受限但可处理的空间中寻找最优近似。这种思想贯穿了现代机器学习的许多领域，从近似推断到强化学习中的策略优化。</p>
<h3 id="1022-elbo">10.2.2 ELBO的推导</h3>
<p>Evidence Lower Bound（ELBO）的推导是VAE理论的核心，它巧妙地将不可计算的KL散度转化为可优化的目标函数。</p>
<p><strong>关键洞察：对数边际似然的分解</strong></p>
<p>我们从对数边际似然$\log p(x)$开始。注意到$\log p(x)$不依赖于$z$，因此：
$$\log p(x) = \mathbb{E}_{q_\phi(z|x)}[\log p(x)]$$
这看似平凡的等式是推导的起点。接下来，我们利用条件概率的定义$p(x) = \frac{p(x,z)}{p(z|x)}$：
$$\log p(x) = \mathbb{E}_{q_\phi(z|x)}\left[\log\frac{p(x,z)}{p(z|x)}\right]$$
现在引入一个巧妙的技巧——乘以1，但以$\frac{q_\phi(z|x)}{q_\phi(z|x)}$的形式：
$$\log p(x) = \mathbb{E}_{q_\phi(z|x)}\left[\log\frac{p(x,z)}{q_\phi(z|x)} \cdot \frac{q_\phi(z|x)}{p(z|x)}\right]$$
利用对数的性质$\log(ab) = \log a + \log b$：
$$\log p(x) = \mathbb{E}_{q_\phi(z|x)}\left[\log\frac{p(x,z)}{q_\phi(z|x)}\right] + \mathbb{E}_{q_\phi(z|x)}\left[\log\frac{q_\phi(z|x)}{p(z|x)}\right]$$
第二项正是KL散度的定义：
$$\log p(x) = \underbrace{\mathbb{E}_{q_\phi(z|x)}\left[\log\frac{p(x,z)}{q_\phi(z|x)}\right]}_{\text{ELBO}(\phi,\theta)} + \underbrace{\text{KL}[q_\phi(z|x) | p(z|x)]}_{\geq 0}$$
<strong>ELBO的性质与意义</strong>：</p>
<p>由于KL散度非负，我们得到：
$$\log p(x) \geq \text{ELBO}(\phi,\theta)$$
这个不等式告诉我们几个重要事实：</p>
<ol>
<li><strong>ELBO是证据的下界</strong>：ELBO永远不会超过对数边际似然（证据）</li>
<li><strong>等号成立条件</strong>：当且仅当$q_\phi(z|x) = p(z|x)$时，ELBO等于$\log p(x)$</li>
<li><strong>双重优化目标</strong>：最大化ELBO既增加了边际似然的下界，又减小了近似后验与真实后验的差距</li>
</ol>
<h3 id="1023-elbo">10.2.3 ELBO的可解释形式</h3>
<p>将ELBO展开为更具洞察力的形式，我们可以看到其深层的统计含义。</p>
<p><strong>标准分解</strong>：</p>
<p>通过展开联合概率$p(x,z) = p(x|z)p(z)$：
$$\text{ELBO} = \mathbb{E}_{q_\phi(z|x)}\left[\log\frac{p_\theta(x|z)p(z)}{q_\phi(z|x)}\right]$$</p>
<p>$$= \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] + \mathbb{E}_{q_\phi(z|x)}\left[\log\frac{p(z)}{q_\phi(z|x)}\right]$$</p>
<p>$$= \underbrace{\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]}_{\text{重构项}} - \underbrace{\text{KL}[q_\phi(z|x) | p(z)]}_{\text{正则项}}$$
<strong>深入理解两个关键项</strong>：</p>
<ol>
<li><strong>重构项（Reconstruction Term）</strong>：
$$\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]$$
这项衡量了通过隐变量$z$重构原始数据$x$的能力：</li>
</ol>
<ul>
<li>对于高斯解码器：等价于均方误差的负值</li>
<li>对于伯努利解码器：等价于二元交叉熵的负值</li>
<li>直观理解：确保编码后的信息足以重构原始数据</li>
<li>信息论视角：最大化条件对数似然，保留重要信息</li>
</ul>
<ol start="2">
<li><strong>正则项（Regularization Term）</strong>：
$$-\text{KL}[q_\phi(z|x) | p(z)]$$
这项约束近似后验接近先验分布：</li>
</ol>
<ul>
<li>防止后验分布偏离先验太远</li>
<li>确保隐空间的规整性和连续性</li>
<li>使得从先验采样能生成合理的样本</li>
<li>信息论视角：限制隐变量携带的信息量（信息瓶颈）</li>
</ul>
<p><strong>权衡与平衡</strong>：</p>
<p>ELBO揭示了VAE中的根本权衡：</p>
<ul>
<li><strong>过度重构</strong>：如果只优化重构项，编码器可能为每个样本学习独特的编码，失去泛化能力</li>
<li><strong>过度正则</strong>：如果正则项过强，所有样本都被编码到相同分布，失去区分能力</li>
<li><strong>最优平衡</strong>：在保持良好重构的同时，维持隐空间的规整性</li>
</ul>
<p><strong>信息论的深层含义</strong>：</p>
<p>从信息论角度，ELBO可以重写为：
$$\text{ELBO} = -\mathbb{E}_{q_\phi}[\text{失真}] - \beta \cdot I(X; Z)$$
其中失真度量重构误差，$I(X; Z)$是输入和隐变量的互信息。这揭示了VAE在进行有损压缩：在限定信息传输率的条件下最小化失真。</p>
<p><strong>Rule of Thumb</strong>：</p>
<ul>
<li>训练初期，可以使用较小的KL权重（如0.1），让模型先学会重构</li>
<li>逐渐增加KL权重到1，实现标准ELBO</li>
<li>对于复杂数据集，考虑使用循环退火策略</li>
<li>监控两项的相对大小：理想情况下，两项应该在同一数量级</li>
</ul>
<h2 id="103-vae">10.3 VAE的架构与实现细节</h2>
<h3 id="1031">10.3.1 编码器设计</h3>
<p>编码器是VAE的认知器官，负责将高维观察压缩为低维的概率表示。其设计直接影响模型的表示能力和训练稳定性。</p>
<p><strong>架构的概率解释</strong>：</p>
<p>编码器实现了推断网络$q_\phi(z|x)$，输出一个条件高斯分布：</p>
<div class="codehilite"><pre><span></span><code>        输入 x (D维)
          ↓
    共享层 h = f_φ(x)
    (特征提取网络)
          ↓
    ┌─────┴─────┐
    ↓           ↓
  μ层: W_μh+b_μ  logσ²层: W_σh+b_σ
    ↓           ↓
  μ(x) ∈ R^d   logσ²(x) ∈ R^d

输出: q_φ(z|x) = N(μ(x), diag(exp(logσ²(x))))
</code></pre></div>

<p><strong>关键设计决策及其原理</strong>：</p>
<ol>
<li><strong>为什么使用对角协方差矩阵？</strong></li>
</ol>
<p>完整的协方差矩阵需要$O(d^2)$参数，而对角形式只需$O(d)$：</p>
<ul>
<li><strong>计算效率</strong>：对于$d=100$的隐空间，完整协方差需要10,000个参数，对角只需100个</li>
<li><strong>优化稳定性</strong>：避免协方差矩阵的正定性约束</li>
<li><strong>经验发现</strong>：对角协方差通常足够表达，因为神经网络已经学习了特征的去相关</li>
</ul>
<p>如果需要更灵活的后验，可以考虑：</p>
<ul>
<li>归一化流（Normalizing Flows）</li>
<li>低秩加对角（Low-rank plus diagonal）</li>
<li>混合高斯（Mixture of Gaussians）</li>
</ul>
<ol start="2">
<li><strong>为什么输出log方差？</strong></li>
</ol>
<p>直接输出方差存在严重的数值问题：</p>
<div class="codehilite"><pre><span></span><code>问题：σ² = network(x)  # 可能输出负值或极大值
解决：logσ² = network(x); σ² = exp(logσ²)
</code></pre></div>

<p>log方差的优势：</p>
<ul>
<li><strong>无界输出</strong>：网络可以输出任意实数，exp保证方差为正</li>
<li><strong>梯度稳定</strong>：避免方差接近0时的梯度爆炸</li>
<li><strong>对称范围</strong>：logσ²可以对称地表示大方差和小方差</li>
</ul>
<ol start="3">
<li><strong>网络深度与容量的选择</strong>：</li>
</ol>
<p>编码器的容量需要平衡：</p>
<ul>
<li><strong>太浅</strong>：无法捕捉数据的复杂模式</li>
<li><strong>太深</strong>：可能导致后验崩塌，编码器"太聪明"</li>
</ul>
<p>典型架构：</p>
<ul>
<li>MNIST：2-3层全连接，每层400-800单元</li>
<li>CIFAR-10：4-5层卷积 + 2层全连接</li>
<li>高分辨率图像：ResNet或VGG风格的深层网络</li>
</ul>
<h3 id="1032">10.3.2 解码器设计</h3>
<p>解码器是VAE的生成器官，将隐空间的点映射回数据空间的概率分布。其设计需要匹配数据的统计特性。</p>
<p><strong>输出分布的选择原则</strong>：</p>
<p>解码器定义条件分布$p_\theta(x|z)$，选择取决于数据类型和建模假设：</p>
<ol>
<li><strong>连续数据 - 高斯分布</strong>：
$$p_\theta(x|z) = \mathcal{N}(x; \mu_\theta(z), \sigma^2I)$$
适用场景：</li>
</ol>
<ul>
<li>自然图像的原始像素值</li>
<li>音频波形</li>
<li>连续传感器数据</li>
</ul>
<p>实践细节：</p>
<ul>
<li>固定方差：通常设$\sigma^2=1$，简化为MSE损失</li>
<li>学习方差：输出均值和log方差，但训练更困难</li>
<li>异方差：每个维度独立的方差，增加灵活性</li>
</ul>
<ol start="2">
<li><strong>二值数据 - 伯努利分布</strong>：
$$p_\theta(x|z) = \prod_{i=1}^D \text{Bernoulli}(x_i; p_i)$$
其中$p_i = \text{sigmoid}(\text{network}_\theta(z)_i)$</li>
</ol>
<p>适用场景：</p>
<ul>
<li>二值图像（如MNIST）</li>
<li>离散特征</li>
<li>0-1标签</li>
</ul>
<p>损失函数自然成为二元交叉熵。</p>
<ol start="3">
<li>
<p><strong>分类数据 - 分类分布</strong>：
$$p_\theta(x|z) = \text{Categorical}(\text{softmax}(\text{network}_\theta(z)))$$
用于离散类别数据或分词后的文本。</p>
</li>
<li>
<p><strong>多峰数据 - 混合模型</strong>：
$$p_\theta(x|z) = \sum_{k=1}^K \pi_k(z) \mathcal{N}(x; \mu_k(z), \Sigma_k)$$
适合复杂的多模态分布。</p>
</li>
</ol>
<p><strong>解码器架构的对称性原则</strong>：</p>
<p>通常解码器镜像编码器的架构：</p>
<ul>
<li>编码器用卷积 → 解码器用转置卷积</li>
<li>编码器降采样 → 解码器上采样</li>
<li>编码器池化 → 解码器上采样</li>
</ul>
<p>这种对称性有助于信息的保留和重构。</p>
<h3 id="1033">10.3.3 损失函数的实际形式</h3>
<p>将理论ELBO转化为可计算的损失函数需要仔细的推导和实现。</p>
<p><strong>完整损失函数的推导</strong>：</p>
<p>对于最常见的设置（高斯编码器、高斯解码器、标准高斯先验），ELBO展开为：
$$\mathcal{L}_{VAE} = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - \text{KL}[q_\phi(z|x) | p(z)]$$
<strong>重构项的具体形式</strong>：</p>
<ol>
<li>
<p><strong>高斯解码器</strong>（固定方差$\sigma^2=1$）：
$$\mathbb{E}_{q}[\log p_\theta(x|z)] = -\frac{1}{2}\mathbb{E}_{q}[|x - \mu_\theta(z)|^2] + \text{const}$$
实践中使用单样本估计：
$$\approx -\frac{1}{2}|x - \hat{x}|^2$$</p>
</li>
<li>
<p><strong>伯努利解码器</strong>：
$$\mathbb{E}_{q}[\log p_\theta(x|z)] = \mathbb{E}_{q}\left[\sum_i x_i\log p_i + (1-x_i)\log(1-p_i)\right]$$
这正是二元交叉熵。</p>
</li>
</ol>
<p><strong>KL项的闭式解</strong>：</p>
<p>对于两个高斯分布$q = \mathcal{N}(\mu_q, \Sigma_q)$和$p = \mathcal{N}(\mu_p, \Sigma_p)$：
$$\text{KL}[q|p] = \frac{1}{2}\left[\text{tr}(\Sigma_p^{-1}\Sigma_q) + (\mu_p-\mu_q)^T\Sigma_p^{-1}(\mu_p-\mu_q) - d + \log\frac{|\Sigma_p|}{|\Sigma_q|}\right]$$
当$p = \mathcal{N}(0, I)$且$q$使用对角协方差时：
$$\text{KL} = \frac{1}{2}\sum_{i=1}^d \left[\mu_i^2 + \sigma_i^2 - \log\sigma_i^2 - 1\right]$$
<strong>批处理和估计器</strong>：</p>
<p>实际实现中，对于批次大小$B$：
$$\mathcal{L}_{batch} = \frac{1}{B}\sum_{i=1}^B \left[\text{重构损失}(x_i, \hat{x}_i) + \text{KL损失}(q_\phi(z|x_i), p(z))\right]$$
<strong>数值稳定性技巧</strong>：</p>
<ol>
<li><strong>KL项的阈值</strong>：</li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="n">kl</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.5</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="n">mu</span><span class="o">**</span><span class="mi">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nb">exp</span><span class="p">(</span><span class="n">logvar</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">logvar</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span>
<span class="n">kl</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">kl</span><span class="p">,</span><span class="w"> </span><span class="nb">min</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span><span class="w">  </span><span class="c1"># 数值误差可能导致负值</span>
</code></pre></div>

<ol start="2">
<li><strong>重构项的缩放</strong>：
   对于高维数据，重构项可能主导损失：</li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="n">loss</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">reconstruction</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">data_dim</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">beta</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">kl</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">latent_dim</span>
</code></pre></div>

<ol start="3">
<li><strong>梯度裁剪</strong>：
   防止训练初期的梯度爆炸：</li>
</ol>
<div class="codehilite"><pre><span></span><code>torch.nn.utils.clip_grad_norm_(parameters, max_norm=1.0)
</code></pre></div>

<p><strong>Rule of Thumb</strong>：</p>
<ul>
<li>对于图像数据，通常使用高斯解码器配合MSE损失</li>
<li>对于二值数据，使用伯努利解码器配合BCE损失</li>
<li>KL项在训练初期可能不稳定，考虑使用warmup策略</li>
<li>监控KL项的每个维度，识别"死亡"维度（KL接近0）</li>
</ul>
<h2 id="104">10.4 重参数化技巧</h2>
<h3 id="1041">10.4.1 梯度传播的挑战</h3>
<p>从$q_\phi(z|x)$采样是随机操作，无法直接反向传播梯度：</p>
<div class="codehilite"><pre><span></span><code>不可微的采样过程：
x → 编码器 → (μ, σ²) → 采样 z ~ N(μ, σ²) → 解码器 → x̂
                            ↑
                      梯度无法通过
</code></pre></div>

<h3 id="1042">10.4.2 重参数化的核心思想</h3>
<p>将随机性从参数中分离出来：
$$z = \mu + \sigma \odot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)$$
其中$\odot$表示逐元素乘积。</p>
<div class="codehilite"><pre><span></span><code>可微的重参数化：
x → 编码器 → (μ, σ²) ──────→ z = μ + σ⊙ε → 解码器 → x̂
                    ↑                ↑
                梯度可传播        ε ~ N(0,I)
</code></pre></div>

<h3 id="1043">10.4.3 重参数化的一般形式</h3>
<p>对于其他分布族，重参数化技巧可以推广：</p>
<ul>
<li><strong>Gumbel-Softmax</strong>：用于离散分布的连续松弛</li>
<li><strong>Gamma分布</strong>：使用形状-尺度参数化</li>
<li><strong>Beta分布</strong>：通过Kumaraswamy分布近似</li>
</ul>
<p><strong>Rule of Thumb</strong>：当需要从复杂分布采样时，优先考虑是否存在可重参数化的形式。</p>
<h2 id="105-vae">10.5 β-VAE与解耦表示</h2>
<h3 id="1051">10.5.1 解耦表示的动机</h3>
<p>理想的表示应该将数据的独立生成因子分离到不同的隐变量维度：</p>
<div class="codehilite"><pre><span></span><code>原始图像          解耦的隐变量
┌────────┐      z₁: 物体类型
│ 红色   │      z₂: 颜色
│ 圆形   │  →   z₃: 形状
│ 物体   │      z₄: 位置
└────────┘      z₅: 大小
</code></pre></div>

<h3 id="1052-vae">10.5.2 β-VAE的损失函数</h3>
<p>β-VAE通过调整KL项的权重来促进解耦：
$$\mathcal{L}_{\beta\text{-VAE}} = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - \beta \cdot \text{KL}[q_\phi(z|x) | p(z)]$$</p>
<ul>
<li>$\beta &gt; 1$：增强正则化，促进解耦但可能降低重构质量</li>
<li>$\beta &lt; 1$：减弱正则化，提高重构质量但降低解耦程度</li>
</ul>
<h3 id="1053">10.5.3 解耦度的定量评估</h3>
<p><strong>互信息间隙（MIG）</strong>：
$$\text{MIG} = \frac{1}{K}\sum_{k=1}^K \frac{I(z_j^*; v_k) - I(z_j^{**}; v_k)}{H(v_k)}$$
其中$v_k$是第$k$个生成因子，$z_j^*$和$z_j^{**}$是互信息最高和次高的隐变量。</p>
<p><strong>Rule of Thumb</strong>：$\beta$的选择依赖于任务。对于需要可解释表示的任务，通常$\beta \in [4, 10]$；对于纯生成任务，$\beta \in [1, 2]$。</p>
<h2 id="106">10.6 后验崩塌问题</h2>
<h3 id="1061">10.6.1 问题描述</h3>
<p>后验崩塌是VAE训练中的常见问题，表现为：</p>
<ul>
<li>近似后验退化为先验：$q_\phi(z|x) \approx p(z)$</li>
<li>隐变量不携带信息：$I(x; z) \approx 0$</li>
<li>解码器忽略隐变量，退化为无条件生成器</li>
</ul>
<h3 id="1062">10.6.2 问题的根源</h3>
<p>从优化角度分析，当解码器足够强大时，可能出现局部最优：
$$\text{KL}[q_\phi(z|x) | p(z)] = 0 \Rightarrow q_\phi(z|x) = p(z)$$
此时ELBO简化为：
$$\text{ELBO} = \mathbb{E}_{p(z)}[\log p_\theta(x|z)]$$</p>
<h3 id="1063">10.6.3 解决方案</h3>
<p><strong>1. KL退火（KL Annealing）</strong>：
渐进式增加KL项的权重：
$$\mathcal{L}_t = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - \lambda_t \cdot \text{KL}[q_\phi(z|x) | p(z)]$$
其中$\lambda_t$从0逐渐增加到1。</p>
<p><strong>2. 自由比特（Free Bits）</strong>：
为每个隐变量维度设置最小信息量：
$$\mathcal{L}_{FB} = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - \sum_i \max(\lambda, \text{KL}_i)$$</p>
<p><strong>3. 降低解码器容量</strong>：
使用较浅的解码器网络，迫使模型利用隐变量。</p>
<p><strong>Rule of Thumb</strong>：对于文本VAE，后验崩塌尤其严重，建议结合多种技术；对于图像VAE，通常KL退火就足够。</p>
<h2 id="107-vae">10.7 VAE的变体与扩展</h2>
<h3 id="1071-vaecvae">10.7.1 条件VAE（CVAE）</h3>
<p>引入条件信息$c$（如类别标签）：
$$q_\phi(z|x,c), \quad p_\theta(x|z,c)$$
应用场景：</p>
<ul>
<li>可控生成：指定属性生成样本</li>
<li>半监督学习：利用少量标签改善表示</li>
</ul>
<h3 id="1072-vaehierarchical-vae">10.7.2 层次VAE（Hierarchical VAE）</h3>
<p>使用多层隐变量构建更丰富的表示：</p>
<div class="codehilite"><pre><span></span><code>x ← p(x|z₁) ← z₁ ← p(z₁|z₂) ← z₂ ← p(z₂)
</code></pre></div>

<p>优势：</p>
<ul>
<li>捕获多尺度特征</li>
<li>更灵活的先验分布</li>
<li>改善后验崩塌问题</li>
</ul>
<h3 id="1073-vaevq-vae">10.7.3 向量量化VAE（VQ-VAE）</h3>
<p>使用离散隐表示和码本：
$$z_q = \arg\min_{e_k \in \mathcal{E}} |z_e - e_k|_2$$
其中$\mathcal{E} = \{e_1, ..., e_K\}$是可学习的码本。</p>
<p>特点：</p>
<ul>
<li>离散隐空间便于建模</li>
<li>避免后验崩塌</li>
<li>可与自回归模型结合</li>
</ul>
<h2 id="108-diederik-p-kingmavae">10.8 历史人物：Diederik P. Kingma与VAE的诞生</h2>
<h3 id="1081">10.8.1 学术背景</h3>
<p>Diederik P. Kingma在阿姆斯特丹大学攻读博士期间，专注于概率模型和深度学习的结合。2013年，他与导师Max Welling共同提出了变分自编码器，几乎同时，Danilo Rezende等人独立提出了类似的想法。</p>
<h3 id="1082">10.8.2 关键贡献</h3>
<ol>
<li><strong>重参数化技巧的创新</strong></li>
</ol>
<p>Kingma最重要的贡献是重参数化技巧，巧妙地解决了随机节点的梯度传播问题。这个看似简单的技巧实际上打开了随机计算图优化的大门。</p>
<ol start="2">
<li><strong>Adam优化器</strong></li>
</ol>
<p>同年，Kingma还与Jimmy Ba共同提出了Adam优化器，成为深度学习中最流行的优化算法之一。有趣的是，Adam的名字来源于"Adaptive Moment Estimation"，也暗含"从头开始"的寓意。</p>
<h3 id="1083-vae">10.8.3 VAE论文的影响</h3>
<p>原始VAE论文"Auto-Encoding Variational Bayes"的特点：</p>
<ul>
<li>简洁优雅的数学推导</li>
<li>统一了深度学习与贝叶斯推断</li>
<li>启发了大量后续研究</li>
</ul>
<p><strong>历史趣事</strong>：VAE论文最初投稿ICLR 2014时是作为workshop论文，但因其重要性后来被提升为会议论文，成为ICLR历史上被引用最多的论文之一。</p>
<h3 id="1084">10.8.4 后续发展</h3>
<p>Kingma继续在生成模型领域做出贡献：</p>
<ul>
<li><strong>Glow</strong>：基于流的生成模型</li>
<li><strong>变分扩散模型</strong>：VAE视角下的扩散模型理解</li>
<li>创立公司将生成模型应用于实际问题</li>
</ul>
<p><strong>名言</strong>："The best way to understand something is to try to change it." - Kingma经常引用Kurt Lewin的这句话，体现了他通过生成来理解数据的哲学。</p>
<h2 id="109-vae">10.9 现代连接：VAE在大语言模型时代的应用</h2>
<h3 id="1091-llm">10.9.1 LLM嵌入空间的压缩</h3>
<p>现代LLM的嵌入维度通常很高（如4096维），VAE可用于学习低维表示：</p>
<p><strong>应用场景</strong>：</p>
<ol>
<li><strong>语义搜索加速</strong>：将高维嵌入压缩到低维空间，加快检索速度</li>
<li><strong>嵌入量化</strong>：结合VQ-VAE实现离散化存储</li>
<li><strong>跨模型对齐</strong>：学习不同LLM嵌入空间的共同表示</li>
</ol>
<p><strong>实践案例</strong>：</p>
<div class="codehilite"><pre><span></span><code>原始嵌入（4096维） → VAE编码器 → 压缩表示（128维）
                                    ↓
                              语义搜索/聚类
</code></pre></div>

<p>压缩比可达32:1，同时保留95%以上的语义信息。</p>
<h3 id="1092">10.9.2 可控文本生成</h3>
<p><strong>VAE-LLM混合架构</strong>：</p>
<ol>
<li><strong>隐变量注入</strong>：</li>
</ol>
<div class="codehilite"><pre><span></span><code>文本 → LLM编码器 → VAE → z → LLM解码器 → 生成文本
                           ↑
                       控制信号
</code></pre></div>

<ol start="2">
<li><strong>风格迁移</strong>：
   - 内容编码器：提取语义信息
   - 风格编码器：提取风格特征
   - 交叉重组实现风格迁移</li>
</ol>
<h3 id="1093-llm">10.9.3 LLM的不确定性量化</h3>
<p>VAE提供了量化LLM不确定性的原则性方法：</p>
<p><strong>后验方差作为不确定性度量</strong>：
$$\text{不确定性} = \text{Tr}(\Sigma_\phi(x))$$
应用：</p>
<ul>
<li>识别模型不确定的预测</li>
<li>主动学习中的样本选择</li>
<li>幻觉检测</li>
</ul>
<h3 id="1094">10.9.4 多模态理解中的桥梁作用</h3>
<p>VAE在连接不同模态中发挥重要作用：</p>
<div class="codehilite"><pre><span></span><code>图像 → 视觉编码器 ↘
                    VAE共享隐空间 → 多模态表示
文本 → 文本编码器 ↗
</code></pre></div>

<p><strong>CLIP-VAE架构</strong>：</p>
<ul>
<li>利用CLIP的对齐特性</li>
<li>VAE学习模态不变的隐表示</li>
<li>实现零样本跨模态生成</li>
</ul>
<h3 id="1095">10.9.5 计算效率优化</h3>
<p><strong>稀疏VAE for LLM</strong>：</p>
<p>通过学习稀疏隐表示减少计算：
$$\mathcal{L}_{sparse} = \mathcal{L}_{VAE} + \lambda |z|_1$$
效果：</p>
<ul>
<li>推理速度提升2-3倍</li>
<li>内存占用减少50%</li>
<li>保持生成质量</li>
</ul>
<p><strong>Rule of Thumb</strong>：在LLM应用中，VAE的隐维度通常设为原始维度的1/8到1/32，平衡压缩率和信息保留。</p>
<h2 id="1010">10.10 本章小结</h2>
<h3 id="_1">核心概念回顾</h3>
<ol>
<li>
<p><strong>变分推断框架</strong>：
   - 用可处理的分布$q_\phi(z|x)$近似难解的后验$p(z|x)$
   - ELBO作为log似然的下界，同时也是优化目标</p>
</li>
<li>
<p><strong>ELBO的两种理解</strong>：
   - 信息论视角：重构项 - KL正则项
   - 期望最大化视角：期望步骤的下界</p>
</li>
<li>
<p><strong>重参数化技巧</strong>：
   - 核心创新：$z = \mu + \sigma \odot \epsilon$
   - 使随机采样可微，实现端到端训练</p>
</li>
<li>
<p><strong>关键问题与解决方案</strong>：
   - 后验崩塌：KL退火、自由比特、架构设计
   - 解耦表示：β-VAE通过调整KL权重
   - 离散隐变量：VQ-VAE使用向量量化</p>
</li>
</ol>
<h3 id="_2">重要公式总结</h3>
<ol>
<li>
<p><strong>ELBO</strong>：
$$\text{ELBO} = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - \text{KL}[q_\phi(z|x) | p(z)]$$</p>
</li>
<li>
<p><strong>高斯VAE的KL散度</strong>（闭式解）：
$$\text{KL} = \frac{1}{2}\sum_{i=1}^d \left(\mu_i^2 + \sigma_i^2 - \log\sigma_i^2 - 1\right)$$</p>
</li>
<li>
<p><strong>β-VAE目标</strong>：
$$\mathcal{L}_{\beta} = \mathbb{E}_{q_\phi}[\log p_\theta(x|z)] - \beta \cdot \text{KL}[q_\phi | p]$$</p>
</li>
</ol>
<h3 id="_3">实用建议</h3>
<ol>
<li>
<p><strong>架构设计</strong>：
   - 编码器输出均值和log方差
   - 解码器根据数据类型选择合适的分布
   - 隐维度通常为输入维度的1/10到1/100</p>
</li>
<li>
<p><strong>训练技巧</strong>：
   - 使用KL退火避免后验崩塌
   - 监控重构项和KL项的平衡
   - 批次大小影响KL估计的方差</p>
</li>
<li>
<p><strong>超参数选择</strong>：
   - β值：生成质量优先选1-2，解耦优先选4-10
   - 隐维度：过小信息损失，过大难以正则化
   - 学习率：编码器和解码器可使用不同学习率</p>
</li>
</ol>
<h2 id="1011_1">10.11 常见陷阱与错误</h2>
<h3 id="1">陷阱1：忽视数值稳定性</h3>
<p><strong>问题</strong>：直接输出方差而非log方差</p>
<div class="codehilite"><pre><span></span><code><span class="err">错误：σ²</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">network</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w">  </span><span class="c1"># 可能为负或过大</span>
<span class="err">正确：</span><span class="n">log_var</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">network</span><span class="p">(</span><span class="n">x</span><span class="p">);</span><span class="w"> </span><span class="err">σ²</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">exp</span><span class="p">(</span><span class="n">log_var</span><span class="p">)</span>
</code></pre></div>

<p><strong>解决</strong>：始终使用log方差，并在计算时加入小常数避免数值问题</p>
<h3 id="2kl">陷阱2：KL权重设置不当</h3>
<p><strong>问题</strong>：β设置过大导致信息瓶颈</p>
<ul>
<li>症状：重构质量极差，隐变量几乎不变</li>
<li>诊断：监控KL项，如果接近0说明后验崩塌</li>
</ul>
<p><strong>解决</strong>：使用退火策略或自适应β</p>
<h3 id="3">陷阱3：批次统计的误用</h3>
<p><strong>问题</strong>：在计算KL时使用批次统计而非样本统计</p>
<div class="codehilite"><pre><span></span><code>错误：kl = mean(kl_per_dim)  # 跨维度平均
正确：kl = sum(kl_per_dim)   # 跨维度求和
</code></pre></div>

<h3 id="4">陷阱4：先验选择不当</h3>
<p><strong>问题</strong>：盲目使用标准高斯先验</p>
<ul>
<li>对于某些数据，其他先验可能更合适</li>
<li>如混合高斯、或学习的先验</li>
</ul>
<p><strong>解决</strong>：根据数据特性选择或学习先验</p>
<h3 id="5">陷阱5：评估指标的误解</h3>
<p><strong>问题</strong>：仅关注重构误差</p>
<ul>
<li>VAE的目标不仅是重构，还包括学习良好的隐表示</li>
<li>需要综合评估生成质量、插值平滑性、解耦程度</li>
</ul>
<p><strong>解决</strong>：使用多个指标：FID、IS、MIG等</p>
<h3 id="6">陷阱6：采样数量不足</h3>
<p><strong>问题</strong>：训练时每个数据点只采样一个z</p>
<ul>
<li>导致梯度估计方差大</li>
<li>特别是在隐维度较高时</li>
</ul>
<p><strong>解决</strong>：使用重要性加权或多次采样</p>
<h2 id="1012_1">10.12 练习题</h2>
<h3 id="_4">基础题</h3>
<p><strong>习题10.1</strong> 推导ELBO
证明对于任意分布$q(z|x)$，都有：
$$\log p(x) \geq \mathbb{E}_{q(z|x)}[\log p(x,z) - \log q(z|x)]$$
<em>提示：使用Jensen不等式或KL散度的非负性</em></p>
<details>
<summary>答案</summary>
<p>方法一（Jensen不等式）：
$$\log p(x) = \log \int p(x,z)dz = \log \int q(z|x)\frac{p(x,z)}{q(z|x)}dz$$
$$\geq \int q(z|x)\log\frac{p(x,z)}{q(z|x)}dz = \text{ELBO}$$
方法二（KL散度）：
$$\text{KL}[q(z|x)|p(z|x)] = \int q(z|x)\log\frac{q(z|x)}{p(z|x)}dz \geq 0$$
$$= \int q(z|x)\log q(z|x)dz - \int q(z|x)\log p(z|x)dz$$
$$= \int q(z|x)\log q(z|x)dz - \int q(z|x)\log\frac{p(x,z)}{p(x)}dz$$
$$= -\text{ELBO} + \log p(x)$$
因此$\log p(x) \geq \text{ELBO}$。</p>
</details>
<p><strong>习题10.2</strong> 高斯KL散度计算
对于两个高斯分布$q = \mathcal{N}(\mu_1, \sigma_1^2)$和$p = \mathcal{N}(\mu_2, \sigma_2^2)$，推导KL散度的闭式解。</p>
<p><em>提示：利用高斯分布的熵和交叉熵</em></p>
<details>
<summary>答案</summary>
<p>KL散度定义：
$$\text{KL}[q|p] = \int q(x)\log\frac{q(x)}{p(x)}dx$$
对于高斯分布：
$$\text{KL}[q|p] = \log\frac{\sigma_2}{\sigma_1} + \frac{\sigma_1^2 + (\mu_1-\mu_2)^2}{2\sigma_2^2} - \frac{1}{2}$$
当$p = \mathcal{N}(0,1)$时（VAE的常见情况）：
$$\text{KL} = \frac{1}{2}(\mu_1^2 + \sigma_1^2 - \log\sigma_1^2 - 1)$$</p>
</details>
<p><strong>习题10.3</strong> 重参数化实现
解释为什么直接从$\mathcal{N}(\mu, \sigma^2)$采样无法反向传播，而重参数化$z = \mu + \sigma\epsilon$（其中$\epsilon \sim \mathcal{N}(0,1)$）可以。</p>
<p><em>提示：考虑计算图中的随机节点</em></p>
<details>
<summary>答案</summary>
<p>直接采样时，$z \sim \mathcal{N}(\mu, \sigma^2)$是一个随机操作，梯度无法通过随机采样操作传播回$\mu$和$\sigma$。</p>
<p>重参数化后：</p>
<ul>
<li>$\epsilon$的随机性与参数$\mu, \sigma$无关</li>
<li>$z = \mu + \sigma\epsilon$是确定性操作</li>
<li>梯度可以通过确定性操作传播：
$$\frac{\partial z}{\partial \mu} = 1, \quad \frac{\partial z}{\partial \sigma} = \epsilon$$
这样，随机性被"外部化"，梯度可以正常流动。</li>
</ul>
</details>
<p><strong>习题10.4</strong> β-VAE的信息瓶颈
说明当β→∞时，β-VAE会发生什么？这与信息瓶颈原理有何联系？</p>
<p><em>提示：考虑极限情况下的优化目标</em></p>
<details>
<summary>答案</summary>
<p>当β→∞时，损失函数被KL项主导：
$$\mathcal{L} \approx -β \cdot \text{KL}[q_\phi(z|x) | p(z)]$$
为最小化损失，模型会让$q_\phi(z|x) \approx p(z)$，即后验接近先验，隐变量不再携带关于x的信息。</p>
<p>这对应信息瓶颈的极端情况：</p>
<ul>
<li>互信息$I(X; Z) \to 0$</li>
<li>形成最严格的信息瓶颈</li>
<li>只保留最关键的信息（如果有的话）</li>
</ul>
<p>实践中，需要在信息保留和压缩之间找平衡。</p>
</details>
<h3 id="_5">挑战题</h3>
<p><strong>习题10.5</strong> 条件VAE的ELBO
推导条件VAE（CVAE）的ELBO，其中需要建模$p(x|c)$，c是条件信息。</p>
<p><em>提示：在所有分布中加入条件c</em></p>
<details>
<summary>答案</summary>
<p>条件VAE的生成过程：</p>
<ol>
<li>给定条件c</li>
<li>从先验采样：$z \sim p(z|c)$</li>
<li>生成数据：$x \sim p_\theta(x|z,c)$</li>
</ol>
<p>ELBO推导：
$$\log p(x|c) \geq \mathbb{E}_{q_\phi(z|x,c)}[\log p_\theta(x|z,c)] - \text{KL}[q_\phi(z|x,c) | p(z|c)]$$
关键区别：</p>
<ul>
<li>编码器：$q_\phi(z|x,c)$接收x和c</li>
<li>解码器：$p_\theta(x|z,c)$基于z和c生成</li>
<li>先验：可以是条件先验$p(z|c)$或固定先验$p(z)$</li>
</ul>
<p>应用：可控生成、半监督学习、多任务学习。</p>
</details>
<p><strong>习题10.6</strong> VQ-VAE的梯度估计
VQ-VAE使用向量量化，这是不可微操作。解释straight-through估计器如何解决这个问题。</p>
<p><em>提示：考虑前向传播和反向传播的不同处理</em></p>
<details>
<summary>答案</summary>
<p>VQ-VAE的向量量化操作：
$$z_q = \arg\min_{e_k} |z_e - e_k|_2$$
这是离散选择，不可微。Straight-through估计器的策略：</p>
<p><strong>前向传播</strong>：使用量化后的值
$$z_q = e_{k^*}, \quad k^* = \arg\min_k |z_e - e_k|_2$$
<strong>反向传播</strong>：将梯度直接传递
$$\frac{\partial \mathcal{L}}{\partial z_e} := \frac{\partial \mathcal{L}}{\partial z_q}$$
额外的损失项：</p>
<ol>
<li><strong>承诺损失</strong>：$|z_e - \text{sg}[z_q]|_2^2$ - 让编码器输出接近码本</li>
<li><strong>码本损失</strong>：$|\text{sg}[z_e] - z_q|_2^2$ - 更新码本向量</li>
</ol>
<p>其中sg表示stop gradient。这种设计巧妙地实现了离散表示的端到端学习。</p>
</details>
<p><strong>习题10.7</strong> 层次VAE的ELBO
对于两层VAE：$p(x|z_1)p(z_1|z_2)p(z_2)$，推导其ELBO并解释每一项的含义。</p>
<p><em>提示：逐层应用变分推断</em></p>
<details>
<summary>答案</summary>
<p>两层VAE的ELBO：
$$\log p(x) \geq \mathbb{E}_{q(z_1,z_2|x)}[\log p(x,z_1,z_2) - \log q(z_1,z_2|x)]$$
假设后验分解为：$q(z_1,z_2|x) = q(z_1|x)q(z_2|z_1,x)$</p>
<p>展开得到：
$$\text{ELBO} = \mathbb{E}_{q(z_1|x)}[\log p(x|z_1)] - \text{KL}[q(z_1|x) | p(z_1|z_2)]$$
$$- \mathbb{E}_{q(z_1|x)}[\text{KL}[q(z_2|z_1,x) | p(z_2)]]$$
含义：</p>
<ul>
<li>第一项：重构损失</li>
<li>第二项：第一层隐变量的正则化（条件先验）</li>
<li>第三项：第二层隐变量的正则化</li>
</ul>
<p>优势：更灵活的先验$p(z_1|z_2)$可以更好地匹配数据分布。</p>
</details>
<p><strong>习题10.8</strong> 开放思考：VAE与扩散模型的联系
扩散模型可以看作是层次VAE的极限情况。请解释这种联系，并讨论两者的优劣。</p>
<p><em>提示：考虑扩散模型的马尔可夫链结构</em></p>
<details>
<summary>答案</summary>
<p><strong>联系</strong>：
扩散模型的前向过程：
$$q(x_1,...,x_T|x_0) = \prod_{t=1}^T q(x_t|x_{t-1})$$
可以看作T层的VAE，每层添加少量噪声。</p>
<p><strong>ELBO的相似性</strong>：
扩散模型的目标也是最大化ELBO：
$$\text{ELBO} = \mathbb{E}_q[\log p(x_0|x_1)] - \sum_{t=2}^T \text{KL}[q(x_{t-1}|x_t,x_0) | p(x_{t-1}|x_t)]$$</p>
<p><strong>关键区别</strong>：</p>
<ol>
<li><strong>层数</strong>：VAE通常2-3层，扩散模型可达1000层</li>
<li><strong>隐变量维度</strong>：VAE降维，扩散模型保持维度</li>
<li><strong>先验设计</strong>：VAE学习编码，扩散模型固定加噪过程</li>
</ol>
<p><strong>优劣比较</strong>：</p>
<ul>
<li>VAE：可解释的隐空间，快速采样，但生成质量受限</li>
<li>扩散模型：高质量生成，但采样慢，隐空间不可解释</li>
</ul>
<p><strong>统一视角</strong>：两者都是通过分层隐变量建模复杂分布，权衡了表达能力和计算效率。</p>
</details>
<hr />
<p><strong>继续学习</strong>：<a href="chapter11.html">第11章：扩散模型</a> →</p>
            </article>
            
            <nav class="page-nav"><a href="chapter9.html" class="nav-link prev">← 第9章：大语言模型</a><a href="chapter11.html" class="nav-link next">第11章：扩散模型 →</a></nav>
        </main>
    </div>
</body>
</html>