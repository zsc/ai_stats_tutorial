# 第1章：优化基础与梯度下降

优化是现代人工智能的数学基石。从最简单的线性回归到复杂的深度神经网络，几乎所有机器学习算法的核心都是求解一个优化问题。本章将从优化理论的基本概念出发，深入探讨梯度下降算法及其变体，为后续章节打下坚实的理论基础。通过本章学习，你将理解为什么梯度下降能够工作，如何选择合适的学习率，以及在实践中如何避免常见的优化陷阱。

## 1.1 凸优化与非凸优化

### 1.1.1 凸集与凸函数

在优化理论中，凸性是一个核心概念，它决定了优化问题的难易程度。理解凸性不仅是理论分析的基础，更是判断实际问题求解难度的关键工具。

**定义1.1（凸集）**：集合 $C \subseteq \mathbb{R}^n$ 是凸集，当且仅当对于任意 $x, y \in C$ 和 $\lambda \in [0, 1]$，有：
$$\lambda x + (1-\lambda)y \in C$$

直观理解：凸集内任意两点的连线仍在集合内。这个性质保证了优化过程中的"直线搜索"不会离开可行域。

```
凸集示例：              非凸集示例：
    ____                  ___   ___
   /    \                /   \_/   \
  |      |              |           |
   \____/                \___   ___/
                             \_/
```

**常见凸集例子**：
- **超平面**：$\{x : a^Tx = b\}$
- **半空间**：$\{x : a^Tx \leq b\}$
- **球**：$\{x : \|x - x_0\| \leq r\}$
- **椭球**：$\{x : (x-x_0)^TP^{-1}(x-x_0) \leq 1\}$，其中 $P \succ 0$
- **多面体**：$\{x : Ax \preceq b\}$（线性不等式的交集）
- **锥**：$\{x : x = \theta v, \theta \geq 0\}$

**凸集的运算性质**：
1. **交集保凸性**：凸集的交集仍是凸集
2. **仿射变换保凸性**：若 $C$ 是凸集，则 $\{Ax + b : x \in C\}$ 也是凸集
3. **透视变换**：$P(x, t) = x/t$ 保持凸性（$t > 0$）

**定义1.2（凸函数）**：函数 $f: \mathbb{R}^n \rightarrow \mathbb{R}$ 是凸函数，当且仅当其定义域是凸集，且对于任意 $x, y$ 在定义域内和 $\lambda \in [0, 1]$，有：
$$f(\lambda x + (1-\lambda)y) \leq \lambda f(x) + (1-\lambda)f(y)$$

这个不等式被称为Jensen不等式，它表明函数曲线位于任意两点连线的下方。

**几何解释**：凸函数的上境图（epigraph）$\text{epi}(f) = \{(x, t) : f(x) \leq t\}$ 是凸集。这提供了从集合角度理解函数凸性的方式。

**严格凸与强凸**：
- **严格凸**：当 $x \neq y$ 且 $\lambda \in (0, 1)$ 时，不等式严格成立
- **强凸**：存在 $\mu > 0$，使得 $f(x) - \frac{\mu}{2}\|x\|^2$ 仍是凸函数

强凸性保证了函数有唯一最小值点，这在优化算法的收敛性分析中至关重要。

### 1.1.2 局部最优与全局最优

凸优化最重要的性质是：**局部最优即全局最优**。这个性质是凸优化理论的基石，也是为什么凸问题在实践中如此重要的原因。

**定义1.3（局部最优）**：点 $x^*$ 是函数 $f$ 的局部最小值点，如果存在邻域 $\mathcal{N}(x^*)$，使得对所有 $x \in \mathcal{N}(x^*)$，有 $f(x^*) \leq f(x)$。

**定义1.4（全局最优）**：点 $x^*$ 是函数 $f$ 的全局最小值点，如果对定义域内所有 $x$，有 $f(x^*) \leq f(x)$。

**定理1.1（凸函数的基本定理）**：对于凸函数 $f$，如果 $x^*$ 是局部最小值点，则 $x^*$ 也是全局最小值点。

**证明思路**：反证法。假设 $x^*$ 是局部最优但非全局最优，则存在 $y$ 使得 $f(y) < f(x^*)$。由凸性，连线上所有点的函数值都小于 $f(x^*)$，与局部最优矛盾。

这个性质使得凸优化问题相对容易求解。相比之下，非凸优化可能存在多个局部最优：

```
凸函数（单峰）：          非凸函数（多峰）：
    \       /                /\    /\
     \     /                /  \  /  \
      \___/                /    \/    \
       x*                  x1*  x2*  x3*
    (全局最优)           (多个局部最优)
```

**实际意义**：
- **凸问题**：任何局部搜索算法（如梯度下降）都能找到全局最优
- **非凸问题**：需要全局优化技术（如模拟退火、遗传算法）或多次随机初始化

**深度学习中的非凸性**：
神经网络的损失函数通常是高度非凸的，但实践表明：
1. **过参数化**使得大部分局部最优性能相近
2. **SGD的噪声**帮助逃离差的局部最优
3. **良好的初始化**可以避免糟糕的区域

### 1.1.3 凸性判定

在实践中，我们需要判断函数是否为凸函数。以下是常用的判定方法，从简单到复杂逐步深入。

**零阶条件（定义法）**：
直接验证Jensen不等式。适用于简单函数，但通常较繁琐。

**一阶条件**：可微函数 $f$ 是凸函数，当且仅当对所有 $x, y$ 在定义域内：
$$f(y) \geq f(x) + \nabla f(x)^T(y-x)$$

这意味着函数始终位于其切线（一阶泰勒近似）上方。

**几何直觉**：凸函数的切线是全局下界。
```
      f(y)
       /|
      / |
     /  |  f(x) + ∇f(x)^T(y-x)
    /   |
   /____|
   x    y
```

**二阶条件**：二次可微函数 $f$ 是凸函数，当且仅当其Hessian矩阵半正定：
$$\nabla^2 f(x) \succeq 0, \quad \forall x$$

**Hessian矩阵的计算**：
对于 $f: \mathbb{R}^n \to \mathbb{R}$，Hessian是 $n \times n$ 矩阵：
$$[\nabla^2 f(x)]_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j}$$

**半正定判定**：
- 所有特征值 $\lambda_i \geq 0$
- 所有主子式非负
- 对所有 $v \neq 0$，有 $v^T\nabla^2 f(x)v \geq 0$

**保凸运算**：
1. **非负加权和**：$f = \sum_i \alpha_i f_i$，其中 $\alpha_i \geq 0$，$f_i$ 凸
2. **复合**：$g(f(x))$，其中 $g$ 凸递增，$f$ 凸
3. **逐点最大**：$f(x) = \max_i f_i(x)$，其中 $f_i$ 凸
4. **透视函数**：$f(x, t) = t \cdot g(x/t)$，其中 $g$ 凸，$t > 0$

**经验法则与常见函数**：
- **线性/仿射**：$f(x) = a^Tx + b$ 既凸又凹
- **二次函数**：$f(x) = \frac{1}{2}x^TAx + b^Tx + c$ 当 $A \succeq 0$ 时为凸
- **指数函数**：$e^{ax}$ 对任意 $a$ 为凸
- **幂函数**：$x^p$ 在 $x > 0$ 时，$p \geq 1$ 或 $p \leq 0$ 为凸
- **对数函数**：$-\log(x)$ 在 $x > 0$ 时为凸
- **负熵**：$x\log(x)$ 在 $x > 0$ 时为凸
- **范数**：$\|x\|_p$ 对 $p \geq 1$ 为凸
- **Log-sum-exp**：$\log(\sum_i e^{x_i})$ 为凸（softmax的基础）

**机器学习中的凸函数**：
- **均方误差**：$(y - w^Tx)^2$ 关于 $w$ 为凸
- **交叉熵**：$-y\log(p) - (1-y)\log(1-p)$ 关于 $p$ 为凸
- **铰链损失**：$\max(0, 1 - y \cdot w^Tx)$ 关于 $w$ 为凸
- **正则化项**：$\|w\|_1$（L1）和 $\|w\|_2^2$（L2）都是凸的

## 1.2 梯度下降及其变体

梯度下降是优化算法的基石，其核心思想简单而深刻：沿着负梯度方向移动以减小函数值。这个想法可以追溯到柯西1847年的工作，但直到今天仍是训练神经网络的主要方法。

**梯度的几何意义**：
梯度 $\nabla f(x)$ 指向函数增长最快的方向，因此负梯度 $-\nabla f(x)$ 是函数下降最快的方向。这可以通过泰勒展开证明：
$$f(x + \delta) \approx f(x) + \nabla f(x)^T\delta + O(\|\delta\|^2)$$

当 $\|\delta\|$ 固定时，$\delta = -\alpha \nabla f(x)$ 使得一阶项最小。

### 1.2.1 批量梯度下降（Batch Gradient Descent）

批量梯度下降使用整个数据集计算梯度，是最基础的优化算法。

**更新规则**：
$$\theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal{L}(\theta_t)$$

其中 $\eta$ 是学习率，$\mathcal{L}(\theta) = \frac{1}{N}\sum_{i=1}^N \ell(\theta; x_i, y_i)$ 是损失函数。

**算法流程**：
```
算法：批量梯度下降
输入：初始参数 θ₀，学习率 η，收敛阈值 ε
输出：优化后的参数 θ*

1. t ← 0
2. while ||∇L(θₜ)|| > ε do
3.     gₜ ← ∇L(θₜ)           # 计算全数据集梯度
4.     θₜ₊₁ ← θₜ - η·gₜ       # 参数更新
5.     t ← t + 1
6. return θₜ
```

**收敛条件的选择**：
- **梯度范数**：$\|\nabla \mathcal{L}(\theta)\| < \epsilon$（一阶条件）
- **函数值变化**：$|\mathcal{L}(\theta_t) - \mathcal{L}(\theta_{t-1})| < \epsilon$
- **参数变化**：$\|\theta_t - \theta_{t-1}\| < \epsilon$
- **最大迭代次数**：$t > T_{max}$（防止无限循环）

实践中通常组合使用多个条件。

**优点**：
- **收敛稳定**：每步都是真实梯度方向
- **理论保证**：在凸函数上保证收敛到全局最优
- **易于分析**：收敛速率有明确的理论界

**缺点**：
- **计算代价高**：每次迭代需要遍历所有 $N$ 个数据点
- **内存需求大**：需要同时加载所有数据
- **冗余计算**：相似样本提供的梯度信息有重叠
- **局部最优**：在非凸问题上容易陷入局部最优

**计算复杂度分析**：
- 单次迭代：$O(Nd)$，其中 $d$ 是参数维度
- 达到 $\epsilon$ 精度：$O(Nd/\epsilon)$（凸函数）
- 内存需求：$O(Nd + d)$

### 1.2.2 随机梯度下降（Stochastic Gradient Descent, SGD）

SGD是现代深度学习的workhorse，通过引入随机性大幅提高计算效率。

**核心思想**：用单个样本的梯度估计整体梯度。

**更新规则**：
$$\theta_{t+1} = \theta_t - \eta \nabla_\theta \ell(\theta_t; x_{i_t}, y_{i_t})$$

其中 $i_t$ 是在时刻 $t$ 随机选择的样本索引。

**关键洞察——无偏估计**：虽然单个样本的梯度有噪声，但期望值等于真实梯度：
$$\mathbb{E}_{i \sim \text{Uniform}(1,N)}[\nabla_\theta \ell(\theta; x_i, y_i)] = \frac{1}{N}\sum_{i=1}^N \nabla_\theta \ell(\theta; x_i, y_i) = \nabla_\theta \mathcal{L}(\theta)$$

这保证了SGD在期望意义下沿着正确的方向前进。

**梯度噪声的分析**：
定义梯度噪声：$\xi_t = \nabla_\theta \ell(\theta_t; x_{i_t}, y_{i_t}) - \nabla_\theta \mathcal{L}(\theta_t)$

噪声性质：
- **零均值**：$\mathbb{E}[\xi_t] = 0$
- **有界方差**：$\mathbb{E}[\|\xi_t\|^2] \leq \sigma^2$

**SGD的优势**：
1. **计算效率**：单次更新只需 $O(d)$ 而非 $O(Nd)$
2. **在线学习**：可以处理流式数据
3. **逃离局部最优**：噪声帮助跳出浅层局部最优
4. **隐式正则化**：噪声起到正则化作用，提高泛化

**SGD的挑战**：
1. **收敛慢**：$O(1/\sqrt{T})$ vs 批量GD的 $O(1/T)$
2. **振荡**：在最优点附近持续振荡
3. **学习率敏感**：需要仔细调整学习率计划

**方差减小技巧**：
- **递减学习率**：$\eta_t = \eta_0/\sqrt{t}$ 或 $\eta_t = \eta_0/(1 + \lambda t)$
- **平均化**：Polyak平均 $\bar{\theta} = \frac{1}{T}\sum_{t=1}^T \theta_t$
- **重要性采样**：根据梯度大小调整采样概率

### 1.2.3 小批量梯度下降（Mini-batch Gradient Descent）

小批量方法是实践中的黄金标准，巧妙平衡了计算效率和收敛稳定性。

**更新规则**：
$$\theta_{t+1} = \theta_t - \eta \frac{1}{|B|} \sum_{i \in B} \nabla_\theta \ell(\theta_t; x_i, y_i)$$

其中 $B$ 是批量（batch），$|B|$ 是批量大小。

**为什么使用小批量？**
1. **硬件效率**：现代GPU/TPU针对矩阵运算优化，批量计算比逐样本快
2. **梯度质量**：减小方差，更稳定的更新方向
3. **并行化**：批内样本可以并行处理
4. **内存利用**：充分利用GPU内存带宽

**批量大小的影响**：
```
批量大小    噪声水平    收敛速度    泛化性能    GPU利用率
1          最高        最慢        好          低
32         高          慢          较好        中
128        中          中          中          高
512        低          快          较差        很高
2048+      很低        很快        差          饱和
```

**批量大小选择的经验法则**：
- **小批量（8-32）**：
  - 更好的泛化（噪声作为正则化）
  - 适合小模型或内存受限场景
  - 常用于强化学习
  
- **中批量（64-256）**：
  - 最常用的范围
  - 良好的效率-性能平衡
  - 适合大多数视觉和NLP任务
  
- **大批量（512-4096）**：
  - 训练稳定，收敛快
  - 需要学习率warmup和特殊技巧
  - 用于大规模分布式训练

**大批量训练的挑战与解决**：
1. **泛化差距**：大批量模型测试性能往往较差
   - 解决：使用更长的训练时间
   - LARS/LAMB优化器专门为大批量设计
   
2. **学习率缩放**：
   - 线性缩放：$\eta \propto |B|$（批量翻倍，学习率翻倍）
   - 平方根缩放：$\eta \propto \sqrt{|B|}$（更保守）
   
3. **初始不稳定**：
   - 学习率warmup：前几个epoch逐渐增加学习率
   - 梯度累积：模拟大批量而不增加内存

**批量构造策略**：
- **随机采样**：最常用，每个epoch重新打乱
- **分层采样**：保证批内类别平衡
- **难例挖掘**：优先采样损失大的样本
- **课程学习**：从易到难逐步增加难度

### 1.2.4 动量方法（Momentum）

动量方法通过累积历史梯度信息来加速收敛：

$$\begin{aligned}
v_{t+1} &= \beta v_t + (1-\beta) g_t \\
\theta_{t+1} &= \theta_t - \eta v_{t+1}
\end{aligned}$$

其中 $\beta$ 是动量系数（通常为0.9）。

物理直觉：想象一个球在山谷中滚动，动量帮助它越过小的局部起伏。

```
无动量：                  有动量：
  振荡路径                 平滑路径
  /\/\/\                    ___
 /      \                  /   \
/        \                /     \
```

**Nesterov加速梯度（NAG）**：
先根据动量"预测"下一步位置，然后在该位置计算梯度：

$$\begin{aligned}
v_{t+1} &= \beta v_t + (1-\beta) \nabla_\theta \mathcal{L}(\theta_t - \eta \beta v_t) \\
\theta_{t+1} &= \theta_t - \eta v_{t+1}
\end{aligned}$$

## 1.3 收敛性分析

理解优化算法的收敛性对于实践应用至关重要。

### 1.3.1 Lipschitz连续性

**定义1.3**：函数 $f$ 的梯度是 $L$-Lipschitz连续的，如果：
$$\|\nabla f(x) - \nabla f(y)\| \leq L\|x - y\|$$

直观理解：梯度变化不会太剧烈。

**定理1.2（梯度下降收敛性）**：对于 $L$-光滑的凸函数，选择学习率 $\eta \leq 1/L$，梯度下降保证：
$$f(\theta_T) - f(\theta^*) \leq \frac{\|\theta_0 - \theta^*\|^2}{2\eta T}$$

这表明误差以 $O(1/T)$ 的速率下降。

### 1.3.2 强凸性

**定义1.4**：函数 $f$ 是 $\mu$-强凸的，如果：
$$f(y) \geq f(x) + \nabla f(x)^T(y-x) + \frac{\mu}{2}\|y-x\|^2$$

强凸函数有唯一的全局最优点。

**定理1.3（强凸函数的线性收敛）**：对于 $\mu$-强凸且 $L$-光滑的函数，梯度下降达到线性收敛率：
$$f(\theta_T) - f(\theta^*) \leq \left(1 - \frac{\mu}{L}\right)^T [f(\theta_0) - f(\theta^*)]$$

条件数 $\kappa = L/\mu$ 决定了收敛速度：
- $\kappa$ 接近1：快速收敛
- $\kappa$ 很大：缓慢收敛

### 1.3.3 SGD的收敛性

SGD的收敛性更复杂，因为存在梯度噪声。

**定理1.4（SGD收敛性）**：假设梯度噪声有界：$\mathbb{E}[\|\nabla \ell - \nabla \mathcal{L}\|^2] \leq \sigma^2$，使用递减学习率 $\eta_t = \eta_0/\sqrt{t}$：
$$\mathbb{E}[f(\theta_T) - f(\theta^*)] \leq O\left(\frac{1}{\sqrt{T}}\right)$$

注意SGD的收敛速率 $O(1/\sqrt{T})$ 慢于批量梯度下降的 $O(1/T)$。

## 1.4 学习率选择的经验法则

学习率是优化中最重要的超参数。选择不当会导致训练失败。

### 1.4.1 固定学习率策略

**经验法则**：
1. 从较大学习率开始（如0.1），观察损失曲线
2. 如果损失发散或振荡，除以10
3. 如果收敛太慢，乘以2-3
4. 典型范围：[1e-4, 1]

```
学习率过大：              学习率适中：           学习率过小：
损失发散                  快速下降               缓慢下降
  /\  /\                    \                      \___
 /  \/  \                    \___                      \___
/        \                      \___                      \__
```

### 1.4.2 学习率衰减策略

**步长衰减（Step Decay）**：
$$\eta_t = \eta_0 \cdot \gamma^{\lfloor t/s \rfloor}$$
其中 $\gamma$ 通常为0.1，$s$ 是衰减周期。

**指数衰减（Exponential Decay）**：
$$\eta_t = \eta_0 \cdot e^{-\lambda t}$$

**余弦退火（Cosine Annealing）**：
$$\eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})(1 + \cos(\pi t/T))$$

### 1.4.3 自适应学习率

现代深度学习常用自适应方法（Adam、RMSprop等），它们根据梯度历史自动调整学习率。核心思想：
- 频繁更新的参数：降低学习率
- 稀疏更新的参数：提高学习率

**实用建议**：
- 计算机视觉：SGD + 动量 + 学习率衰减
- NLP/推荐系统：Adam/AdamW
- 强化学习：Adam
- 微调预训练模型：较小学习率（1e-5到1e-4）

## 1.5 历史人物：柯西与最速下降法的诞生

奥古斯丁-路易·柯西（Augustin-Louis Cauchy, 1789-1857）是19世纪最伟大的数学家之一。1847年，他在研究天体力学问题时提出了最速下降法（method of steepest descent），这是梯度下降算法的前身。

### 柯西的关键贡献

柯西面临的问题是求解非线性方程组，他提出了革命性的想法：
1. **局部线性化**：在当前点用泰勒展开近似函数
2. **最速下降方向**：沿负梯度方向是函数值下降最快的方向
3. **迭代求解**：通过反复迭代逼近最优解

柯西的原始论文标题是《Méthode générale pour la résolution des systèmes d'équations simultanées》（求解联立方程组的一般方法）。有趣的是，柯西当时并没有使用"梯度"这个术语，而是用"导数系数"来描述。

### 从天文学到AI的跨越

柯西方法最初用于计算行星轨道，需要求解复杂的非线性方程。170多年后，同样的数学原理支撑着训练包含数万亿参数的神经网络。这种跨越世纪的数学传承令人惊叹。

**历史趣事**：柯西是个多产的数学家，一生发表了789篇论文。巴黎科学院因为他投稿太多，不得不限制每人每周最多提交一篇论文——这个规定被称为"柯西规则"。

## 1.6 现代连接：LLM训练中的梯度累积与混合精度优化

大语言模型（LLM）的训练将梯度下降推向了工程极限。以GPT-3（1750亿参数）为例，直接应用梯度下降会遇到严重的内存和计算挑战。

### 1.6.1 梯度累积（Gradient Accumulation）

**问题**：大模型需要大批量训练以保证稳定性，但GPU内存有限。

**解决方案**：梯度累积将大批量分解为多个小批量：

```python
# 伪代码
effective_batch_size = 4096
micro_batch_size = 32
accumulation_steps = effective_batch_size // micro_batch_size

for step in range(accumulation_steps):
    loss = compute_loss(batch[step])
    loss = loss / accumulation_steps  # 规范化
    loss.backward()  # 累积梯度
    
optimizer.step()  # 仅在累积完成后更新
optimizer.zero_grad()
```

**数学等价性**：
$$\nabla_\theta \mathcal{L}_{large} = \frac{1}{N}\sum_{i=1}^N \nabla_\theta \ell_i = \frac{1}{K}\sum_{k=1}^K \left(\frac{1}{M}\sum_{m=1}^M \nabla_\theta \ell_{k,m}\right)$$

其中 $N = K \times M$ 是总批量大小。

### 1.6.2 混合精度训练（Mixed Precision Training）

**核心思想**：使用FP16进行前向/反向传播，但保持FP32的主权重副本。

**优势**：
- 内存减少约50%
- 计算速度提升2-3倍（在Tensor Core上）

**关键技术**：
1. **损失缩放（Loss Scaling）**：防止FP16的梯度下溢
   $$\mathcal{L}_{scaled} = s \cdot \mathcal{L}, \quad \nabla_\theta \mathcal{L} = \frac{1}{s} \nabla_\theta \mathcal{L}_{scaled}$$
   
2. **动态损失缩放**：自适应调整缩放因子
   - 如果梯度溢出：减小 $s$
   - 如果连续N步无溢出：增大 $s$

### 1.6.3 梯度检查点（Gradient Checkpointing）

**内存-计算权衡**：通过重计算节省激活内存。

不保存所有中间激活，而是：
1. 前向传播时只保存关键检查点
2. 反向传播时重新计算所需激活

内存复杂度：$O(\sqrt{n})$ vs $O(n)$（n是层数）

### 1.6.4 LLM训练的实际考虑

**ChatGPT/GPT-4级别模型的典型设置**：
- 批量大小：数百万tokens
- 学习率：带warmup的余弦衰减
- 优化器：AdamW（修正的Adam，解耦权重衰减）
- 梯度裁剪：防止梯度爆炸（通常clip_norm=1.0）

**规模法则（Scaling Laws）**：
Kaplan等人发现损失与模型大小的幂律关系：
$$\mathcal{L}(N) = \left(\frac{N_c}{N}\right)^{\alpha_N}$$

其中 $N$ 是参数数量，$\alpha_N \approx 0.076$。

这意味着：
- 10倍参数 → 损失降低约15%
- 性能提升是可预测的
- 但计算成本呈超线性增长

## 本章小结

本章我们系统学习了优化理论的基础知识，这些内容构成了现代AI算法的数学基石。

### 核心概念回顾

1. **凸性的重要性**：
   - 凸优化问题保证局部最优即全局最优
   - 非凸优化是深度学习的常态，需要特殊技巧

2. **梯度下降算法族**：
   - 批量GD：稳定但计算密集
   - SGD：高效但有噪声
   - 小批量GD：实践中的平衡选择
   - 动量方法：加速收敛并减少振荡

3. **收敛性理论**：
   - 光滑性（Lipschitz连续）确保稳定性
   - 强凸性带来线性收敛
   - SGD收敛较慢但实践效果好

4. **学习率策略**：
   - 固定学习率：简单但需要调优
   - 学习率衰减：提高后期收敛精度
   - 自适应方法：根据梯度历史自动调整

### 关键公式汇总

| 概念 | 公式 | 说明 |
|------|------|------|
| 凸函数定义 | $f(\lambda x + (1-\lambda)y) \leq \lambda f(x) + (1-\lambda)f(y)$ | Jensen不等式 |
| 梯度下降 | $\theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal{L}(\theta_t)$ | 基本更新规则 |
| 动量更新 | $v_{t+1} = \beta v_t + (1-\beta) g_t$ | 指数移动平均 |
| 凸函数收敛 | $f(\theta_T) - f(\theta^*) \leq O(1/T)$ | 次线性收敛 |
| 强凸收敛 | $f(\theta_T) - f(\theta^*) \leq (1-\mu/L)^T[f(\theta_0) - f(\theta^*)]$ | 线性收敛 |
| SGD收敛 | $\mathbb{E}[f(\theta_T) - f(\theta^*)] \leq O(1/\sqrt{T})$ | 带噪声的收敛 |

### 实用要点

- **调试优化问题时**，先检查函数是否凸，这决定了问题的难度
- **选择优化器时**，考虑问题特性：稀疏数据用自适应方法，密集数据用SGD+动量
- **设置学习率时**，从较大值开始逐步降低，观察损失曲线
- **批量大小选择**，需要在计算效率和收敛稳定性之间平衡
- **现代大模型训练**，需要工程技巧如梯度累积、混合精度来突破硬件限制

## 练习题

### 基础题

**习题1.1** 判断下列函数是否为凸函数：
- (a) $f(x) = e^x + e^{-x}$
- (b) $f(x) = x^3$
- (c) $f(x) = \log(1 + e^x)$（LogSumExp）
- (d) $f(x, y) = xy$

*提示*：使用二阶导数判定法或定义验证。

<details>
<summary>答案</summary>

(a) **凸函数**。$f''(x) = e^x + e^{-x} > 0$ 对所有 $x$ 成立。

(b) **非凸函数**。$f''(x) = 6x$，当 $x < 0$ 时 $f''(x) < 0$。

(c) **凸函数**。$f''(x) = \frac{e^x}{(1+e^x)^2} > 0$ 对所有 $x$ 成立。

(d) **非凸非凹**。Hessian矩阵 $H = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}$ 的特征值为 $\pm 1$，既有正值又有负值。
</details>

**习题1.2** 给定凸函数 $f(x) = \frac{1}{2}x^TAx + b^Tx$，其中 $A$ 是正定矩阵：
- (a) 求最优解 $x^*$
- (b) 如果 $A$ 的条件数为100，使用梯度下降需要多少次迭代才能将误差减小到初始的1%？

*提示*：利用一阶最优性条件和线性收敛公式。

<details>
<summary>答案</summary>

(a) 一阶最优性条件：$\nabla f(x^*) = Ax^* + b = 0$，因此 $x^* = -A^{-1}b$。

(b) 条件数 $\kappa = 100$，收敛率 $r = 1 - 1/\kappa = 0.99$。
要使误差降到1%：$(0.99)^T < 0.01$
$T > \frac{\log(0.01)}{\log(0.99)} \approx 458$ 次迭代。
</details>

**习题1.3** 考虑函数 $f(x) = \frac{1}{n}\sum_{i=1}^n (a_i^Tx - b_i)^2$（最小二乘问题）：
- (a) 计算梯度 $\nabla f(x)$
- (b) 计算Hessian矩阵 $\nabla^2 f(x)$
- (c) 该函数是否强凸？如果是，给出强凸参数

*提示*：将求和展开，利用矩阵形式简化。

<details>
<summary>答案</summary>

(a) $\nabla f(x) = \frac{2}{n}\sum_{i=1}^n a_i(a_i^Tx - b_i) = \frac{2}{n}(A^TAx - A^Tb)$
其中 $A = [a_1, ..., a_n]^T$。

(b) $\nabla^2 f(x) = \frac{2}{n}A^TA$

(c) 如果 $A$ 列满秩，则 $A^TA$ 正定，函数强凸。
强凸参数 $\mu = \frac{2}{n}\lambda_{min}(A^TA)$。
</details>

### 挑战题

**习题1.4** （动量方法的收敛性）考虑二次函数 $f(x) = \frac{1}{2}x^2$，使用动量方法：
$$v_{t+1} = \beta v_t - \eta \nabla f(x_t), \quad x_{t+1} = x_t + v_{t+1}$$
- (a) 写出该系统的递推关系矩阵形式
- (b) 分析什么条件下系统收敛
- (c) 最优的 $\beta$ 和 $\eta$ 选择是什么？

*提示*：将系统写成 $z_{t+1} = Az_t$ 形式，分析矩阵 $A$ 的谱半径。

<details>
<summary>答案</summary>

(a) 令 $z_t = [x_t, v_t]^T$，则：
$$z_{t+1} = \begin{bmatrix} 1 & 1 \\ -\eta & \beta \end{bmatrix} z_t$$

(b) 收敛条件：特征多项式 $\lambda^2 - \beta\lambda + \eta = 0$ 的根模长小于1。
要求：$\eta < \frac{(1+\beta)^2}{4}$ 且 $\beta < 1$。

(c) 临界阻尼时收敛最快：$\beta = 2\sqrt{\eta} - 1$。
对于 $\eta = 0.25$，最优 $\beta = 0$（无动量）。
</details>

**习题1.5** （SGD的方差减小）考虑有限和优化问题：
$$f(x) = \frac{1}{n}\sum_{i=1}^n f_i(x)$$
比较以下三种策略的梯度估计方差：
- (a) 均匀随机采样一个样本
- (b) 不重复地遍历所有样本（随机排列）
- (c) 重要性采样：以概率 $p_i \propto \|\nabla f_i(x)\|$ 采样

*提示*：计算每种方法的梯度估计方差。

<details>
<summary>答案</summary>

(a) 均匀采样方差：$\text{Var}[g] = \frac{1}{n}\sum_{i=1}^n \|\nabla f_i - \nabla f\|^2$

(b) 随机排列在一个epoch内方差为0（每个样本恰好用一次）。

(c) 重要性采样：选择 $p_i = \frac{\|\nabla f_i\|}{\sum_j \|\nabla f_j\|}$，
估计器 $g = \frac{1}{np_i}\nabla f_i$ 无偏，但方差可能更大。
最优采样概率需要考虑梯度相似度。
</details>

**习题1.6** （自适应学习率）Adam优化器维护一阶和二阶矩的指数移动平均：
$$m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t$$
$$v_t = \beta_2 v_{t-1} + (1-\beta_2)g_t^2$$
- (a) 为什么需要偏差修正 $\hat{m}_t = m_t/(1-\beta_1^t)$？
- (b) 证明当梯度稀疏时，Adam的有效学习率自动增大
- (c) Adam在什么情况下可能不收敛？

*提示*：分析初始化偏差和稀疏梯度的影响。

<details>
<summary>答案</summary>

(a) 初始 $m_0 = 0$，导致 $\mathbb{E}[m_t] = (1-\beta_1^t)\mathbb{E}[g]$。
偏差修正使估计无偏：$\mathbb{E}[\hat{m}_t] = \mathbb{E}[g]$。

(b) 稀疏梯度时，$v_t$ 对非零梯度累积慢，导致 $\sqrt{v_t}$ 小，
有效学习率 $\eta/\sqrt{v_t + \epsilon}$ 增大。

(c) Reddi等人证明Adam在某些非凸问题上可能发散。
反例：交替的梯度模式可能导致二阶矩估计过小。
解决方案：AMSGrad保持 $v_t$ 的最大值。
</details>

**习题1.7** （思考题）现代大语言模型使用的优化技巧中，哪些违背了传统优化理论？为什么它们在实践中仍然有效？讨论以下现象：
- 使用远大于理论上限的学习率
- 在非凸问题上不追求全局最优
- 过参数化模型的隐式正则化

*提示*：考虑高维空间的几何性质和神经网络的特殊结构。

<details>
<summary>答案</summary>

这是一个开放性问题，关键观察包括：

1. **大学习率**：神经网络的损失景观在大部分区域相对平坦，噪声帮助逃离sharp minima，提高泛化。

2. **局部最优**：高维空间中，大部分局部最优的性能相近（"Anna Karenina原理"）。网络宽度足够时，局部最优接近全局最优。

3. **过参数化**：
   - 梯度下降的隐式偏好：倾向于找到范数小的解
   - 神经切线核（NTK）理论：宽网络近似线性
   - 双下降现象：过参数化后测试误差再次下降

这些现象表明，深度学习的成功部分源于问题的特殊结构，而非单纯的优化技术。
</details>

## 常见陷阱与错误

在实际应用优化算法时，即使是经验丰富的从业者也容易犯以下错误：

### 1. 学习率设置不当

**错误表现**：
- 学习率过大：损失值爆炸（NaN）或剧烈振荡
- 学习率过小：训练极慢，容易陷入局部最优

**调试技巧**：
```
监控指标：
- 损失曲线斜率
- 梯度范数 ||∇L||
- 参数更新比例 ||Δθ||/||θ||
```

**经验法则**：参数更新比例应在1e-3到1e-2之间。

### 2. 梯度消失与爆炸

**识别方法**：
- 梯度消失：深层梯度范数接近0
- 梯度爆炸：梯度范数突然增大到1e3以上

**解决方案**：
- 梯度裁剪：`clip_grad_norm_(parameters, max_norm=1.0)`
- 更好的初始化：Xavier/He初始化
- 批归一化或层归一化
- 残差连接

### 3. 批量大小的误区

**常见错误**：
- 认为批量越大越好（实际上大批量可能损害泛化）
- 改变批量大小后不调整学习率

**正确做法**：
- 线性缩放规则：批量大小翻倍，学习率也翻倍
- 但要注意warmup：大批量需要学习率预热

### 4. 优化器选择不当

**典型场景与推荐**：
| 场景 | 错误选择 | 正确选择 | 原因 |
|------|----------|----------|------|
| 稀疏数据 | SGD | Adam/AdaGrad | 需要自适应学习率 |
| 噪声数据 | Adam | SGD+动量 | Adam可能过拟合噪声 |
| 微调 | 大学习率Adam | 小学习率SGD | 防止灾难性遗忘 |

### 5. 过早停止或过度训练

**判断标准**：
- 验证损失不再下降（连续5-10个epoch）
- 验证损失开始上升（过拟合）
- 梯度范数持续小于阈值

**最佳实践**：
- 保存验证损失最低的模型
- 使用早停（early stopping）但要有耐心参数
- 学习率衰减后继续训练几个epoch

### 6. 数值稳定性问题

**常见问题**：
```python
# 错误：直接计算softmax
exp_x = np.exp(x)
softmax = exp_x / np.sum(exp_x)  # 可能溢出

# 正确：减去最大值
x_max = np.max(x)
exp_x = np.exp(x - x_max)
softmax = exp_x / np.sum(exp_x)
```

### 7. 随机种子的忽视

**问题**：结果不可重现，难以调试。

**解决**：
```python
# 固定所有随机种子
random.seed(42)
np.random.seed(42)
torch.manual_seed(42)
```

### 8. 梯度累积的错误实现

**常见错误**：
```python
# 错误：忘记规范化
for batch in mini_batches:
    loss = compute_loss(batch)
    loss.backward()  # 梯度会越累积越大

# 正确：除以累积步数
for batch in mini_batches:
    loss = compute_loss(batch)
    loss = loss / accumulation_steps
    loss.backward()
```

### 9. 学习率调度器的误用

**问题**：
- 在错误的位置调用scheduler.step()
- 多个调度器冲突
- warmup设置不合理

**最佳实践**：
- 每个epoch结束后调用scheduler
- Warmup步数 = 总步数的1-5%
- 记录学习率变化用于调试

### 10. 忽视硬件特性

**效率陷阱**：
- 批量大小不是8的倍数（Tensor Core利用率低）
- 梯度同步频率过高（分布式训练）
- 混合精度训练未开启自动混合精度（AMP）

**调试建议总结**：

1. **建立基线**：先用小数据集验证算法正确性
2. **逐步增加复杂度**：简单模型 → 复杂模型
3. **可视化一切**：损失曲线、梯度分布、参数分布
4. **版本控制**：记录每次实验的超参数和结果
5. **单元测试**：测试梯度计算、损失函数等关键组件

记住：优化问题的调试往往比实现更耗时。保持耐心，系统地排查问题。

---

[返回目录](index.md) | [下一章：统计学习理论 →](chapter2.md)